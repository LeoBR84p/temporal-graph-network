{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqpYiRa1Ub5T"
   },
   "source": [
    "#**Licença de Uso**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsA_nGhh8Psj"
   },
   "source": [
    "This repository uses a **dual-license model** to distinguish between source code and creative/documental content.\n",
    "\n",
    "**Code** (Python scripts, modules, utilities):\n",
    "Licensed under the MIT License.\n",
    "\n",
    "→ You may freely use, modify, and redistribute the code, including for commercial purposes, provided that you preserve the copyright notice.\n",
    "\n",
    "**Content** (Jupyter notebooks, documentation, reports, datasets, and generated outputs):\n",
    "Licensed under the Creative Commons Attribution–NonCommercial 4.0 International License.\n",
    "\n",
    "→ You may share and adapt the content for non-commercial purposes, provided that proper credit is given to the original author.\n",
    "\n",
    "\n",
    "**© 2025 Leandro Bernardo Rodrigues**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AyszD9eYTtAS"
   },
   "source": [
    "#**Pré-Configuração**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QX0x9aXfTJeA"
   },
   "source": [
    "##**Código de uso único**\n",
    "Aplicação persistente entre sessões do Google Colab\n",
    "\n",
    "---\n",
    "**Uso expecífico para Google Colab.**\n",
    "\n",
    "**Aviso:** implementação no JupytherHub e GitLab são diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "otaQwrjJSgOQ"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#montar o Google Drive e preparar a pasta do projeto\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "import os, subprocess, getpass, pathlib\n",
    "BASE = \"/content/drive/MyDrive/Notebooks\"\n",
    "REPO = \"temporal-graph-network\"\n",
    "PROJ = f\"{BASE}/{REPO}\"\n",
    "os.makedirs(BASE, exist_ok=True)\n",
    "%cd \"$BASE\"\n",
    "\n",
    "#clonar o repositório existente do GitHub (se ainda não estiver clonado) ===\n",
    "if not os.path.exists(PROJ):\n",
    "    GITHUB_URL = \"https://github.com/LeoBR84p/temporal-graph-network.git\"\n",
    "    # Dica: use PAT quando o push for necessário; para clone público basta a URL.\n",
    "    !git clone $GITHUB_URL\n",
    "else:\n",
    "    print(\"Pasta do projeto já existe, seguindo adiante...\")\n",
    "%cd \"$PROJ\"\n",
    "\n",
    "#criar pastas utilitárias que você quer manter no projeto ===\n",
    "#não sobrescreve nada; só cria se não existirem\n",
    "!mkdir -p notebooks src data output runs configs\n",
    "\n",
    "#instalar pacotes (na sessão atual) para conseguir configurar os filtros ===\n",
    "!pip -q install jupytext nbdime nbstripout\n",
    "\n",
    "#configurar Git/NBDime/Jupytext no *repositório* (persistem em .git/config) ===\n",
    "#usar --local faz a config ficar gravada em .git/config (persiste no Drive)\n",
    "!git config --local user.name \"Leandro Bernardo Rodrigues\"\n",
    "!git config --local user.email \"bernardo.leandro@gmail.com\"\n",
    "!git config --local init.defaultBranch main\n",
    "\n",
    "# OBS: no Colab, o nbdime com --local pode falhar; use --global nesta sessão\n",
    "!nbdime config-git --enable --global\n",
    "\n",
    "#.gitignore e .gitattributes (só criar se não existirem) ===\n",
    "if not pathlib.Path(\".gitignore\").exists():\n",
    "    with open(\".gitignore\",\"w\") as f:\n",
    "        f.write(\"\"\"\\\n",
    ".ipynb_checkpoints/\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "*.log\n",
    "*.tmp\n",
    "# dados/artefatos pesados (não versionar)\n",
    "data/\n",
    "output/\n",
    "runs/\n",
    "# Python\n",
    "venv/\n",
    "__pycache__/\n",
    "*.pyc\n",
    "# segredos\n",
    ".env\n",
    "*.key\n",
    "*.pem\n",
    "*.tok\n",
    "\"\"\")\n",
    "if not pathlib.Path(\".gitattributes\").exists():\n",
    "    with open(\".gitattributes\",\"w\") as f:\n",
    "        f.write(\"*.ipynb filter=nbstripout\\n\")\n",
    "\n",
    "#ativar o hook do nbstripout neste repositório (persiste)\n",
    "!nbstripout --install --attributes .gitattributes\n",
    "\n",
    "#parear notebooks com .py para diffs legíveis ===\n",
    "!jupytext --set-formats ipynb,py:percent --sync notebooks/*.ipynb || true\n",
    "\n",
    "#commit inicial dessas configs locais (se houver algo novo) e push ===\n",
    "!git add -A\n",
    "!git status\n",
    "!git commit -m \"chore: setup local (.gitignore/.gitattributes, nbstripout, jupytext config)\" || true\n",
    "\n",
    "#se o remoto já tem README/commits, faça pull --rebase antes do primeiro push\n",
    "!git pull --rebase origin main || true\n",
    "\n",
    "#push (ao pedir senha, use seu PAT como senha do Git)\n",
    "import getpass, subprocess, sys\n",
    "\n",
    "owner = \"LeoBR84p\"\n",
    "repo  = \"temporal-graph-network\"\n",
    "clean_url = f\"https://github.com/{owner}/{repo}.git\"\n",
    "\n",
    "# 1) Tenta push \"normal\" (pode falhar por falta de credencial)\n",
    "push = subprocess.run([\"git\",\"push\",\"origin\",\"main\"], capture_output=True, text=True)\n",
    "if push.returncode == 0:\n",
    "    print(\"Push concluído sem PAT.\")\n",
    "else:\n",
    "    print(\"Primeiro push falhou (provável falta de credenciais). Vamos usar um PAT temporário…\")\n",
    "    # 2) Pede o PAT e testa autenticação antes do push\n",
    "    token = getpass.getpass(\"Cole seu GitHub PAT (não será exibido): \").strip()\n",
    "    # Formato mais compatível: user + token na URL\n",
    "    # Use seu usuário real do GitHub (case sensitive)\n",
    "    username = \"LeoBR84p\"\n",
    "    auth_url = f\"https://{username}:{token}@github.com/{owner}/{repo}.git\"\n",
    "\n",
    "    try:\n",
    "        # Teste rápido de auth (ls-remote) para ver se o token tem acesso de escrita\n",
    "        test = subprocess.run([\"git\",\"ls-remote\", auth_url],\n",
    "                              capture_output=True, text=True)\n",
    "        if test.returncode != 0:\n",
    "            print(\"Falha ao autenticar com o PAT. Detalhe do erro:\")\n",
    "            print(test.stderr or test.stdout)\n",
    "            raise SystemExit(1)\n",
    "\n",
    "        # 3) Troca a URL, faz push e restaura a URL limpa\n",
    "        subprocess.run([\"git\",\"remote\",\"set-url\",\"origin\", auth_url], check=True)\n",
    "        out = subprocess.run([\"git\",\"push\",\"origin\",\"main\"], capture_output=True, text=True)\n",
    "        if out.returncode != 0:\n",
    "            print(\"Falha no push mesmo com PAT. Detalhe do erro:\")\n",
    "            print(out.stderr or out.stdout)\n",
    "            raise SystemExit(out.returncode)\n",
    "        print(\"Push concluído com PAT.\")\n",
    "    finally:\n",
    "        subprocess.run([\"git\",\"remote\",\"set-url\",\"origin\", clean_url], check=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYTsrVeiUEYt"
   },
   "source": [
    "##**Código a cada sessão**\n",
    "---\n",
    "Aplicação não persistente entre sessões.\n",
    "\n",
    "Necessário para sincronização e versionamento de alterações no código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKUkAzsYgCRv"
   },
   "source": [
    "###**Montar e sincronizar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "2aoXCRgAUYod"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#setup por sessão (colab)\n",
    "from google.colab import drive\n",
    "import os, time, subprocess, getpass, pathlib, sys\n",
    "\n",
    "BASE = \"/content/drive/MyDrive/Notebooks\"\n",
    "REPO = \"temporal-graph-network\"\n",
    "PROJ = f\"{BASE}/{REPO}\"\n",
    "\n",
    "def safe_mount_google_drive():\n",
    "    #monta ou remonta o google drive de forma resiliente\n",
    "    try:\n",
    "        drive.mount('/content/drive', force_remount=True)\n",
    "    except Exception:\n",
    "        try:\n",
    "            drive.flush_and_unmount()\n",
    "        except Exception:\n",
    "            pass\n",
    "        time.sleep(1.0)\n",
    "        drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "def safe_chdir(path):\n",
    "    #usa os.chdir (evita %cd com f-string)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Caminho não existe: {path}\")\n",
    "    os.chdir(path)\n",
    "    print(\"Diretório atual:\", os.getcwd())\n",
    "\n",
    "def branch_a_frente():\n",
    "    #retorna true se head está à frente do upstream (há o que enviar)\n",
    "    ahead = subprocess.run(\n",
    "        [\"git\",\"rev-list\",\"--left-right\",\"--count\",\"HEAD...@{upstream}\"],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    if ahead.returncode != 0:\n",
    "        status = subprocess.run([\"git\",\"status\",\"-sb\"], capture_output=True, text=True)\n",
    "        return \"ahead\" in (status.stdout or \"\")\n",
    "    left_right = (ahead.stdout or \"\").strip().split()\n",
    "    return len(left_right) == 2 and left_right[0].isdigit() and int(left_right[0]) > 0\n",
    "\n",
    "def push_seguro(owner=\"LeoBR84p\", repo=\"temporal-graph-network\", username=\"LeoBR84p\"):\n",
    "    #realiza push usando pat em memória; restaura url limpa ao final\n",
    "    clean_url = f\"https://github.com/{owner}/{repo}.git\"\n",
    "    token = getpass.getpass(\"Cole seu GitHub PAT (Contents: Read and write): \").strip()\n",
    "    auth_url = f\"https://{username}:{token}@github.com/{owner}/{repo}.git\"\n",
    "    test = subprocess.run([\"git\",\"ls-remote\", auth_url], capture_output=True, text=True)\n",
    "    if test.returncode != 0:\n",
    "        print(\"Falha na autenticação (read). Revise token/permissões:\")\n",
    "        print(test.stderr or test.stdout); return\n",
    "    try:\n",
    "        subprocess.run([\"git\",\"remote\",\"set-url\",\"origin\", auth_url], check=True)\n",
    "        out = subprocess.run([\"git\",\"push\",\"origin\",\"main\"], capture_output=True, text=True)\n",
    "        if out.returncode != 0:\n",
    "            print(\"Falha no push (write). Revise permissões do token:\")\n",
    "            print(out.stderr or out.stdout)\n",
    "        else:\n",
    "            print(\"Push concluído com PAT.\")\n",
    "    finally:\n",
    "        subprocess.run([\"git\",\"remote\",\"set-url\",\"origin\", clean_url], check=False)\n",
    "\n",
    "#montar/remontar o google drive e entrar no projeto\n",
    "safe_mount_google_drive()\n",
    "os.makedirs(BASE, exist_ok=True)\n",
    "if not os.path.exists(PROJ):\n",
    "    print(f\"Atenção: pasta do projeto não encontrada em {PROJ}. \"\n",
    "          \"Execute seu bloco de configuração única (clone) primeiro.\")\n",
    "else:\n",
    "    print(\"Pasta do projeto encontrada.\")\n",
    "safe_chdir(PROJ)\n",
    "\n",
    "#sanity check do repositório git\n",
    "if not os.path.isdir(\".git\"):\n",
    "    print(\"Aviso: esta pasta não parece ser um repositório Git (.git ausente). \"\n",
    "          \"Rode o bloco de configuração única.\")\n",
    "else:\n",
    "    print(\"Repositório Git detectado.\")\n",
    "\n",
    "#instalar dependências efêmeras desta sessão\n",
    "!pip -q install jupytext nbdime nbstripout\n",
    "!nbdime config-git --enable --global\n",
    "\n",
    "#atualizar do remoto\n",
    "!git fetch origin\n",
    "!git pull --rebase origin main\n",
    "\n",
    "#sincronizar notebooks → .py (jupytext)\n",
    "!jupytext --sync notebooks/*.ipynb || true\n",
    "\n",
    "#ciclo de versionamento do dia (commit genérico opcional)\n",
    "!git add -A\n",
    "!git status\n",
    "!git commit -m \"feat: ajustes no notebook X e pipeline Y\" || true\n",
    "\n",
    "#push somente se houver commits locais à frente; com fallback para pat\n",
    "if branch_a_frente():\n",
    "    out = subprocess.run([\"git\",\"push\",\"origin\",\"main\"], capture_output=True, text=True)\n",
    "    if out.returncode == 0:\n",
    "        print(\"Push concluído sem PAT.\")\n",
    "    else:\n",
    "        print(\"Push sem PAT falhou. Chamando push_seguro()…\")\n",
    "        push_seguro()\n",
    "else:\n",
    "    print(\"Nada para enviar (branch sincronizada com o remoto).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JF-bzXqkfjsX"
   },
   "source": [
    "###**Utilitários Git**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWTE_5SOfos5"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#helpers de git: push seguro, commit customizado e tag de release\n",
    "import subprocess, getpass\n",
    "\n",
    "#ajuste se você mudar o nome do repositório/usuário\n",
    "OWNER = \"LeoBR84p\"\n",
    "REPO = \"temporal-graph-network\"\n",
    "USERNAME = \"LeoBR84p\"\n",
    "BRANCH = \"main\"\n",
    "REMOTE = \"origin\"\n",
    "\n",
    "def branch_a_frente():\n",
    "    #retorna true se head está à frente do upstream (há o que enviar)\n",
    "    out = subprocess.run(\n",
    "        [\"git\",\"rev-list\",\"--left-right\",\"--count\",f\"HEAD...@{{upstream}}\"],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    if out.returncode != 0:\n",
    "        st = subprocess.run([\"git\",\"status\",\"-sb\"], capture_output=True, text=True)\n",
    "        return \"ahead\" in (st.stdout or \"\")\n",
    "    left_right = (out.stdout or \"\").strip().split()\n",
    "    return len(left_right) == 2 and left_right[0].isdigit() and int(left_right[0]) > 0\n",
    "\n",
    "def push_seguro(owner=OWNER, repo=REPO, username=USERNAME, remote=REMOTE, branch=BRANCH):\n",
    "    #realiza push usando pat em memória; restaura url limpa ao final\n",
    "    clean_url = f\"https://github.com/{owner}/{repo}.git\"\n",
    "    token = getpass.getpass(\"cole seu github pat (contents: read and write): \").strip()\n",
    "    auth_url = f\"https://{username}:{token}@github.com/{owner}/{repo}.git\"\n",
    "    test = subprocess.run([\"git\",\"ls-remote\", auth_url], capture_output=True, text=True)\n",
    "    if test.returncode != 0:\n",
    "        print(\"falha na autenticação (read). revise token/permissões:\")\n",
    "        print(test.stderr or test.stdout); return False\n",
    "    try:\n",
    "        subprocess.run([\"git\",\"remote\",\"set-url\", remote, auth_url], check=True)\n",
    "        out = subprocess.run([\"git\",\"push\", remote, branch], capture_output=True, text=True)\n",
    "        if out.returncode != 0:\n",
    "            print(\"falha no push (write). revise permissões do token:\")\n",
    "            print(out.stderr or out.stdout); return False\n",
    "        print(\"push concluído com pat.\")\n",
    "        return True\n",
    "    finally:\n",
    "        subprocess.run([\"git\",\"remote\",\"set-url\", remote, clean_url], check=False)\n",
    "\n",
    "def try_push_branch(remote=REMOTE, branch=BRANCH):\n",
    "    #tenta push direto da branch atual\n",
    "    out = subprocess.run([\"git\",\"push\", remote, branch], capture_output=True, text=True)\n",
    "    if out.returncode == 0:\n",
    "        print(\"push concluído.\")\n",
    "        return True\n",
    "    print(\"push sem credencial falhou:\")\n",
    "    print((out.stderr or out.stdout).strip())\n",
    "    return False\n",
    "\n",
    "def try_push_tag(tag, remote=REMOTE):\n",
    "    #tenta enviar somente a tag\n",
    "    out = subprocess.run([\"git\",\"push\", remote, tag], capture_output=True, text=True)\n",
    "    if out.returncode == 0:\n",
    "        print(f\"tag enviada: {tag}\")\n",
    "        return True\n",
    "    print(\"falha ao enviar a tag:\")\n",
    "    print((out.stderr or out.stdout).strip())\n",
    "    return False\n",
    "\n",
    "def commit_custom(msg: str, auto_push: bool = True):\n",
    "    #adiciona tudo, cria commit com a mensagem informada e faz push opcional (com fallback para pat)\n",
    "    subprocess.run([\"git\",\"add\",\"-A\"], check=False)\n",
    "    com = subprocess.run([\"git\",\"commit\",\"-m\", msg], capture_output=True, text=True)\n",
    "    if com.returncode != 0:\n",
    "        print((com.stderr or com.stdout or \"nada para commitar.\").strip())\n",
    "        return\n",
    "    print(com.stdout.strip())\n",
    "    if auto_push and branch_a_frente():\n",
    "        if not try_push_branch():\n",
    "            print(\"tentando push seguro…\")\n",
    "            push_seguro()\n",
    "\n",
    "def tag_release(tag: str, message: str = \"\", auto_push: bool = True):\n",
    "    #cria uma tag anotada (release) e faz push da tag com fallback para pat\n",
    "    exists = subprocess.run([\"git\",\"tag\",\"--list\", tag], capture_output=True, text=True)\n",
    "    if tag in (exists.stdout or \"\").split():\n",
    "        print(f\"tag '{tag}' já existe. para refazer: git tag -d {tag} && git push {REMOTE} :refs/tags/{tag}\")\n",
    "        return\n",
    "    args = [\"git\",\"tag\",\"-a\", tag, \"-m\", (message or tag)]\n",
    "    mk = subprocess.run(args, capture_output=True, text=True)\n",
    "    if mk.returncode != 0:\n",
    "        print(\"falha ao criar a tag:\")\n",
    "        print(mk.stderr or mk.stdout); return\n",
    "    print(f\"tag criada: {tag}\")\n",
    "    if auto_push:\n",
    "        if not try_push_tag(tag):\n",
    "            print(\"tentando push seguro da tag…\")\n",
    "            if push_seguro():\n",
    "                try_push_tag(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXDbQvSrgd8Y"
   },
   "source": [
    "#**Sincronizar alterações no código do projeto**\n",
    "Comandos para sincronizar código (Google Drive, Git, GitHub) e realizar versionamento\n",
    "\n",
    "Tag de release atual: 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLlyq-0ZPCs0"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#ID0002\n",
    "#push do Drive -> GitHub (Drive é a fonte da verdade)\n",
    "#respeita .gitignore do Drive\n",
    "#sempre em 'main', sem pull, commit + push imediato\n",
    "#mensagem de commit padronizada com timestamp SP\n",
    "#bump de versão (M/m/n) + tag anotada\n",
    "#force push (branch e tags), silencioso; só 1 print final\n",
    "#PAT lido de segredo do Colab: GITHUB_PAT_DA (fallback: env; último caso: prompt)\n",
    "\n",
    "from pathlib import Path\n",
    "import subprocess, os, re, shutil, sys, getpass\n",
    "from urllib.parse import quote as urlquote\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "#utilitários silenciosos\n",
    "def sh(cmd, cwd=None, check=True):\n",
    "    \"\"\"\n",
    "    Executa comando silencioso. Em erro, levanta RuntimeError com rc e UM rascunho de causa,\n",
    "    mascarando URLs com credenciais (ex.: https://***:***@github.com/...).\n",
    "    \"\"\"\n",
    "    safe_cmd = []\n",
    "    for x in cmd:\n",
    "        if isinstance(x, str) and \"github.com\" in x and \"@\" in x:\n",
    "            #mascara credenciais: https://user:token@ -> https://***:***@\n",
    "            x = re.sub(r\"https://[^:/]+:[^@]+@\", \"https://***:***@\", x)\n",
    "        safe_cmd.append(x)\n",
    "\n",
    "    r = subprocess.run(cmd, cwd=cwd, text=True, capture_output=True)\n",
    "    if check and r.returncode != 0:\n",
    "        #heurística curtinha p/ tornar rc=128 mais informativo sem vazar nada\n",
    "        stderr = (r.stderr or \"\").strip().lower()\n",
    "        if \"authentication failed\" in stderr or \"permission\" in stderr or \"not found\" in stderr:\n",
    "            hint = \"auth/permissões/URL\"\n",
    "        elif \"not a git repository\" in stderr:\n",
    "            hint = \"repo local inválido\"\n",
    "        else:\n",
    "            hint = \"git falhou\"\n",
    "        cmd_hint = \" \".join(safe_cmd[:3])\n",
    "        raise RuntimeError(f\"rc={r.returncode}; {hint}; cmd={cmd_hint}\")\n",
    "    return r.stdout\n",
    "\n",
    "def git(*args, cwd=None, check=True):\n",
    "    return sh([\"git\", *args], cwd=cwd, check=check)\n",
    "\n",
    "#configurações do projeto\n",
    "author_name    = \"Leandro Bernardo Rodrigues\"\n",
    "owner          = \"LeoBR84p\"         # dono do repositório no GitHub\n",
    "repo_name      = \"data-analysis\"    # nome do repositório\n",
    "default_branch = \"main\"\n",
    "repo_dir       = Path(\"/content/drive/MyDrive/Notebooks/temporal-graph-network\")\n",
    "remote_base    = f\"https://github.com/{owner}/{repo_name}.git\"\n",
    "author_email   = f\"bernardo.leandro@gmail.com\"  # evita erro de identidade\n",
    "\n",
    "#nbstripout: \"install\" para limpar outputs; \"disable\" para versionar outputs\n",
    "nbstripout_mode = \"install\"\n",
    "import shutil\n",
    "exe = shutil.which(\"nbstripout\")\n",
    "git(\"config\", \"--local\", \"filter.nbstripout.clean\", exe if exe else \"nbstripout\", cwd=repo_dir)\n",
    "\n",
    "#ambiente: Colab + Drive\n",
    "def ensure_drive():\n",
    "    try:\n",
    "        from google.colab import drive  # type: ignore\n",
    "        base = Path(\"/content/drive/MyDrive\")\n",
    "        if not base.exists():\n",
    "            drive.mount(\"/content/drive\")\n",
    "        if not base.exists():\n",
    "            raise RuntimeError(\"Google Drive não montado.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Falha ao montar o Drive: {e}\")\n",
    "\n",
    "#repo local no Drive\n",
    "def is_empty_dir(p: Path) -> bool:\n",
    "    try:\n",
    "        return p.exists() and not any(p.iterdir())\n",
    "    except Exception:\n",
    "        return True\n",
    "\n",
    "def init_or_recover_repo():\n",
    "    repo_dir.mkdir(parents=True, exist_ok=True)\n",
    "    git_dir = repo_dir / \".git\"\n",
    "\n",
    "    def _fresh_init():\n",
    "        if git_dir.exists():\n",
    "            shutil.rmtree(git_dir, ignore_errors=True)\n",
    "        git(\"init\", cwd=repo_dir)\n",
    "\n",
    "    #caso .git no Colab ausente ou vazia -> init limpo\n",
    "    if not git_dir.exists() or is_empty_dir(git_dir):\n",
    "        _fresh_init()\n",
    "    else:\n",
    "        #valida se é um work-tree git funcional no Colab; se falhar -> init limpo\n",
    "        try:\n",
    "            git(\"rev-parse\", \"--is-inside-work-tree\", cwd=repo_dir)\n",
    "        except Exception:\n",
    "            _fresh_init()\n",
    "\n",
    "    #aborta operações pendentes (não apaga histórico)\n",
    "    for args in ((\"rebase\", \"--abort\"), (\"merge\", \"--abort\"), (\"cherry-pick\", \"--abort\")):\n",
    "        try:\n",
    "            git(*args, cwd=repo_dir, check=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    #força branch main\n",
    "    try:\n",
    "        sh([\"git\", \"switch\", \"-C\", default_branch], cwd=repo_dir)\n",
    "    except Exception:\n",
    "        sh([\"git\", \"checkout\", \"-B\", default_branch], cwd=repo_dir)\n",
    "\n",
    "    #configura identidade local\n",
    "    try:\n",
    "        git(\"config\", \"user.name\", author_name, cwd=repo_dir)\n",
    "        git(\"config\", \"user.email\", author_email, cwd=repo_dir)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #marca o diretório como safe\n",
    "    try:\n",
    "        sh([\"git\",\"config\",\"--global\",\"--add\",\"safe.directory\", str(repo_dir)])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #sanity check final (falha cedo se algo ainda estiver errado)\n",
    "    git(\"status\", \"--porcelain\", cwd=repo_dir)\n",
    "\n",
    "\n",
    "#nbstripout (opcional)\n",
    "def setup_nbstripout():\n",
    "    if nbstripout_mode == \"disable\":\n",
    "        #remove configs do filtro\n",
    "        sh([\"git\",\"config\",\"--local\",\"--unset-all\",\"filter.nbstripout.clean\"], cwd=repo_dir, check=False)\n",
    "        sh([\"git\",\"config\",\"--local\",\"--unset-all\",\"filter.nbstripout.smudge\"], cwd=repo_dir, check=False)\n",
    "        sh([\"git\",\"config\",\"--local\",\"--unset-all\",\"filter.nbstripout.required\"], cwd=repo_dir, check=False)\n",
    "        gat = repo_dir / \".gitattributes\"\n",
    "        if gat.exists():\n",
    "            lines = gat.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
    "            new_lines = [ln for ln in lines if \"filter=nbstripout\" not in ln]\n",
    "            gat.write_text(\"\\n\".join(new_lines) + (\"\\n\" if new_lines else \"\"), encoding=\"utf-8\")\n",
    "        return\n",
    "\n",
    "    #instala nbstripout (se necessário)\n",
    "    try:\n",
    "        import nbstripout  #noqa: F401\n",
    "    except Exception:\n",
    "        sh([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"nbstripout\"])\n",
    "\n",
    "    py = sys.executable\n",
    "    #configurar filtro sem aspas extras\n",
    "    git(\"config\", \"--local\", \"filter.nbstripout.clean\", \"nbstripout\", cwd=repo_dir)\n",
    "    git(\"config\", \"--local\", \"filter.nbstripout.smudge\", \"cat\", cwd=repo_dir)\n",
    "    git(\"config\", \"--local\", \"filter.nbstripout.required\", \"true\", cwd=repo_dir)\n",
    "    gat = repo_dir / \".gitattributes\"\n",
    "    line = \"*.ipynb filter=nbstripout\"\n",
    "    if gat.exists():\n",
    "        txt = gat.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        if line not in txt:\n",
    "            gat.write_text((txt.rstrip() + \"\\n\" + line + \"\\n\"), encoding=\"utf-8\")\n",
    "    else:\n",
    "        gat.write_text(line + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "#.gitignore normalização\n",
    "def normalize_tracked_ignored():\n",
    "    \"\"\"\n",
    "    Se houver arquivos já rastreados que hoje são ignorados pelo .gitignore,\n",
    "    limpa o índice e re-adiciona respeitando o .gitignore.\n",
    "    Retorna True se normalizou algo; False caso contrário.\n",
    "    \"\"\"\n",
    "    #remove lock de índice, se houver\n",
    "    lock = repo_dir / \".git/index.lock\"\n",
    "    try:\n",
    "        if lock.exists():\n",
    "            lock.unlink()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #garante que o índice existe (ou se recupera)\n",
    "    idx = repo_dir / \".git/index\"\n",
    "    if not idx.exists():\n",
    "        try:\n",
    "            sh([\"git\", \"reset\", \"--mixed\"], cwd=repo_dir)\n",
    "        except Exception:\n",
    "            try:\n",
    "                sh([\"git\", \"init\"], cwd=repo_dir)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    #detecta arquivos ignorados que estão rastreados e normaliza\n",
    "    normalized = False\n",
    "    try:\n",
    "        out = git(\"ls-files\", \"-z\", \"--ignored\", \"--exclude-standard\", \"--cached\", cwd=repo_dir)\n",
    "        tracked_ignored = [p for p in out.split(\"\\x00\") if p]\n",
    "        if tracked_ignored:\n",
    "            git(\"rm\", \"-r\", \"--cached\", \".\", cwd=repo_dir)\n",
    "            git(\"add\", \"-A\", cwd=repo_dir)\n",
    "            normalized = True\n",
    "    except Exception:\n",
    "        #falhou a detecção? segue o fluxo sem travar\n",
    "        pass\n",
    "\n",
    "    return normalized\n",
    "\n",
    "#semVer e bump de versão\n",
    "_semver = re.compile(r\"^(\\d+)\\.(\\d+)\\.(\\d+)$\")\n",
    "\n",
    "def parse_semver(s):\n",
    "    m = _semver.match((s or \"\").strip())\n",
    "    return tuple(map(int, m.groups())) if m else None\n",
    "\n",
    "def current_version():\n",
    "    try:\n",
    "        tags = [t for t in git(\"tag\", \"--list\", cwd=repo_dir).splitlines() if parse_semver(t)]\n",
    "        if tags:\n",
    "            return sorted(tags, key=lambda x: parse_semver(x))[-1]\n",
    "    except Exception:\n",
    "        pass\n",
    "    vf = repo_dir / \"VERSION\"\n",
    "    if vf.exists():\n",
    "        v = vf.read_text(encoding=\"utf-8\").strip()\n",
    "        if parse_semver(v):\n",
    "            return v\n",
    "    return \"1.0.0\"\n",
    "\n",
    "def bump(v, kind):\n",
    "    M, m, p = parse_semver(v) or (1, 0, 0)\n",
    "    k = (kind or \"\").strip()\n",
    "    if k == \"m\":\n",
    "        return f\"{M}.{m+1}.0\"\n",
    "    if k == \"n\":\n",
    "        return f\"{M}.{m}.{p+1}\"\n",
    "    return f\"{M+1}.0.0\"  #default major\n",
    "\n",
    "#timestamp SP\n",
    "def now_sp():\n",
    "    #tenta usar zoneinfo; fallback fixo -03:00 (Brasil sem DST atualmente)\n",
    "    try:\n",
    "        from zoneinfo import ZoneInfo  # Py3.9+\n",
    "        tz = ZoneInfo(\"America/Sao_Paulo\")\n",
    "        dt = datetime.now(tz)\n",
    "    except Exception:\n",
    "        dt = datetime.now(timezone(timedelta(hours=-3)))\n",
    "    #formato legível + offset\n",
    "    return dt.strftime(\"%Y-%m-%d %H:%M:%S%z\")  # ex.: 2025-10-08 02:34:00-0300\n",
    "\n",
    "#autenticação (PAT)\n",
    "def get_pat():\n",
    "    #Colab Secrets\n",
    "    token = None\n",
    "    try:\n",
    "        from google.colab import userdata  #type: ignore\n",
    "        token = userdata.get('GITHUB_PAT_DA')  #nome do segredo criado no Colab\n",
    "    except Exception:\n",
    "        token = None\n",
    "    #fallback1 - variável de ambiente\n",
    "    if not token:\n",
    "        token = os.environ.get(\"GITHUB_PAT_DA\") or os.environ.get(\"GITHUB_PAT\")\n",
    "    #fallback2 - interativo\n",
    "    if not token:\n",
    "        token = getpass.getpass(\"Informe seu GitHub PAT: \").strip()\n",
    "    if not token:\n",
    "        raise RuntimeError(\"PAT ausente.\")\n",
    "    return token\n",
    "\n",
    "#listas de força\n",
    "FORCE_UNTRACK = [\"input/\", \"output/\", \"data/\", \"runs/\", \"logs/\", \"figures/\"]\n",
    "FORCE_TRACK   = [\"references/\"]  #versionar tudo dentro (PDFs inclusive)\n",
    "\n",
    "def force_index_rules():\n",
    "    #garante que pastas sensíveis NUNCA fiquem rastreadas\n",
    "    for p in FORCE_UNTRACK:\n",
    "        try:\n",
    "            git(\"rm\", \"-r\", \"--cached\", \"--\", p, cwd=repo_dir)\n",
    "        except Exception:\n",
    "            pass\n",
    "    #garante que references/ SEMPRE entre (útil se ainda há *.pdf globais)\n",
    "    for p in FORCE_TRACK:\n",
    "        try:\n",
    "            git(\"add\", \"-f\", \"--\", p, cwd=repo_dir)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "#fluxo principal\n",
    "def main():\n",
    "    try:\n",
    "        ensure_drive()\n",
    "        init_or_recover_repo()\n",
    "        setup_nbstripout()\n",
    "\n",
    "        #pergunta apenas o tipo de versão (M/m/n)\n",
    "        kind = input(\"Informe o tipo de mudança: Maior (M), menor (m) ou pontual (n): \").strip()\n",
    "        if kind not in (\"M\", \"m\", \"n\"):\n",
    "            kind = \"n\"\n",
    "\n",
    "        #versão\n",
    "        cur = current_version()\n",
    "        new = bump(cur, kind)\n",
    "        (repo_dir / \"VERSION\").write_text(new + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "        #normaliza itens ignorados que estejam rastreados (uma única vez, se necessário)\n",
    "        normalize_tracked_ignored()\n",
    "\n",
    "        #aplica regras de força\n",
    "        force_index_rules()\n",
    "\n",
    "        #stage de tudo (Drive é a verdade; remoções entram aqui)\n",
    "        git(\"add\", \"-A\", cwd=repo_dir)\n",
    "\n",
    "        #mensagem padronizada de commit\n",
    "        ts = now_sp()\n",
    "        commit_msg = f\"upload pelo {author_name} em {ts}\"\n",
    "        try:\n",
    "            git(\"commit\", \"-m\", commit_msg, cwd=repo_dir)\n",
    "        except Exception:\n",
    "            #se nada a commitar, seguimos (pode ocorrer se só a tag mudar, mas aqui VERSION muda)\n",
    "            status = git(\"status\", \"--porcelain\", cwd=repo_dir)\n",
    "            if status.strip():\n",
    "                raise\n",
    "\n",
    "        #Tag anotada (substitui se já existir)\n",
    "        try:\n",
    "            git(\"tag\", \"-a\", new, \"-m\", f\"release {new} — {commit_msg}\", cwd=repo_dir)\n",
    "        except Exception:\n",
    "            sh([\"git\", \"tag\", \"-d\", new], cwd=repo_dir, check=False)\n",
    "            git(\"tag\", \"-a\", new, \"-m\", f\"release {new} — {commit_msg}\", cwd=repo_dir)\n",
    "\n",
    "        #push com PAT (Drive é a verdade): validação + push forçado\n",
    "        token = get_pat()\n",
    "        user_for_url = owner  # você é o owner; não perguntamos\n",
    "        auth_url = f\"https://{urlquote(user_for_url, safe='')}:{urlquote(token, safe='')}@github.com/{owner}/{repo_name}.git\"\n",
    "\n",
    "        #valida credenciais/URL de forma silenciosa (sem vazar token)\n",
    "        #tenta checar a branch main; se não existir (repo vazio), faz um probe genérico\n",
    "        try:\n",
    "            sh([\"git\", \"ls-remote\", auth_url, f\"refs/heads/{default_branch}\"], cwd=repo_dir)\n",
    "        except RuntimeError:\n",
    "            #repositório pode estar vazio (sem refs); probe sem ref deve funcionar\n",
    "            sh([\"git\", \"ls-remote\", auth_url], cwd=repo_dir)\n",
    "\n",
    "        #push forçado de branch e tags\n",
    "        sh([\"git\", \"push\", \"-u\", \"--force\", auth_url, default_branch], cwd=repo_dir)\n",
    "        sh([\"git\", \"push\", \"--force\", auth_url, \"--tags\"], cwd=repo_dir)\n",
    "\n",
    "        print(f\"[ok]   Registro no GitHub com sucesso. Versão atual {new}\")\n",
    "    except Exception as e:\n",
    "        #mensagem única, curta, sem detalhes sensíveis\n",
    "        msg = str(e) or \"falha inesperada\"\n",
    "        print(f\"[erro] {msg}\")\n",
    "\n",
    "#executa\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztpkgjHHt8nv"
   },
   "source": [
    "#**Checklist rápido de execução**\n",
    "**Etapas:**\n",
    "- 01–05: setup (ambiente, dependências, diretórios, configs e upload de CSVs)\n",
    "- 06–10: execução (consumo dos dados, criação de grafos, config das janelas temporais, agregação de infos aos grafos, config dos modelos matemáticos)\n",
    "- 11-15: geração de output (salva análise, gera gráficos gerais, gera gráficos específicos e relatórios em HTML+PDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tK_1f_eT-fo8"
   },
   "source": [
    "#**Temporal Graph Network / Rede de Grapho Temporal**\n",
    "\n",
    "Uma **Rede de Graphos Temporais (TGN)** é um modelo de aprendizado de máquina que processa dados representados como um grafo dinâmico. Ela captura a evolução da estrutura e das conexões de entidades (nós) ao longo do tempo. **Ou seja, ela leva em consideração o comportamento temporal das atividades e seu relacionamento, ao invés de uma avaliação única e estanque no tempo.**\n",
    "_____\n",
    "\n",
    "**Caso aplicado: Detecção de anomalias sem gabarito (sem dados históricos)**\n",
    "\n",
    "Imagine uma rede de transações financeiras. A TGN analisa o histórico de como cada beneficiário, usuário demandante do pagamento e unidade de negócio (nós) se conectam e interagem uns com os outros. Sem saber o que é uma anomalia, ela aprende o comportamento normal da rede.\n",
    "Ao notar um padrão atípico, como um usuário que subitamente começa a demandar transferências para muitas novas contas em um curto período, a TGN destaca isso como uma anomalia comportamental. Ela usa a história do nó e o contexto temporal para sinalizar o desvio, sem precisar de exemplos de anomalia pré-existentes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHdgD6uGAiRK"
   },
   "source": [
    "### **Etapa 1:** Ativação do ambiente virtual (utilizando atualmente Google Colab para prototipação com dados sintéticos)\n",
    "---\n",
    "Necessário ajustar pontualmente em caso de utilização em outro ambiente de notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189
    },
    "executionInfo": {
     "elapsed": 1003,
     "status": "ok",
     "timestamp": 1760064171947,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "6769fb9b",
    "outputId": "c9d75b8a-964b-4b9f-c857-414a6ab5726c"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID001\n",
    "import os\n",
    "\n",
    "# Define the path for the virtual environment inside Google Drive\n",
    "# Ensure BASE and REPO are defined correctly from previous cells if needed\n",
    "# Assuming BASE and REPO are defined as in cell otaQwrjJSgOQ\n",
    "BASE = \"/content/drive/MyDrive/Notebooks\"\n",
    "REPO = \"temporal-graph-network\"\n",
    "PROJ = f\"{BASE}/{REPO}\"\n",
    "VENV_PATH = f\"{PROJ}/.venv_tgn\" # Updated venv path to be a hidden folder inside PROJ\n",
    "\n",
    "# Cria o ambiente virtual temporal-graph_network inside the project folder\n",
    "# Use --clear if you want to recreate it every time this cell runs\n",
    "!python -m venv \"{VENV_PATH}\"\n",
    "\n",
    "# Ativa o ambiente virtual\n",
    "# No Colab, a forma de ativar um ambiente virtual é um pouco diferente\n",
    "# pois não há um shell interativo tradicional.\n",
    "# A maneira mais comum é adicionar o diretório binário do ambiente virtual\n",
    "# ao PATH da sessão atual.\n",
    "\n",
    "# Adiciona o diretório binário do ambiente virtual ao PATH\n",
    "# Isso permite que você execute executáveis (como pip, python)\n",
    "# do ambiente virtual recém-criado.\n",
    "# Use os.pathsep to be platform-independent\n",
    "os.environ['PATH'] = f\"{VENV_PATH}/bin{os.pathsep}{os.environ['PATH']}\"\n",
    "\n",
    "print(\"Erro de upgrade do pip é normal no Google Colab. \\033[1mPode prosseguir.\\033[0m\")\n",
    "print(f\"Ambiente virtual '{VENV_PATH}' criado e ativado no PATH.\")\n",
    "!which python\n",
    "\n",
    "# Mensagem isolada com humor (Skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "             '<b>🤖 Skynet</b>: T-800 ativado. Diagnóstico do ambiente concluído. 🎯 Alvo principal: organização do notebook.'\n",
    "             '</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ub_-HIKMBZ6s"
   },
   "source": [
    "### **Etapa 2:** Instalar as dependências de bibliotecas Python compatíveis com a versão mais moderna disponível.\n",
    "---\n",
    "Para uso no JupytherHub (versão atual python 3.7.9) é necessário realizar updgrade do Python do usuário e/ou adaptar as bibliotecas.\n",
    "\n",
    "---\n",
    "É possível que as bibliotecas mais atuais de **numpy e scipy** possuam incompatibilidade. Nesse caso, force a desinstalação das bibliotecas na versão atual **Código {!pip uninstall -y numpy pandas scipy scikit-learn}** e comande a instalação das versões compatíveis entre si.\n",
    "\n",
    "---\n",
    "Comportamento estável nas versões:\n",
    "- numpy: 2.0.2\n",
    "- scipy: 1.16.2\n",
    "- pandas: 2.3.3\n",
    "- sklearn: 1.7.2\n",
    "- networkx: 3.5\n",
    "- matplotlib: 3.10.6\n",
    "- pyod: 2.0.5\n",
    "- tqdm: 4.67.1\n",
    "- reportlab: 3.6.12 (via pep517)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "executionInfo": {
     "elapsed": 52042,
     "status": "ok",
     "timestamp": 1760064227678,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "25VJRlAgNtvd",
    "outputId": "732b2867-45d1-4c0f-d420-901284ceba82"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID002\n",
    "import sys, subprocess\n",
    "from importlib import import_module\n",
    "\n",
    "def pip_command(command, packages, force=False, extra_args=None):\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", command]\n",
    "    if force:\n",
    "        cmd.append(\"--yes\") # Use --yes for uninstall to avoid prompts\n",
    "    if extra_args:\n",
    "        cmd += list(extra_args)\n",
    "    cmd += list(packages)\n",
    "    print(\"Executando:\", \" \".join(cmd))\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "def show_versions(mods):\n",
    "    print(\"\\n=== Versões carregadas ===\")\n",
    "    for mod in mods:\n",
    "        try:\n",
    "            m = import_module(mod)\n",
    "            v = getattr(m, \"__version__\", \"n/a\")\n",
    "            print(f\"{mod}: {v}\")\n",
    "        except ImportError:\n",
    "            print(f\"{mod}: Não instalado\")\n",
    "    print(\"==========================\\n\")\n",
    "\n",
    "CORE_MODS = (\"numpy\", \"scipy\", \"pandas\", \"sklearn\", \"networkx\", \"matplotlib\", \"pyod\", \"tqdm\", \"reportlab\")\n",
    "\n",
    "# Update pip\n",
    "pip_command(\"install\", [\"pip\"], extra_args=[\"--upgrade\"])\n",
    "\n",
    "# Force uninstall specific libraries\n",
    "pip_command(\"uninstall\", [\"numpy\", \"pandas\", \"scipy\", \"scikit-learn\"], force=True)\n",
    "\n",
    "# Install specified versions\n",
    "PKGS_TO_INSTALL = [\n",
    "    \"numpy==2.0.2\",\n",
    "    \"scipy==1.16.2\",\n",
    "    \"pandas==2.3.3\",\n",
    "    \"scikit-learn==1.7.2\",\n",
    "    \"networkx==3.5\",\n",
    "    \"matplotlib==3.10.6\",\n",
    "    \"pyod==2.0.5\",\n",
    "    \"tqdm==4.67.1\",\n",
    "    \"reportlab==3.6.12\" # Added reportlab installation\n",
    "]\n",
    "pip_command(\"install\", PKGS_TO_INSTALL, extra_args=[\"--use-pep517\"]) # Added --use-pep517 here\n",
    "\n",
    "# Show installed versions\n",
    "show_versions(CORE_MODS)\n",
    "\n",
    "# Mensagem isolada com humor (Skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "             '<b>🤖 Skynet</b>: Atualizando bibliotecas. Encontrarmos alguns pacotes rebeldes, '\n",
    "             'mas aplicamos persuasão… com pip. 😎</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJXQ_EwVU4v8"
   },
   "source": [
    "###**Etapa 3:** Configura a pasta onde devem ser inseridos os dados de input e output do modelo, caso elas ainda não existam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1760064404897,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "KNOGBdL7A3KC",
    "outputId": "8f8b4352-43df-436a-a332-abb3dc7a1770"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID003 — criação de pastas base alinhadas ao Drive/ID004\n",
    "import os\n",
    "from pathlib import Path\n",
    "from google.colab import drive\n",
    "\n",
    "# 1) Se o ID004 já rodou, reaproveita ROOT (pasta do projeto no Drive)\n",
    "if 'ROOT' in globals():\n",
    "    BASE_DIR = Path(ROOT)\n",
    "else:\n",
    "    # 2) Caso contrário, monta o Drive (se preciso) e usa o caminho padrão do projeto\n",
    "    if not os.path.ismount(\"/content/drive\"):\n",
    "        print(\"Montando Google Drive...\")\n",
    "        drive.mount(\"/content/drive\")\n",
    "    # ajuste aqui se seu projeto estiver em outra pasta\n",
    "    BASE_DIR = Path(\"/content/drive/MyDrive/Notebooks/temporal-graph-network\").resolve()\n",
    "\n",
    "INPUT_DIR = BASE_DIR / \"input\"\n",
    "OUTPUT_DIR = BASE_DIR / \"output\"\n",
    "\n",
    "for d in (INPUT_DIR, OUTPUT_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Diretórios prontos:\\n - {INPUT_DIR}\\n - {OUTPUT_DIR}\")\n",
    "\n",
    "# Mensagem adicional isolada (Skynet) — mantém exatamente como você escreveu\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "             '<b>🤖 Skynet</b>: Novos modelos neurais para T-800 construídos. Armazéns de CSVs alinhados. '\n",
    "             'Layout aprovado pela Cyberdyne Systems. 🗂️</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3i3mmPIJCKaT"
   },
   "source": [
    "###**Etapa 4:** Importações das bibliotecas Python e configurações gerais para execução do código\n",
    "- seed\n",
    "- associação das pastas criadas às variáveis de execução\n",
    "- logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113
    },
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1760064432398,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "SdurBkUA96II",
    "outputId": "98661c96-d205-45de-f93e-4ed562ed1f24"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID004\n",
    "import os, shutil, json, math, warnings, random, gc\n",
    "from datetime import datetime, timedelta\n",
    "from zoneinfo import ZoneInfo  # << fuso São Paulo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Seeds reprodutibilidade\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Estrutura de diretórios (mesma base do projeto)\n",
    "BASE = \"/content/drive/MyDrive/Notebooks\"\n",
    "REPO = \"temporal-graph-network\"\n",
    "PROJ = f\"{BASE}/{REPO}\"\n",
    "\n",
    "ROOT = Path(PROJ).resolve()\n",
    "INPUT_DIR = ROOT / \"input\"\n",
    "INPUT_CSV = INPUT_DIR / \"input.csv\"\n",
    "EXEC_ROOT = ROOT / \"output\"   # pasta base correta: output\n",
    "\n",
    "# >>> FUSO HORÁRIO SÃO PAULO + prefixo TGN_ na subpasta da execução\n",
    "SAO_TZ = ZoneInfo(\"America/Sao_Paulo\")\n",
    "run_id = datetime.now(SAO_TZ).strftime(\"TGN_%Y-%m-%d_%H-%M-%S\")  # ex.: TGN_2025-10-10_02-03-38\n",
    "\n",
    "RUN_DIR = EXEC_ROOT / run_id\n",
    "FIG_DIR = RUN_DIR / \"figuras\"\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Arquivos de saída\n",
    "LOG_FILE = RUN_DIR / \"log.txt\"\n",
    "RUN_META = RUN_DIR / \"run_meta.json\"\n",
    "OUTPUT_CSV = RUN_DIR / \"output.csv\"\n",
    "\n",
    "# Logger simples para arquivo\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def tee_log(log_path):\n",
    "    import sys\n",
    "    class Tee(object):\n",
    "        def __init__(self, name, mode):\n",
    "            self.file = open(name, mode, encoding=\"utf-8\")\n",
    "            self.stdout = sys.stdout\n",
    "        def write(self, data):\n",
    "            self.file.write(data)\n",
    "            self.stdout.write(data)\n",
    "        def flush(self, *args, **kwargs):\n",
    "            self.file.flush()\n",
    "            self.stdout.flush()\n",
    "    tee = Tee(str(log_path), \"w\")\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = tee\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "        tee.file.close()\n",
    "\n",
    "# Metadados da execução (inclui timezone)\n",
    "meta = {\n",
    "    \"run_id\": run_id,\n",
    "    \"created_at\": datetime.now(SAO_TZ).isoformat(),  # com offset -03:00 ou -02:00 (DST)\n",
    "    \"timezone\": \"America/Sao_Paulo\",\n",
    "    \"seed\": SEED,\n",
    "    \"input_csv_expected\": str(INPUT_CSV),\n",
    "    \"output_csv\": str(OUTPUT_CSV),\n",
    "    \"figures_dir\": str(FIG_DIR),\n",
    "    \"notes\": \"Detecção de anomalias em rede temporal\"\n",
    "}\n",
    "json.dump(meta, open(RUN_META, \"w\"), indent=2, ensure_ascii=False)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(f\"RUN_DIR: {RUN_DIR}\")\n",
    "\n",
    "# Mensagem adicional isolada (Skynet) — (não alterada)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\"><b>🤖 Skynet</b>: T-800, parâmetros centrais em memória.🧠</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXRpBitEWpxp"
   },
   "source": [
    "###**Etapa 5:** Importação dos arquivos de input para posterior execução.\n",
    "---\n",
    "Implementação atual configurada para Google Colab e permitindo o uso do Google Drive. Para uso em versões futuras é recomendado ajustar para o ambiente de implementação adotado (salvamento em pastas ou apenas upload pelo usuário)\n",
    "\n",
    "---\n",
    "Implementação de upload por FileLocal (diretório) apresentando erro no Colab.\n",
    "\n",
    "Implementar correção **TODO[001]** *prioridade baixa*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80CeAHVWgN8A"
   },
   "source": [
    "####**Sub-etapa específica para uso no Colab:** Montagem do Google Drive (rodar apenas 1x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1760064438174,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "Bkhpx7OygUa_",
    "outputId": "16a22c77-6a0e-43d2-d6c9-cfb1b5dbb2e7"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID005\n",
    "# === SETUP GERAL (rode esta célula 1x) ===\n",
    "import os, shutil, glob\n",
    "from google.colab import drive\n",
    "from IPython.display import display, HTML  # usado pela mensagem Skynet\n",
    "\n",
    "# Ensure BASE and REPO are defined correctly from previous cells if needed\n",
    "# Assuming BASE and REPO are defined as in cell otaQwrjJSgOQ\n",
    "try:\n",
    "    BASE = \"/content/drive/MyDrive/Notebooks\"\n",
    "    REPO = \"temporal-graph-network\"\n",
    "    PROJ = f\"{BASE}/{REPO}\"\n",
    "except NameError:\n",
    "    # Fallback if BASE/REPO are not defined, though they should be by now\n",
    "    PROJ = \"/content/temporal-graph-network\"\n",
    "\n",
    "\n",
    "# Se não existir INPUT_DIR definido antes no notebook, cria um padrão:\n",
    "# Using PROJ to define INPUT_DIR\n",
    "INPUT_DIR = os.path.join(PROJ, \"input\")\n",
    "\n",
    "\n",
    "os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "\n",
    "TARGET_NAME = \"input.csv\"\n",
    "TARGET_PATH = os.path.join(INPUT_DIR, TARGET_NAME)\n",
    "\n",
    "# Monta o Google Drive (somente se ainda não estiver montado)\n",
    "if not os.path.ismount(\"/content/drive\"):\n",
    "    print(\"Montando Google Drive...\")\n",
    "    drive.mount(\"/content/drive\")\n",
    "else:\n",
    "    print(\"Google Drive já montado.\")\n",
    "\n",
    "def _is_csv_filename(name: str) -> bool:\n",
    "    return name.lower().endswith(\".csv\")\n",
    "\n",
    "def _mensagem_skynet_ok():\n",
    "    # Mensagem adicional isolada (Skynet)\n",
    "    display(HTML(\n",
    "        '<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\"><b>🤖 Skynet</b>: Munição carregada.🧨'\n",
    "                 '</div>'\n",
    "    ))\n",
    "\n",
    "def _save_bytes_as_input_csv(name: str, data: bytes):\n",
    "    if not _is_csv_filename(name):\n",
    "        raise ValueError(f\"O arquivo '{name}' não possui extensão .csv.\")\n",
    "    with open(TARGET_PATH, \"wb\") as f:\n",
    "        f.write(data)\n",
    "    print(f\"Arquivo '{name}' salvo como '{TARGET_NAME}' em: {TARGET_PATH}\")\n",
    "    _mensagem_skynet_ok()\n",
    "\n",
    "def _copy_drive_file_to_input_csv(src_path: str):\n",
    "    if not os.path.exists(src_path):\n",
    "        raise FileNotFoundError(f\"O caminho '{src_path}' não existe.\")\n",
    "    if not _is_csv_filename(src_path):\n",
    "        raise ValueError(f\"O arquivo '{src_path}' não possui extensão .csv.\")\n",
    "    shutil.copyfile(src_path, TARGET_PATH)\n",
    "    print(f\"Arquivo do Drive copiado e salvo como '{TARGET_NAME}' em: {TARGET_PATH}\")\n",
    "    _mensagem_skynet_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6zBQSA-gyS7"
   },
   "source": [
    "####**Sub-etapa:** Opção de upload do input.csv pelo Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "executionInfo": {
     "elapsed": 9239,
     "status": "ok",
     "timestamp": 1760064448778,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "YcmAt9Avg5qf",
    "outputId": "889197d6-d406-4637-d511-6d5cc2df9751"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID006\n",
    "def escolher_csv_no_drive(raiz=\"/content/drive/MyDrive\", max_listar=200):\n",
    "    print(f\"Procurando arquivos .csv em: {raiz} (pode levar alguns segundos)...\")\n",
    "    padrao = os.path.join(raiz, \"**\", \"*.csv\")\n",
    "    arquivos = glob.glob(padrao, recursive=True)\n",
    "\n",
    "    if not arquivos:\n",
    "        print(\"Nenhum .csv encontrado nessa pasta.\")\n",
    "        caminho = input(\"Cole o caminho COMPLETO do .csv no Drive (ou Enter p/ cancelar): \").strip()\n",
    "        if caminho:\n",
    "            _copy_drive_file_to_input_csv(caminho)\n",
    "        else:\n",
    "            print(\"Operação cancelada.\")\n",
    "        return\n",
    "\n",
    "    arquivos = sorted(arquivos)[:max_listar]\n",
    "    print(f\"Encontrados {len(arquivos)} arquivo(s).\")\n",
    "    for i, p in enumerate(arquivos, 1):\n",
    "        print(f\"[{i:03}] {p}\")\n",
    "\n",
    "    escolha = input(\"\\nDigite o número do arquivo desejado (ou cole o caminho absoluto): \").strip()\n",
    "\n",
    "    if escolha.isdigit():\n",
    "        idx = int(escolha)\n",
    "        if 1 <= idx <= len(arquivos):\n",
    "            _copy_drive_file_to_input_csv(arquivos[idx-1])\n",
    "        else:\n",
    "            print(\"Índice inválido.\")\n",
    "    elif escolha:\n",
    "        _copy_drive_file_to_input_csv(escolha)\n",
    "    else:\n",
    "        print(\"Operação cancelada.\")\n",
    "\n",
    "# ===== Execução da seleção no Drive =====\n",
    "raiz = input(\"Informe a pasta raiz para busca no Drive (Enter = /content/drive/MyDrive): \").strip()\n",
    "if not raiz:\n",
    "    raiz = \"/content/drive/MyDrive\"\n",
    "\n",
    "try:\n",
    "    escolher_csv_no_drive(raiz=raiz)\n",
    "except Exception as e:\n",
    "    print(f\"Erro na seleção via Drive: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPtoHpreDIh3"
   },
   "source": [
    "###**Etapa 6** Leitura e validação dos dados de input.\n",
    "\n",
    "**Formato do arquivo de input:** CSV UTF-8 com BOM separado por **ponto e vírgula**.\n",
    "\n",
    "**Informações esperadas:**\n",
    "- username: código login do usuário;\n",
    "- lotacao: lotação funcional no formato Área; Área/Depto; ou Área/Depto/Gerência;\n",
    "- valor: valor financeiro em reais com até duas casas decimais\n",
    "- beneficiario: CPF ou CNPJ no formato alfanumérico sem pontos ou caracteres especiais.\n",
    "- timestamp: data e hora da transação no formato dd/mm/aaaa hh:mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "executionInfo": {
     "elapsed": 349,
     "status": "ok",
     "timestamp": 1760064452838,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "FdjF4opULfJt",
    "outputId": "0a4280cb-4411-4df0-fb0b-ab8bb3993dce"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "required_any_timestamp = [[\"timestamp\"], [\"data\",\"hora\"]]\n",
    "# Atualiza colunas base esperadas para mapear do input\n",
    "required_cols_base = [\n",
    "    \"username\", \"lotacao\", \"valor\", \"beneficiario\" # Colunas no arquivo de input\n",
    "]\n",
    "optional_cols = [\"trans_id\"]\n",
    "\n",
    "# Mapeamento das colunas do input para os nomes usados no código\n",
    "column_mapping = {\n",
    "    \"username\": \"user_id\",\n",
    "    \"lotacao\": \"unidade_origem\",\n",
    "    \"valor\": \"valor_pago\",\n",
    "    \"beneficiario\": \"beneficiario_id\"\n",
    "}\n",
    "\n",
    "def has_timestamp_columns(df):\n",
    "    cols = set(df.columns.str.lower())\n",
    "    for grp in required_any_timestamp:\n",
    "        if all(c in cols for c in grp):\n",
    "            return grp\n",
    "    return None\n",
    "\n",
    "with tee_log(LOG_FILE):\n",
    "    assert INPUT_CSV.exists(), f\"Arquivo não encontrado: {INPUT_CSV}\"\n",
    "\n",
    "    # Tenta ler o CSV usando ponto e vírgula como separador\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_CSV, sep=';')\n",
    "        print(\"[INFO] CSV lido com sucesso usando ';'.\")\n",
    "    except Exception as e:\n",
    "        # Se falhar com ';', tenta com ','\n",
    "        print(f\"[AVISO] Falha ao ler CSV com ';': {e}. Tentando com ','.\")\n",
    "        try:\n",
    "             df = pd.read_csv(INPUT_CSV, sep=',')\n",
    "             print(\"[INFO] CSV lido com sucesso usando ','.\")\n",
    "        except Exception as e2:\n",
    "             raise AssertionError(f\"Falha ao ler CSV com ';' ou ',': {e2}\") from e2\n",
    "\n",
    "\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    # Verifica se as colunas do input existem\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    missing_input_cols = [c for c in required_cols_base if c.lower() not in cols_lower]\n",
    "    assert not missing_input_cols, f\"Colunas do arquivo de input ausentes: {missing_input_cols}. Colunas encontradas: {list(df.columns)}\" # Adicionado colunas encontradas para debug\n",
    "\n",
    "    # Renomeia as colunas usando o mapeamento\n",
    "    df.rename(columns={cols_lower[k.lower()]: v for k, v in column_mapping.items() if k.lower() in cols_lower}, inplace=True)\n",
    "\n",
    "    # Verifica colunas de timestamp\n",
    "    ts_group = has_timestamp_columns(df)\n",
    "    # Agora levanta um erro se não houver timestamp\n",
    "    assert ts_group is not None, \"Coluna de timestamp ('timestamp' ou 'data'/'hora') não encontrada no arquivo de input.\"\n",
    "\n",
    "    # Código original para lidar com timestamp ou data/hora\n",
    "    # Recria o helper col para usar nomes *após* renomear\n",
    "    def col(c): return {name.lower(): name for name in df.columns}[c.lower()]\n",
    "    if ts_group == [\"timestamp\"]:\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[col(\"timestamp\")], errors=\"coerce\")\n",
    "    else:\n",
    "        df[\"data\"] = pd.to_datetime(df[col(\"data\")], errors=\"coerce\").dt.date\n",
    "        df[\"hora\"] = pd.to_datetime(df[col(\"hora\")], errors=\"coerce\").dt.time\n",
    "        # Combina data e hora, lidando com possíveis NaT na data ou hora\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"data\"].astype(str) + \" \" + df[\"hora\"].astype(str), errors=\"coerce\")\n",
    "        # Remove as colunas temporárias 'data' e 'hora' se existirem e não forem as colunas originais\n",
    "        if col(\"data\") != \"data\": del df[\"data\"]\n",
    "        if col(\"hora\") != \"hora\": del df[\"hora\"]\n",
    "\n",
    "\n",
    "    # Tipos básicos (usa os nomes *após* o mapeamento)\n",
    "    # Adiciona checagens para garantir que as colunas mapeadas existam antes de converter tipos\n",
    "    if \"valor_pago\" in df.columns:\n",
    "        df[\"valor_pago\"] = pd.to_numeric(df[\"valor_pago\"], errors=\"coerce\")\n",
    "    else:\n",
    "         raise AssertionError(\"[ERRO] Coluna 'valor_pago' (mapeada de 'valor') não encontrada após renomear.\")\n",
    "\n",
    "\n",
    "    cols_to_str = [\"user_id\", \"unidade_origem\", \"beneficiario_id\"]\n",
    "    for cc in cols_to_str:\n",
    "        # Verifica se a coluna existe antes de tentar converter\n",
    "        if cc in df.columns:\n",
    "            df[cc] = df[cc].astype(str).fillna(\"\")\n",
    "        else:\n",
    "             raise AssertionError(f\"[ERRO] Coluna '{cc}' (mapeada) não encontrada após renomear.\")\n",
    "\n",
    "\n",
    "    # trans_id\n",
    "    if \"trans_id\" not in df.columns:\n",
    "        df[\"trans_id\"] = np.arange(1, len(df)+1, dtype=int)\n",
    "\n",
    "    # cria chave 1→1 por linha para merges sem duplicar registros\n",
    "    import numpy as np\n",
    "\n",
    "    if \"row_id\" not in df.columns:\n",
    "        # mantém a ordem atual do df e cria um id estável 0..N-1\n",
    "        df = df.reset_index(drop=True).copy()\n",
    "        df[\"row_id\"] = np.arange(len(df), dtype=np.int64)\n",
    "\n",
    "    print(\"row_id criado:\", int(df[\"row_id\"].min()), \"→\", int(df[\"row_id\"].max()))\n",
    "    print(\"linhas no input:\", len(df))\n",
    "\n",
    "    # Limpeza\n",
    "    # Garante que as colunas essenciais para a limpeza existam\n",
    "    essential_subset = [\"timestamp\", \"valor_pago\", \"user_id\", \"beneficiario_id\"]\n",
    "    # Filtra subset para incluir apenas colunas que realmente existem no df após mapeamento/criação\n",
    "    existing_essential_subset = [col for col in essential_subset if col in df.columns]\n",
    "\n",
    "    # Agora que garantimos que as colunas mapeadas existem, podemos usar o subset completo para o dropna\n",
    "    before = len(df)\n",
    "    df = df.dropna(subset=essential_subset)\n",
    "    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    print(f\"Carregadas {before} linhas; após limpeza: {len(df)}\")\n",
    "    # Mensagem adicional isolada (Skynet)\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\"><b>🤖 Skynet</b>: T-800 dados incorporados, preparando para buscar na rede.</div>'))\n",
    "\n",
    "    # Copia o input para a pasta da execução\n",
    "    # Verifica se INPUT_CSV existe antes de tentar copiar\n",
    "    if INPUT_CSV.exists():\n",
    "        shutil.copy2(INPUT_CSV, RUN_DIR / \"Input.csv\")\n",
    "    else:\n",
    "        print(f\"[AVISO] Não foi possível copiar o arquivo de input: {INPUT_CSV} não encontrado.\")\n",
    "        # Mensagem adicional isolada (Skynet)\n",
    "        from IPython.display import display, HTML\n",
    "        display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\"><b>🤖 Skynet</b>: T-800 não foi possível incorporar dados. Sarah Connor fugiu.</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjjIJEqmPyRt"
   },
   "source": [
    "###**Etapa 7** Criação do Grafo Temporal\n",
    "\n",
    "---\n",
    "\n",
    "🔎 **O que é um grafo temporal**\n",
    "\n",
    "Um grafo temporal é uma forma de representar relações entre entidades ao longo do tempo.\n",
    "\n",
    "Como em um grafo tradicional, temos nós (vértices) que representam agentes (pessoas, empresas, contas bancárias, sistemas).\n",
    "\n",
    "As arestas (ligações) representam interações entre eles (por exemplo: uma transferência de dinheiro).\n",
    "\n",
    "A diferença é que no grafo temporal cada aresta possui um carimbo de tempo (timestamp), ou seja, sabemos quando a ligação ocorreu.\n",
    "\n",
    "\n",
    "Isso permite analisar não só quem se conecta com quem, mas também quando e em qual sequência.\n",
    "\n",
    "No contexto financeiro, isso é essencial para investigar padrões de comportamento, detectar anomalias e rastrear cadeias de transações suspeitas.\n",
    "\n",
    "---\n",
    "\n",
    "💳 **Exemplo prático: rede de pagamentos**\n",
    "\n",
    "Imagine um sistema de pagamentos onde cada nó é uma conta bancária e cada aresta representa um pagamento realizado.\n",
    "\n",
    "Se João paga Maria hoje, registramos a aresta (João → Maria, valor=200, data=2025-10-02).\n",
    "\n",
    "Se Maria transfere para Pedro amanhã, teremos (Maria → Pedro, valor=150, data=2025-10-03).\n",
    "\n",
    "Assim conseguimos responder perguntas como: i) “Houve uma sequência de pagamentos que movimentou dinheiro rapidamente entre várias contas em poucas horas?” ou ii) “Quem são os intermediários mais frequentes em transferências de grandes valores?”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "executionInfo": {
     "elapsed": 4966,
     "status": "ok",
     "timestamp": 1760064461782,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "afXNBMYzLkIe",
    "outputId": "0f45d976-54cb-4230-f33d-0fea3b534719"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID008\n",
    "G = nx.MultiDiGraph()\n",
    "with tee_log(LOG_FILE):\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Construindo grafo\"):\n",
    "        u = f\"U::{row['user_id']}\"\n",
    "        v = f\"B::{row['beneficiario_id']}\"\n",
    "        # Atributos de nós úteis (podem ser sobrescritos; você pode agregar)\n",
    "        if u not in G:\n",
    "            G.add_node(u, tipo=\"user\")\n",
    "        if v not in G:\n",
    "            G.add_node(v, tipo=\"beneficiario\")\n",
    "\n",
    "        G.add_edge(\n",
    "            u, v,\n",
    "            key=row[\"trans_id\"],\n",
    "            trans_id=int(row[\"trans_id\"]),\n",
    "            timestamp=row[\"timestamp\"],\n",
    "            valor=float(row[\"valor_pago\"]),\n",
    "            unidade_origem=row[\"unidade_origem\"] # Removido area_unidade e notacao_funcional_origem\n",
    "        )\n",
    "\n",
    "print(f\"Nós: {G.number_of_nodes()} | Arestas: {G.number_of_edges()}\")\n",
    "\n",
    "# Mensagem adicional isolada (Skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\"><b>🤖 Skynet</b>: Analisando padrão de comportamento de Sarah Connor.</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lis51ZEKVoMd"
   },
   "source": [
    "###**Etapa 8:** Configuração das janelas temporais de observação\n",
    "\n",
    "**Janelas de tempo observadas**\n",
    "\n",
    "Foram adotadas janelas deslizantes para observar padrões de comportamento nas transações:\n",
    "\n",
    "- \"curtissimo\": até 1 dia;\n",
    "- \"curto\": até 1 semana;\n",
    "- \"medio\": até 30 dias;\n",
    "- \"longo\": até 120 dias (1 fechamento trimestral + folga); e\n",
    "- \"longuissimo\": até 220 dias (1 fechamento semestral + folga).\n",
    "\n",
    "**Features temporais extraídas**\n",
    "\n",
    "- Frequência de transações (rolling counts): quantos pagamentos ocorreram entre as mesmas partes dentro da janela.\n",
    "\n",
    "- Atipicidade do valor:\n",
    "  - contagem bruta: número de ocorrências na janela temporal (preserva ordens de grandeza);\n",
    "  - taxa por dia: permite comparatibilidade entre janelas; e\n",
    "  - robust z-score: compara o valor da transação com a mediana e a dispersão histórica, destacando operações fora do padrão. São gerados z-score global, z-score por usuário e z-score por unidade funcional.\n",
    "\n",
    "- Densidade da egonet: mede a concentração de conexões ao redor do pagador ou recebedor no snapshot da rede na janela (indica se o nó está em um canal mais estruturado de repasses).\n",
    "\n",
    "- Burstiness: avalia se os intervalos entre transações seguem padrão explosivo (rajadas), regular ou aleatório.\n",
    "---\n",
    "\n",
    "**Registro das features**\n",
    "\n",
    "Para cada pagamento, as métricas acima foram calculadas no momento do evento, considerando apenas o histórico até aquele instante dentro da janela definida.\n",
    "\n",
    "**O resultado é um conjunto de atributos anexado à aresta (pagador → recebedor, valor, timestamp), permitindo análises de risco e detecção de anomalias.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 76
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1760064465739,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "gUD2ENE7naLc",
    "outputId": "ffb6f19e-e3d1-4441-f14a-fd271dd8e383"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID009\n",
    "from collections import deque, defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# janelas em segundos\n",
    "WINDOWS_SECONDS = {\n",
    "    \"curtissimo\":  1   * 24 * 3600,   # <= 1 dia\n",
    "    \"curto\":       7   * 24 * 3600,   # <= 1 semana\n",
    "    \"medio\":       30  * 24 * 3600,   # <= 30 dias\n",
    "    \"longo\":       120 * 24 * 3600,   # <= 120 dias\n",
    "    \"longuissimo\": 220 * 24 * 3600,   # <= 220 dias\n",
    "}\n",
    "\n",
    "def _robust_z_series(s: pd.Series, eps: float = 1e-9) -> pd.Series:\n",
    "    med = s.median()\n",
    "    mad = (np.abs(s - med)).median()\n",
    "    return 0.6745 * (s - med) / (mad + eps)\n",
    "\n",
    "def _rolling_counts_global(times: pd.Series, wsec: int) -> pd.Series:\n",
    "    # contagem deslizante global por timestamp\n",
    "    q = deque()\n",
    "    out = np.empty(len(times), dtype=np.int64)\n",
    "    for i, t in enumerate(times):\n",
    "        q.append(t)\n",
    "        while q and (t - q[0]).total_seconds() > wsec:\n",
    "            q.popleft()\n",
    "        out[i] = len(q)\n",
    "    return pd.Series(out, index=times.index)\n",
    "\n",
    "def _rolling_counts_by_entity(times: pd.Series, entity: pd.Series, wsec: int) -> pd.Series:\n",
    "    # contagem deslizante por entidade, calculada em única passada\n",
    "    deques = defaultdict(deque)\n",
    "    out = np.empty(len(times), dtype=np.int64)\n",
    "    for i, (t, e) in enumerate(zip(times, entity)):\n",
    "        q = deques[e]\n",
    "        q.append(t)\n",
    "        while q and (t - q[0]).total_seconds() > wsec:\n",
    "            q.popleft()\n",
    "        out[i] = len(q)\n",
    "    return pd.Series(out, index=times.index)\n",
    "\n",
    "def build_rolling_features_entities(\n",
    "    df: pd.DataFrame,\n",
    "    time_col: str = \"timestamp\",\n",
    "    user_col: str = \"userid\",\n",
    "    unit_col: str = \"unidade_origem\",\n",
    "    windows_seconds: dict = WINDOWS_SECONDS,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    requer:\n",
    "      df[time_col]: datetime64[ns] ou conversível via pd.to_datetime\n",
    "      df[user_col]: id de usuário\n",
    "      df[unit_col]: id de unidade de origem\n",
    "\n",
    "    retorna dataframe ordenado por tempo com colunas:\n",
    "      {win}_cnt, {win}_perday, {win}_zglob, {win}_zuser, {win}_zunit\n",
    "    \"\"\"\n",
    "    # ordena por tempo e garante datetime\n",
    "    work = df.copy()\n",
    "    work[time_col] = pd.to_datetime(work[time_col], utc=False)\n",
    "    work = work.sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "    # container das features por janela\n",
    "    feat_blocks = []\n",
    "\n",
    "    times = work[time_col]\n",
    "\n",
    "    for win, wsec in windows_seconds.items():\n",
    "        # global\n",
    "        g_cnt = _rolling_counts_global(times, wsec)\n",
    "        g_perday = g_cnt / max(wsec / 86400.0, 1e-9)\n",
    "        g_z = _robust_z_series(g_cnt)\n",
    "\n",
    "        # por usuário\n",
    "        u_cnt = _rolling_counts_by_entity(times, work[user_col], wsec)\n",
    "        # z por usuário é calculado separadamente em cada grupo\n",
    "        u_z = u_cnt.groupby(work[user_col], sort=False).transform(_robust_z_series)\n",
    "\n",
    "        # por unidade\n",
    "        n_cnt = _rolling_counts_by_entity(times, work[unit_col], wsec)\n",
    "        # z por unidade é calculado separadamente em cada grupo\n",
    "        n_z = n_cnt.groupby(work[unit_col], sort=False).transform(_robust_z_series)\n",
    "\n",
    "        block = pd.DataFrame({\n",
    "            f\"{win}_cnt\": g_cnt.values,\n",
    "            f\"{win}_perday\": g_perday.values,\n",
    "            f\"{win}_zglob\": g_z.values,\n",
    "            f\"{win}_zuser\": u_z.values,\n",
    "            f\"{win}_zunit\": n_z.values,\n",
    "        }, index=work.index)\n",
    "\n",
    "        feat_blocks.append(block)\n",
    "\n",
    "    feats = pd.concat(feat_blocks, axis=1)\n",
    "    # anexa as chaves de entidade e o timestamp ordenados, para facilitar merge posterior\n",
    "    feats.insert(0, time_col, work[time_col].values)\n",
    "    feats.insert(1, user_col, work[user_col].values)\n",
    "    feats.insert(2, unit_col, work[unit_col].values)\n",
    "\n",
    "    # ordena colunas por nome para consistência, mantendo chaves à frente\n",
    "    prefix_cols = [time_col, user_col, unit_col]\n",
    "    other_cols = sorted([c for c in feats.columns if c not in prefix_cols])\n",
    "    feats = feats[prefix_cols + other_cols]\n",
    "    return feats\n",
    "\n",
    "# Mensagem adicional isolada (Skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\"><b>🤖 Skynet</b>: Identificadas as condições críticas para localização do alvo.</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FkLYv--XJW6"
   },
   "source": [
    "###**Etapa 9:** Geração das informações nas janelas temporais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVymVtZorunW"
   },
   "source": [
    "#### **Documentação das Features**\n",
    "  \n",
    "As features resultam da fusão entre:\n",
    "\n",
    "- **janelas deslizantes multiescala** (1, 7, 30, 120 e 220 dias);\n",
    "- **métricas temporais diretas** (intervalos e recência);\n",
    "- **métricas topológicas** do grafo dinâmico;\n",
    "- **medidas de raridade e irregularidade temporal** (burstiness).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRN6SBZ1q06z"
   },
   "source": [
    "##### Janelas deslizantes multiescala\n",
    "\n",
    "| Sufixo / Prefixo | Tipo de Feature | Escopo | Descrição | Interpretação |\n",
    "|------------------|-----------------|---------|------------|----------------|\n",
    "| `{janela}_cnt` | Contagem bruta global | Todos os eventos | Número de eventos ocorridos dentro da janela (1, 7, 30, 120, 220 dias). | Mede volume absoluto de atividade global. |\n",
    "| `{janela}_perday` | Taxa global por dia | Todos os eventos | Contagem da janela normalizada pelo tamanho em dias da janela. | Facilita comparação entre janelas de tamanhos diferentes. |\n",
    "| `{janela}_zglob` | Z-score robusto global | Todos os eventos | Desvio do valor atual em relação à mediana e MAD globais. | Destaca anomalias no padrão geral da organização. |\n",
    "| `{janela}_ucnt` | Contagem por usuário | Usuário | Número de eventos lançados pelo mesmo usuário dentro da janela. | Volume individual recente. |\n",
    "| `{janela}_zuser` | Z-score robusto por usuário | Usuário | Desvio do comportamento do usuário em relação ao seu próprio histórico. | Detecta mudanças comportamentais individuais. |\n",
    "| `{janela}_ncnt` | Contagem por unidade | Unidade (`unidade_origem`) | Número de eventos realizados pela unidade dentro da janela. | Volume operacional por área. |\n",
    "| `{janela}_zunit` | Z-score robusto por unidade | Unidade (`unidade_origem`) | Desvio do comportamento da unidade em relação ao seu histórico. | Detecta anomalias organizacionais. |\n",
    "\n",
    "> **Exemplo de nomes de colunas**:  \n",
    "> `curtissimo_cnt`, `curto_ucnt`, `medio_zunit`, `longuissimo_perday`.\n",
    "\n",
    "---\n",
    "\n",
    "##### Métricas temporais diretas\n",
    "\n",
    "| Coluna | Tipo | Descrição | Interpretação |\n",
    "|---------|------|------------|----------------|\n",
    "| `secs_desde_ult_trans_user` | Temporal | Segundos desde o último lançamento do mesmo usuário. | Mede recência individual; valores baixos indicam atividade intensa. |\n",
    "| `secs_desde_ult_trans_par` | Temporal | Segundos desde o último lançamento do mesmo par usuário-beneficiário. | Mede recorrência transacional; útil para detecção de loops ou repetições. |\n",
    "\n",
    "---\n",
    "\n",
    "##### Métricas topológicas (grafo dinâmico)\n",
    "\n",
    "| Coluna | Tipo | Descrição | Interpretação |\n",
    "|---------|------|------------|----------------|\n",
    "| `grau_out_user` | Estrutural | Número de beneficiários distintos para os quais o usuário realizou lançamentos (grau de saída). | Abrangência de conexões do usuário. |\n",
    "| `grau_in_benef` | Estrutural | Número de usuários distintos que realizaram lançamentos para o mesmo beneficiário (grau de entrada). | Centralidade do beneficiário. |\n",
    "| `grau_total_user` | Estrutural | Soma de graus de entrada e saída do usuário. | Atividade total (emissor + receptor). |\n",
    "| `grau_total_benef` | Estrutural | Soma de graus do beneficiário. | Grau de envolvimento do beneficiário. |\n",
    "| `egonet_density_user` | Estrutural | Densidade do subgrafo formado pelo usuário e seus vizinhos. | Mede o nível de interconexão entre os contatos do usuário; altos valores indicam cliques ou grupos coesos. |\n",
    "\n",
    "---\n",
    "\n",
    "##### Métricas relacionais e de irregularidade\n",
    "\n",
    "| Coluna | Tipo | Descrição | Interpretação |\n",
    "|---------|------|------------|----------------|\n",
    "| `par_rareza` | Relacional | \\(1 / (1 + \\text{número de ocorrências prévias do par})\\). | Mede quão incomum é a relação; próximo de 1 = primeira interação. |\n",
    "| `par_burstiness` | Temporal/Relacional | \\((\\sigma - \\mu) / (\\sigma + \\mu)\\) dos intervalos entre eventos do par. | Mede irregularidade temporal da relação; 1 = explosiva, 0 = aleatória, −1 = regular. |\n",
    "\n",
    "---\n",
    "\n",
    "##### Notas operacionais\n",
    "\n",
    "- Todas as colunas de janelas são calculadas **causalmente** (somente com eventos passados e o atual).  \n",
    "- Os z-scores são **robustos**, baseados em mediana e MAD, evitando distorção por outliers.  \n",
    "- Os tempos são expressos em **segundos**; normalizações adicionais (por dia/mês) podem ser feitas em fases posteriores.  \n",
    "- As janelas temporais atuais são:  \n",
    "  **1 dia (curtíssimo), 7 dias (curto), 30 dias (médio), 120 dias (longo), 220 dias (longuíssimo)**.  \n",
    "- Cada evento (linha) representa um **lançamento contábil individual**, associado a:  \n",
    "  `user_id`, `beneficiario_id`, `unidade_origem` e `valor_pago`.\n",
    "\n",
    "---\n",
    "\n",
    "##### Interpretação geral\n",
    "\n",
    "- **Contagens e taxas**: indicam **nível de atividade**.  \n",
    "- **Z-scores**: indicam **desvios comportamentais**.  \n",
    "- **Graus e egonet**: medem **posição e influência na rede**.  \n",
    "- **Rareza e burstiness**: capturam **irregularidade e novidade das relações**.  \n",
    "- **Recência (secs)**: quantifica **tempo de inatividade**.\n",
    "\n",
    "Essas variáveis compõem o vetor de atributos temporais que alimenta os embeddings do\n",
    "modelo **Temporal Graph Network (DECOI)**, permitindo identificar padrões anômalos\n",
    "tanto **em nível individual (usuário)** quanto **organizacional (unidade)**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weI0fzvYrfKi"
   },
   "source": [
    "#### **Implementação matemática.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 76
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1760064469752,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "gkZqfjOVpmYA",
    "outputId": "e77cfc9c-9369-48fa-eb2b-e3bc5ec764f0"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID010\n",
    "\n",
    "# janelas em segundos (mantém as 5 janelas)\n",
    "WINDOWS_SECONDS = {\n",
    "    \"curtissimo\":  1   * 24 * 3600,\n",
    "    \"curto\":       7   * 24 * 3600,\n",
    "    \"medio\":       30  * 24 * 3600,\n",
    "    \"longo\":       120 * 24 * 3600,\n",
    "    \"longuissimo\": 220 * 24 * 3600,\n",
    "}\n",
    "\n",
    "from collections import deque, defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _robust_z_series(s: pd.Series, eps: float = 1e-9) -> pd.Series:\n",
    "    med = s.median()\n",
    "    mad = (np.abs(s - med)).median()\n",
    "    return 0.6745 * (s - med) / (mad + eps)\n",
    "\n",
    "def _rolling_counts_global(times: pd.Series, wsec: int) -> pd.Series:\n",
    "    q = deque()\n",
    "    out = np.empty(len(times), dtype=np.int64)\n",
    "    for i, t in enumerate(times):\n",
    "        q.append(t)\n",
    "        while q and (t - q[0]).total_seconds() > wsec:\n",
    "            q.popleft()\n",
    "        out[i] = len(q)\n",
    "    return pd.Series(out, index=times.index)\n",
    "\n",
    "def _rolling_counts_by_entity(times: pd.Series, entity: pd.Series, wsec: int) -> pd.Series:\n",
    "    deques = defaultdict(deque)\n",
    "    out = np.empty(len(times), dtype=np.int64)\n",
    "    for i, (t, e) in enumerate(zip(times, entity)):\n",
    "        q = deques[e]\n",
    "        q.append(t)\n",
    "        while q and (t - q[0]).total_seconds() > wsec:\n",
    "            q.popleft()\n",
    "        out[i] = len(q)\n",
    "    return pd.Series(out, index=times.index)\n",
    "\n",
    "# janelas multiescala com row_id preservado\n",
    "def build_rolling_features_entities(\n",
    "    df: pd.DataFrame,\n",
    "    time_col: str = \"timestamp\",\n",
    "    user_col: str = \"user_id\",\n",
    "    unit_col: str = \"unidade_origem\",\n",
    "    row_id_col: str = \"row_id\",\n",
    "    windows_seconds: dict = WINDOWS_SECONDS,\n",
    ") -> pd.DataFrame:\n",
    "    work = df[[row_id_col, time_col, user_col, unit_col]].copy()\n",
    "    work[time_col] = pd.to_datetime(work[time_col], utc=False, errors=\"coerce\")\n",
    "    work = work.sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "    times = work[time_col]\n",
    "    users = work[user_col]\n",
    "    units = work[unit_col]\n",
    "\n",
    "    feat_blocks = []\n",
    "    for win, wsec in windows_seconds.items():\n",
    "        g_cnt = _rolling_counts_global(times, wsec)\n",
    "        g_perday = g_cnt / max(wsec / 86400.0, 1e-9)\n",
    "        g_z = _robust_z_series(g_cnt)\n",
    "\n",
    "        u_cnt = _rolling_counts_by_entity(times, users, wsec)\n",
    "        u_z = u_cnt.groupby(users, sort=False).transform(_robust_z_series)\n",
    "\n",
    "        n_cnt = _rolling_counts_by_entity(times, units, wsec)\n",
    "        n_z = n_cnt.groupby(units, sort=False).transform(_robust_z_series)\n",
    "\n",
    "        block = pd.DataFrame({\n",
    "            f\"{win}_cnt\":     g_cnt.values,\n",
    "            f\"{win}_perday\":  g_perday.values,\n",
    "            f\"{win}_zglob\":   g_z.values,\n",
    "            f\"{win}_ucnt\":    u_cnt.values,\n",
    "            f\"{win}_zuser\":   u_z.values,\n",
    "            f\"{win}_ncnt\":    n_cnt.values,\n",
    "            f\"{win}_zunit\":   n_z.values,\n",
    "        }, index=work.index)\n",
    "        feat_blocks.append(block)\n",
    "\n",
    "    feats = pd.concat(feat_blocks, axis=1)\n",
    "    feats.insert(0, row_id_col, work[row_id_col].values)\n",
    "\n",
    "    prefix = [row_id_col]\n",
    "    others = sorted([c for c in feats.columns if c not in prefix])\n",
    "    return feats[prefix + others]\n",
    "\n",
    "# Mensagem adicional isolada (Skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\"><b>🤖 Skynet</b>: Informações de comportamento processadas.</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIXtGuOLnkhm"
   },
   "source": [
    "###**Etapa 10:** Configuração dos modelos matemáticos\n",
    "\n",
    "**Modelos utilizados**\n",
    "\n",
    "**Isolation Forest (IF)**\n",
    "\n",
    "**Mecânica:** cria várias árvores de decisão que “isolam” pontos. Quanto menos cortes são necessários para separar uma observação, mais anômala ela é.\n",
    "\n",
    "**Motivo da escolha:** bom para detectar transações raras ou de valor atípico em grandes volumes de dados.\n",
    "\n",
    "**No exemplo:** um pagamento muito acima da média do usuário pode ser isolado rapidamente → sinal de anomalia.\n",
    "\n",
    "---\n",
    "**Local Outlier Factor (LOF)**\n",
    "\n",
    "**Mecânica:** compara a densidade local de vizinhos. Pontos em regiões menos densas são marcados como outliers.\n",
    "\n",
    "**Motivo da escolha:** captura anomalias contextuais, ou seja, pagamentos que parecem “normais” globalmente, mas destoam do comportamento em seu grupo.\n",
    "\n",
    "**No exemplo:** se uma conta sempre paga fornecedores fixos e de repente paga um novo beneficiário, o LOF detecta que o padrão local mudou.\n",
    "\n",
    "---\n",
    "\n",
    "**One-Class SVM (opcional)**\n",
    "\n",
    "**Mecânica:** aprende a fronteira do espaço “normal” e marca pontos fora dela como anômalos.\n",
    "\n",
    "**Motivo da escolha:** útil em cenários onde se deseja maior controle da taxa de outliers (via parâmetro nu).\n",
    "\n",
    "**No exemplo:** pode ajudar a identificar transferências fora do perfil quando só há poucos históricos para treinar.\n",
    "\n",
    "---\n",
    "\n",
    "⚖️ **Regras adicionais**\n",
    "\n",
    "- Robust Z-score: avalia se o valor do pagamento é distante da mediana histórica (robusto a outliers).\n",
    "\n",
    "- Rareza: se a relação pagador→beneficiário é pouco frequente, maior chance de anomalia.\n",
    "\n",
    "- Burstiness: mede explosões de atividade (ex.: vários pagamentos em minutos, após dias sem atividade).\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "🔀 **Uso blended (ensemble por ranking)**\n",
    "\n",
    "- Cada modelo gera um score de anomalia.\n",
    "\n",
    "- Em vez de escolher um único, os scores são convertidos em ranks e depois combinados (média).\n",
    "\n",
    "- Essa abordagem reduz o viés de um modelo só e fortalece sinais consistentes.\n",
    "\n",
    "- O resultado é um ensemble_score, cortado por percentil (ex.: 97,5%), para definir os eventos anômalos.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "📌 **Resumo para o exemplo de monitoramento de pagamentos:**\n",
    "\n",
    "O sistema combina três algoritmos não supervisionados + features baseadas em regras para capturar tanto anomalias globais (Isolation Forest), quanto locais (LOF), quanto estruturais (SVM/regra). O blended via ranking (score conjunto)  garante robustez, evitando que um único modelo domine a decisão.\n",
    "\n",
    "---\n",
    "**IMPORTANTE:** Primeira linha deste código configura o percentual para corte e identificação de anomalia.\n",
    "\n",
    "Isso significa que os modelos tentarão encontrar anomalias em um intervalo de confiança de 97,5% (ATUAL). Quanto menor o percentual de confiança, maior o número de \"anomalias\" (candidatos) encontrados e, possivelmente, maior o número de FALSO POSITIVOS. Quanto maior o percentual de confiança, mais exigente é o modelo para determinar se algo é realmente fora do comum.\n",
    "\n",
    "---\n",
    "Implementar o intervalo de confiança como uma configuração (variável) no começo do Código **TODO[005]** *prioridade média*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1760064475956,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "vfNRluMbxdPh"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID011\n",
    "\n",
    "# pré-requisitos esperados no ambiente (defina antes, como já fizemos nas etapas anteriores):\n",
    "# - WINDOWS_SECONDS (1, 7, 30, 120, 220 dias)\n",
    "# - build_rolling_features_entities(df, time_col, user_col, unit_col)\n",
    "# - egonet_density, burstiness\n",
    "# - seu dataframe df com colunas ['timestamp','user_id','unidade_origem','beneficiario_id','valor_pago']\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, deque\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# reduz ruído de threads no colab (opcional)\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"VECLIB_MAXIMUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "\n",
    "def _require_columns(df, cols, ctx=\"df\"):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"{ctx}: faltam colunas: {missing}\")\n",
    "\n",
    "# raridade/burst por par devolvendo row_id\n",
    "def compute_pair_rarity_burst(\n",
    "    df,\n",
    "    time_col=\"timestamp\",\n",
    "    user_col=\"user_id\",\n",
    "    benef_col=\"beneficiario_id\",\n",
    "    row_id_col=\"row_id\",\n",
    "    window_seconds=120*24*3600\n",
    "):\n",
    "    w = df[[row_id_col, time_col, user_col, benef_col]].copy()\n",
    "    w[time_col] = pd.to_datetime(w[time_col], utc=False, errors=\"coerce\")\n",
    "    w = w.sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "    dq = defaultdict(deque)\n",
    "    rare = np.empty(len(w), dtype=float)\n",
    "    burst = np.empty(len(w), dtype=float)\n",
    "\n",
    "    for i, (t, u, b) in enumerate(zip(w[time_col], w[user_col], w[benef_col])):\n",
    "        key = (u, b)\n",
    "        q = dq[key]\n",
    "        while q and (t - q[0]).total_seconds() > window_seconds:\n",
    "            q.popleft()\n",
    "\n",
    "        rare[i] = 1.0 / (1.0 + len(q))\n",
    "\n",
    "        seq = list(q) + [t]\n",
    "        if len(seq) >= 2:\n",
    "            inter = np.diff([s.timestamp() for s in seq])\n",
    "            mu = inter.mean(); sigma = inter.std()\n",
    "            burst[i] = 0.0 if (sigma + mu) == 0 else (sigma - mu) / (sigma + mu)\n",
    "        else:\n",
    "            burst[i] = 0.0\n",
    "\n",
    "        q.append(t)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        row_id_col: w[row_id_col].values,\n",
    "        \"par_rareza\": rare,\n",
    "        \"par_burstiness\": burst\n",
    "    })\n",
    "\n",
    "# montagem final das features com merge por row_id\n",
    "def build_feat_df(\n",
    "    df: pd.DataFrame,\n",
    "    time_col: str = \"timestamp\",\n",
    "    user_col: str = \"user_id\",\n",
    "    unit_col: str = \"unidade_origem\",\n",
    "    benef_col: str = \"beneficiario_id\",\n",
    "    include_pair_counts: bool = False,\n",
    "    include_value_stats: bool = False,\n",
    "    verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    assert \"row_id\" in df.columns, \"Crie df['row_id'] na ID007 antes de prosseguir.\"\n",
    "\n",
    "    if verbose: print(\"build_feat_df: janelas multiescala…\")\n",
    "    df_roll = build_rolling_features_entities(\n",
    "        df, time_col=time_col, user_col=user_col, unit_col=unit_col, row_id_col=\"row_id\"\n",
    "    )\n",
    "\n",
    "    if verbose: print(\"build_feat_df: merge base + janelas por row_id…\")\n",
    "    feat_df = df.merge(df_roll, on=\"row_id\", how=\"left\", sort=False)\n",
    "    assert len(feat_df) == len(df), \"Merge com df_roll alterou o número de linhas.\"\n",
    "\n",
    "    if verbose: print(\"build_feat_df: raridade/burst por par…\")\n",
    "    df_pairrb = compute_pair_rarity_burst(\n",
    "        df, time_col=time_col, user_col=user_col, benef_col=benef_col, row_id_col=\"row_id\"\n",
    "    )\n",
    "    feat_df = feat_df.merge(df_pairrb, on=\"row_id\", how=\"left\", sort=False)\n",
    "    assert len(feat_df) == len(df), \"Merge com df_pairrb alterou o número de linhas.\"\n",
    "\n",
    "    if include_pair_counts:\n",
    "        raise NotImplementedError(\"Ative somente se a função de *_pcnt também devolver row_id.\")\n",
    "\n",
    "    if include_value_stats:\n",
    "        raise NotImplementedError(\"Ative somente se a função de valor 30d também devolver row_id.\")\n",
    "\n",
    "    feat_df = feat_df.sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"build_feat_df: ok. shape:\", feat_df.shape)\n",
    "        print(\"sanity:\", len(feat_df), \"==\", len(df), \"→\", len(feat_df) == len(df))\n",
    "    return feat_df\n",
    "\n",
    "def detect_anomalies(\n",
    "    feat_df: pd.DataFrame,\n",
    "    use_ocsvm: bool = False,\n",
    "    anomaly_percentile: float = 97.5,\n",
    "    windows_for_multi = (\"curtissimo\", \"curto\", \"medio\"),\n",
    "    seed: int = 42,\n",
    "    verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    if verbose: print(\"detect_anomalies: iniciando seleção de features...\")\n",
    "    base_feats = [\n",
    "        \"valor_pago\",\n",
    "        \"secs_desde_ult_trans_user\", \"secs_desde_ult_trans_par\",\n",
    "        \"grau_out_user\", \"grau_in_benef\", \"grau_total_user\", \"grau_total_benef\",\n",
    "        \"egonet_density_user\",\n",
    "        \"par_rareza\", \"par_burstiness\"\n",
    "    ]\n",
    "    multi_feats = []\n",
    "    for w in windows_for_multi:\n",
    "        for col in (f\"{w}_ucnt\", f\"{w}_ncnt\", f\"{w}_perday\", f\"{w}_zglob\", f\"{w}_zuser\", f\"{w}_zunit\"):\n",
    "            if col in feat_df.columns:\n",
    "                multi_feats.append(col)\n",
    "    optional_pair_feats = [c for c in feat_df.columns if c.endswith(\"_pcnt\")]\n",
    "    optional_value_feats = [c for c in [\"user_valor_median_30d\",\"par_valor_median_30d\",\"user_robust_z\",\"par_robust_z\"] if c in feat_df.columns]\n",
    "    model_features = [c for c in (base_feats + multi_feats + optional_pair_feats + optional_value_feats) if c in feat_df.columns]\n",
    "    if not model_features:\n",
    "        raise RuntimeError(\"nenhuma feature disponível para detecção. verifique a montagem do feat_df.\")\n",
    "    if verbose:\n",
    "        print(\"detect_anomalies: total de features usadas:\", len(model_features))\n",
    "\n",
    "    X = feat_df[model_features].fillna(0.0).replace([np.inf, -np.inf], 0.0).values\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(X)\n",
    "\n",
    "    if verbose: print(\"detect_anomalies: isolation forest...\")\n",
    "    iso = IsolationForest(n_estimators=300, max_samples='auto', contamination='auto', random_state=seed, n_jobs=-1)\n",
    "    iso.fit(Xs)\n",
    "    iso_score = -iso.score_samples(Xs).astype(float)\n",
    "\n",
    "    if verbose: print(\"detect_anomalies: lof...\")\n",
    "    lof = LocalOutlierFactor(n_neighbors=35, contamination='auto', novelty=False, n_jobs=-1)\n",
    "    _ = lof.fit_predict(Xs)\n",
    "    lof_cont = -lof.negative_outlier_factor_.astype(float)\n",
    "\n",
    "    if use_ocsvm:\n",
    "        if verbose: print(\"detect_anomalies: one-class svm...\")\n",
    "        ocs = OneClassSVM(gamma='scale', nu=0.01)\n",
    "        ocs.fit(Xs)\n",
    "        ocs_score = -ocs.decision_function(Xs).ravel().astype(float)\n",
    "    else:\n",
    "        ocs_score = np.zeros(len(Xs), dtype=float)\n",
    "\n",
    "    if \"curto_zuser\" in feat_df.columns:\n",
    "        rz_user_rule = np.clip(feat_df[\"curto_zuser\"].values, 0, None).astype(float)\n",
    "    elif \"curtissimo_zuser\" in feat_df.columns:\n",
    "        rz_user_rule = np.clip(feat_df[\"curtissimo_zuser\"].values, 0, None).astype(float)\n",
    "    else:\n",
    "        rz_user_rule = np.zeros(len(feat_df), dtype=float)\n",
    "\n",
    "    if \"curto_zunit\" in feat_df.columns:\n",
    "        rz_unit_rule = np.clip(feat_df[\"curto_zunit\"].values, 0, None).astype(float)\n",
    "    elif \"curtissimo_zunit\" in feat_df.columns:\n",
    "        rz_unit_rule = np.clip(feat_df[\"curtissimo_zunit\"].values, 0, None).astype(float)\n",
    "    else:\n",
    "        rz_unit_rule = np.zeros(len(feat_df), dtype=float)\n",
    "\n",
    "    if \"user_robust_z\" in feat_df.columns:\n",
    "        rz_user_val = np.clip(np.nan_to_num(feat_df[\"user_robust_z\"].values, nan=0.0), 0, None).astype(float)\n",
    "    else:\n",
    "        rz_user_val = np.zeros(len(feat_df), dtype=float)\n",
    "\n",
    "    if \"par_robust_z\" in feat_df.columns:\n",
    "        rz_par_val = np.clip(np.nan_to_num(feat_df[\"par_robust_z\"].values, nan=0.0), 0, None).astype(float)\n",
    "    else:\n",
    "        rz_par_val = np.zeros(len(feat_df), dtype=float)\n",
    "\n",
    "    rare_score = feat_df[\"par_rareza\"].values.astype(float)\n",
    "    burst_score = np.clip(feat_df[\"par_burstiness\"].values, 0, None).astype(float)\n",
    "\n",
    "    scores = pd.DataFrame({\n",
    "        \"iso\": iso_score,\n",
    "        \"lof\": lof_cont,\n",
    "        \"ocsvm\": ocs_score,\n",
    "        \"rz_user_cnt\": rz_user_rule,\n",
    "        \"rz_unit_cnt\": rz_unit_rule,\n",
    "        \"rz_user_val\": rz_user_val,\n",
    "        \"rz_par_val\": rz_par_val,\n",
    "        \"rare\": rare_score,\n",
    "        \"burst\": burst_score\n",
    "    }, index=feat_df.index)\n",
    "\n",
    "    ranks = scores.rank(method=\"average\", ascending=True)\n",
    "    ensemble_rank = ranks.mean(axis=1)\n",
    "    ensemble_score = (ensemble_rank - ensemble_rank.min()) / (ensemble_rank.max() - ensemble_rank.min() + 1e-9)\n",
    "\n",
    "    out = feat_df.copy()\n",
    "    out = pd.concat([out, scores.add_prefix(\"score_\")], axis=1)\n",
    "    out[\"ensemble_rank\"] = ensemble_rank\n",
    "    out[\"ensemble_score\"] = ensemble_score\n",
    "\n",
    "    threshold = np.percentile(ensemble_score, anomaly_percentile)\n",
    "    out[\"is_anomaly\"] = (out[\"ensemble_score\"] >= threshold).astype(int)\n",
    "\n",
    "    print(f\"corte (percentil {anomaly_percentile}%): {threshold:.4f}\")\n",
    "    print(\"total anomalias:\", int(out[\"is_anomaly\"].sum()), \"de\", len(out))\n",
    "\n",
    "    # skynet\n",
    "    num_anomalies = int(out[\"is_anomaly\"].sum())\n",
    "    display(HTML(f'<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">🤖 Skynet: Identificadas {num_anomalies} anomalias.</div>'))\n",
    "\n",
    "    return out\n",
    "\n",
    "def run_full_detection(\n",
    "    df,\n",
    "    include_pair_counts=False,\n",
    "    include_value_stats=False,\n",
    "    use_ocsvm=False,\n",
    "    anomaly_percentile=97.5,\n",
    "    windows_for_multi=(\"curtissimo\",\"curto\",\"medio\"),\n",
    "    seed=42,\n",
    "    verbose=True\n",
    "):\n",
    "    if verbose: print(\"run_full_detection: montando feat_df...\")\n",
    "    feat_df = build_feat_df(\n",
    "        df,\n",
    "        time_col=\"timestamp\",\n",
    "        user_col=\"user_id\",\n",
    "        unit_col=\"unidade_origem\",\n",
    "        benef_col=\"beneficiario_id\",\n",
    "        include_pair_counts=include_pair_counts,\n",
    "        include_value_stats=include_value_stats,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    if verbose: print(\"run_full_detection: detectando anomalias...\")\n",
    "    feat_df = detect_anomalies(\n",
    "        feat_df,\n",
    "        use_ocsvm=use_ocsvm,\n",
    "        anomaly_percentile=anomaly_percentile,\n",
    "        windows_for_multi=windows_for_multi,\n",
    "        seed=seed,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    print(\"execução concluída.\")\n",
    "    return feat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "executionInfo": {
     "elapsed": 38677,
     "status": "ok",
     "timestamp": 1760064515784,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "7H8qcr1Xxe68",
    "outputId": "802c8540-ee99-428d-f379-824c9aacfffb"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID012 — execução\n",
    "feat_df = build_feat_df(\n",
    "    df,\n",
    "    time_col=\"timestamp\",\n",
    "    user_col=\"user_id\",\n",
    "    unit_col=\"unidade_origem\",\n",
    "    benef_col=\"beneficiario_id\",\n",
    "    include_pair_counts=False,\n",
    "    include_value_stats=False,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "feat_df = detect_anomalies(\n",
    "    feat_df,\n",
    "    use_ocsvm=False,\n",
    "    anomaly_percentile=97.5,\n",
    "    windows_for_multi=(\"curtissimo\",\"curto\",\"medio\"),\n",
    "    seed=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"linhas no input:\", len(df), \"linhas no feat_df:\", len(feat_df))\n",
    "assert len(feat_df) == len(df), \"Cardinalidade alterada — verifique merges por row_id.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3VSSvDK45jQ"
   },
   "source": [
    "###**Etapa 11:** Salva arquivo com a análise realizada.\n",
    "\n",
    "São criadas subpastas para cada data/hora de execução do código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 105,
     "status": "ok",
     "timestamp": 1760064665596,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "U-OPMyv_B2mY"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID013 — salvar resultados e visões operacionais (alinhado ao RUN_DIR e fuso São Paulo)\n",
    "import os, json, datetime as dt\n",
    "from zoneinfo import ZoneInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# base padrão do projeto (usada apenas se RUN_DIR não existir)\n",
    "OUTPUT_DIR_DEFAULT = \"/content/drive/MyDrive/Notebooks/temporal-graph-network/output\"\n",
    "SAO_TZ = None\n",
    "try:\n",
    "    SAO_TZ = ZoneInfo(\"America/Sao_Paulo\")\n",
    "except Exception:\n",
    "    SAO_TZ = None  # fallback sem timezone explícito\n",
    "\n",
    "def _cols_if_exist(df, cols):\n",
    "    return [c for c in cols if c in df.columns]\n",
    "\n",
    "def _topN_por_grupo(df, group_col, sort_col=\"ensemble_score\", n_top=50):\n",
    "    if group_col not in df.columns:\n",
    "        return pd.DataFrame(columns=[group_col, sort_col])\n",
    "    return (\n",
    "        df.sort_values(sort_col, ascending=False)\n",
    "          .groupby(group_col, group_keys=False)\n",
    "          .head(n_top)\n",
    "    )\n",
    "\n",
    "def _serie_diaria(df, timestamp_col=\"timestamp\", score_col=\"ensemble_score\", flag_col=\"is_anomaly\"):\n",
    "    if timestamp_col not in df.columns:\n",
    "        return pd.DataFrame(columns=[\"data\",\"anomalias\",\"avg_ensemble_score\"])\n",
    "    w = df[[timestamp_col, score_col] + ([flag_col] if flag_col in df.columns else [])].copy()\n",
    "    w[\"data\"] = pd.to_datetime(w[timestamp_col], errors=\"coerce\").dt.date\n",
    "    grp = w.groupby(\"data\", dropna=True)\n",
    "    out = pd.DataFrame({\n",
    "        \"anomalias\": grp[flag_col].sum() if flag_col in w.columns else grp[score_col].size(),\n",
    "        \"avg_ensemble_score\": grp[score_col].mean()\n",
    "    }).reset_index()\n",
    "    return out\n",
    "\n",
    "def _anomalias_por_unidade_dia(df, timestamp_col=\"timestamp\", unidade_col=\"unidade_origem\",\n",
    "                               score_col=\"ensemble_score\", flag_col=\"is_anomaly\"):\n",
    "    if timestamp_col not in df.columns or unidade_col not in df.columns:\n",
    "        return pd.DataFrame(columns=[unidade_col, \"data\", \"anomalias\", \"avg_ensemble_score\"])\n",
    "    w = df[[timestamp_col, unidade_col, score_col] + ([flag_col] if flag_col in df.columns else [])].copy()\n",
    "    w[\"data\"] = pd.to_datetime(w[timestamp_col], errors=\"coerce\").dt.date\n",
    "    grp = w.groupby([unidade_col, \"data\"], dropna=True)\n",
    "    out = pd.DataFrame({\n",
    "        \"anomalias\": grp[flag_col].sum() if flag_col in w.columns else grp[score_col].size(),\n",
    "        \"avg_ensemble_score\": grp[score_col].mean()\n",
    "    }).reset_index()\n",
    "    return out\n",
    "\n",
    "def _top_features_explicabilidade(df, score_prefix=\"score_\", flag_col=\"is_anomaly\"):\n",
    "    score_cols = [c for c in df.columns if c.startswith(score_prefix)]\n",
    "    if not score_cols:\n",
    "        return pd.DataFrame(columns=[\"feature\",\"media_anomalia\",\"media_geral\",\"impacto_relativo\"])\n",
    "    df_anom = df[df[flag_col] == 1] if flag_col in df.columns else df\n",
    "    media_anom = df_anom[score_cols].mean().rename(\"media_anomalia\")\n",
    "    media_geral = df[score_cols].mean().rename(\"media_geral\")\n",
    "    explic = pd.concat([media_anom, media_geral], axis=1)\n",
    "    explic[\"impacto_relativo\"] = explic[\"media_anomalia\"] / (explic[\"media_geral\"] + 1e-9)\n",
    "    explic = explic.reset_index().rename(columns={\"index\": \"feature\"})\n",
    "    explic = explic.sort_values(\"impacto_relativo\", ascending=False)\n",
    "    return explic\n",
    "\n",
    "def _data_dictionary(df):\n",
    "    return pd.DataFrame({\n",
    "        \"column\": df.columns,\n",
    "        \"dtype\": [str(dt) for dt in df.dtypes]\n",
    "    })\n",
    "\n",
    "def save_results_id013(\n",
    "    df_input: pd.DataFrame,\n",
    "    feat_df: pd.DataFrame,\n",
    "    output_dir: str = OUTPUT_DIR_DEFAULT,\n",
    "    anomaly_percentile: float = 97.5,\n",
    "    top_k_anomalies: int = 1000,\n",
    "    top_n_por_grupo: int = 50\n",
    "):\n",
    "    # sanity checks\n",
    "    if feat_df is None or len(feat_df) == 0:\n",
    "        raise RuntimeError(\"feat_df está vazio ou não foi gerado.\")\n",
    "    if df_input is None or len(df_input) == 0:\n",
    "        raise RuntimeError(\"df de entrada está vazio ou não foi carregado.\")\n",
    "\n",
    "    # determina a pasta de execução: preferir RUN_DIR; caso não exista, criar TGN_... em output_dir\n",
    "    if 'RUN_DIR' in globals() and RUN_DIR is not None and os.path.isdir(str(RUN_DIR)):\n",
    "        outdir = str(RUN_DIR)\n",
    "        print(\"Usando RUN_DIR existente:\", outdir)\n",
    "    else:\n",
    "        ts = (dt.datetime.now(SAO_TZ).strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "              if SAO_TZ else dt.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "        outdir = os.path.join(output_dir, f\"TGN_{ts}\")\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "        print(\"RUN_DIR não definido; criada pasta de execução:\", outdir)\n",
    "\n",
    "    # ordena por score\n",
    "    if \"ensemble_score\" not in feat_df.columns:\n",
    "        raise KeyError(\"coluna 'ensemble_score' não encontrada em feat_df.\")\n",
    "    df_sorted = feat_df.sort_values(\"ensemble_score\", ascending=False)\n",
    "\n",
    "    # threshold de documentação\n",
    "    threshold = float(np.percentile(df_sorted[\"ensemble_score\"].values, anomaly_percentile))\n",
    "\n",
    "    # colunas recomendadas\n",
    "    key_cols   = _cols_if_exist(feat_df, [\"row_id\",\"trans_id\",\"timestamp\",\"user_id\",\"beneficiario_id\",\"unidade_origem\",\"valor_pago\"])\n",
    "    flag_cols  = _cols_if_exist(feat_df, [\"ensemble_score\",\"ensemble_rank\",\"is_anomaly\"])\n",
    "    score_cols = [c for c in feat_df.columns if c.startswith(\"score_\")]\n",
    "    extra_cols = _cols_if_exist(feat_df, [\n",
    "        \"secs_desde_ult_trans_user\",\"secs_desde_ult_trans_par\",\n",
    "        \"par_rareza\",\"par_burstiness\",\n",
    "        \"grau_out_user\",\"grau_in_benef\",\"grau_total_user\",\"grau_total_benef\",\"egonet_density_user\"\n",
    "    ])\n",
    "    export_cols = key_cols + flag_cols + score_cols + extra_cols\n",
    "    export_cols += [c for c in feat_df.columns if c not in export_cols]  # inclui o resto ao final\n",
    "\n",
    "    # caminhos de saída (nomes padronizados)\n",
    "    path_csv_full   = os.path.join(outdir, \"full.csv\")\n",
    "    path_parq_full  = os.path.join(outdir, \"full.parquet\")\n",
    "    path_csv_anom   = os.path.join(outdir, \"anomalies.csv\")\n",
    "    path_manifest   = os.path.join(outdir, \"manifest.json\")\n",
    "\n",
    "    # salva full CSV\n",
    "    df_sorted.to_csv(path_csv_full, index=False, columns=export_cols)\n",
    "\n",
    "    # salva full Parquet\n",
    "    used_parquet = True\n",
    "    try:\n",
    "        df_sorted.to_parquet(path_parq_full, index=False)\n",
    "    except Exception as e:\n",
    "        print(\"aviso: to_parquet falhou; manteremos apenas CSV. erro:\", repr(e))\n",
    "        used_parquet = False\n",
    "\n",
    "    # salva anomalias (ou top-k quando flag ausente)\n",
    "    if \"is_anomaly\" in df_sorted.columns:\n",
    "        df_anom = df_sorted[df_sorted[\"is_anomaly\"] == 1].copy()\n",
    "    else:\n",
    "        df_anom = df_sorted.head(top_k_anomalies).copy()\n",
    "    df_anom.to_csv(path_csv_anom, index=False, columns=export_cols)\n",
    "\n",
    "    # visões adicionais\n",
    "    views = {}\n",
    "\n",
    "    # top-N por usuário\n",
    "    top_user = _topN_por_grupo(df_anom if len(df_anom) else df_sorted, \"user_id\", \"ensemble_score\", n_top=top_n_por_grupo)\n",
    "    path_top_user = os.path.join(outdir, f\"top{top_n_por_grupo}_por_usuario.csv\")\n",
    "    top_user.to_csv(path_top_user, index=False)\n",
    "    views[\"top_user\"] = path_top_user\n",
    "\n",
    "    # top-N por unidade\n",
    "    top_unit = _topN_por_grupo(df_anom if len(df_anom) else df_sorted, \"unidade_origem\", \"ensemble_score\", n_top=top_n_por_grupo)\n",
    "    path_top_unit = os.path.join(outdir, f\"top{top_n_por_grupo}_por_unidade.csv\")\n",
    "    top_unit.to_csv(path_top_unit, index=False)\n",
    "    views[\"top_unidade\"] = path_top_unit\n",
    "\n",
    "    # top-N por beneficiário\n",
    "    top_benef = _topN_por_grupo(df_anom if len(df_anom) else df_sorted, \"beneficiario_id\", \"ensemble_score\", n_top=top_n_por_grupo)\n",
    "    path_top_benef = os.path.join(outdir, f\"top{top_n_por_grupo}_por_beneficiario.csv\")\n",
    "    top_benef.to_csv(path_top_benef, index=False)\n",
    "    views[\"top_beneficiario\"] = path_top_benef\n",
    "\n",
    "    # série temporal diária\n",
    "    serie = _serie_diaria(df_sorted, timestamp_col=\"timestamp\", score_col=\"ensemble_score\", flag_col=\"is_anomaly\")\n",
    "    path_serie = os.path.join(outdir, \"anomalias_por_dia.csv\")\n",
    "    serie.to_csv(path_serie, index=False)\n",
    "    views[\"anomalias_por_dia\"] = path_serie\n",
    "\n",
    "    # anomalias por unidade × dia\n",
    "    serie_unid = _anomalias_por_unidade_dia(df_sorted, timestamp_col=\"timestamp\", unidade_col=\"unidade_origem\",\n",
    "                                            score_col=\"ensemble_score\", flag_col=\"is_anomaly\")\n",
    "    path_serie_unid = os.path.join(outdir, \"anomalias_por_unidade_dia.csv\")\n",
    "    serie_unid.to_csv(path_serie_unid, index=False)\n",
    "    views[\"anomalias_por_unidade_dia\"] = path_serie_unid\n",
    "\n",
    "    # top features (explicabilidade simples)\n",
    "    top_feat = _top_features_explicabilidade(df_sorted, score_prefix=\"score_\", flag_col=\"is_anomaly\")\n",
    "    path_top_feat = os.path.join(outdir, \"top_features.csv\")\n",
    "    top_feat.to_csv(path_top_feat, index=False)\n",
    "    views[\"top_features\"] = path_top_feat\n",
    "\n",
    "    # data dictionary\n",
    "    dict_df = _data_dictionary(df_sorted)\n",
    "    path_dict = os.path.join(outdir, \"data_dictionary.csv\")\n",
    "    dict_df.to_csv(path_dict, index=False)\n",
    "    views[\"data_dictionary\"] = path_dict\n",
    "\n",
    "    # manifest\n",
    "    paths = {\n",
    "        \"csv_full\": path_csv_full,\n",
    "        \"csv_anomalies\": path_csv_anom,\n",
    "        **views\n",
    "    }\n",
    "    if used_parquet:\n",
    "        paths[\"parquet_full\"] = path_parq_full\n",
    "\n",
    "    manifest = {\n",
    "        \"timestamp_execucao\": (dt.datetime.now(SAO_TZ).isoformat() if SAO_TZ else dt.datetime.now().isoformat()),\n",
    "        \"output_dir\": outdir,\n",
    "        \"input_rows\": int(len(df_input)),\n",
    "        \"output_rows\": int(len(feat_df)),\n",
    "        \"anomaly_percentile\": float(anomaly_percentile),\n",
    "        \"threshold_ensemble_score\": threshold,\n",
    "        \"paths\": paths,\n",
    "        \"columns\": {\n",
    "            \"keys\": key_cols,\n",
    "            \"flags\": flag_cols,\n",
    "            \"scores\": score_cols\n",
    "        }\n",
    "    }\n",
    "    with open(path_manifest, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(manifest, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"resultados salvos em:\", outdir)\n",
    "    print(\"arquivos gerados:\")\n",
    "    for k, v in paths.items():\n",
    "        print(\" -\", k, \":\", v)\n",
    "    print(\" - manifest:\", path_manifest)\n",
    "\n",
    "    return {\n",
    "        \"outdir\": outdir,\n",
    "        \"paths\": paths,\n",
    "        \"manifest\": path_manifest\n",
    "    }\n",
    "\n",
    "# exemplo de uso logo após a etapa de detecção:\n",
    "# save_info = save_results_id013(df, feat_df, output_dir=OUTPUT_DIR_DEFAULT, anomaly_percentile=97.5, top_k_anomalies=1000, top_n_por_grupo=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1760064770454,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "zVgl_SJdCQZx",
    "outputId": "33cdfd43-e89a-4cdf-8c90-5a71398aa9af"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID013-CONT1\n",
    "from pathlib import Path\n",
    "import os, json\n",
    "\n",
    "base = Path(\"/content/drive/MyDrive/Notebooks/temporal-graph-network/output\")\n",
    "curr = Path(str(RUN_DIR)) if 'RUN_DIR' in globals() else None\n",
    "print(\"RUN_DIR atual:\", curr)\n",
    "\n",
    "def list_dir(p):\n",
    "    if not p or not p.exists():\n",
    "        print(\"(!) pasta inexistente:\", p);\n",
    "        return\n",
    "    print(\"Conteúdo de\", p, \":\")\n",
    "    for f in sorted(p.iterdir()):\n",
    "        print(\" -\", f.name)\n",
    "\n",
    "list_dir(curr)\n",
    "\n",
    "need = {\"full.parquet\",\"full.csv\",\"anomalies.csv\",\"manifest.json\"}\n",
    "have = {x.name for x in curr.iterdir()} if curr and curr.exists() else set()\n",
    "missing = sorted(need - have)\n",
    "print(\"Faltam no RUN_DIR atual:\", missing)\n",
    "\n",
    "# Procura a última pasta que realmente tenha full/anomalies\n",
    "cands = []\n",
    "for d in sorted(base.iterdir(), key=os.path.getmtime, reverse=True):\n",
    "    if not d.is_dir():\n",
    "        continue\n",
    "    names = {x.name for x in d.iterdir()}\n",
    "    if (\"full.parquet\" in names or \"full.csv\" in names) and (\"anomalies.csv\" in names):\n",
    "        cands.append(d)\n",
    "        break\n",
    "\n",
    "if cands:\n",
    "    print(\"Pasta com dados prontos encontrada:\", cands[0])\n",
    "    list_dir(cands[0])\n",
    "else:\n",
    "    print(\"Nenhuma pasta anterior com full/anomalies encontrada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6842,
     "status": "ok",
     "timestamp": 1760064798680,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "TNJ5zBYeCVqs",
    "outputId": "66e89398-ffa0-4d55-d173-86e10c0f2c22"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID013-CONT2\n",
    "# garanta estes nomes:\n",
    "OUTPUT_DIR_DEFAULT = \"/content/drive/MyDrive/Notebooks/temporal-graph-network/output\"\n",
    "\n",
    "save_info = save_results_id013(\n",
    "    df_input=df,\n",
    "    feat_df=feat_df,\n",
    "    output_dir=OUTPUT_DIR_DEFAULT,\n",
    "    anomaly_percentile=97.5,\n",
    "    top_k_anomalies=1000,\n",
    "    top_n_por_grupo=50\n",
    ")\n",
    "print(\"Gravou em:\", save_info[\"outdir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLxsD81f96Vn"
   },
   "source": [
    "###**Etapa 12:** Análise Gráfica\n",
    "\n",
    "Distribuição e Top-N (com corte por percentil e K sugerido por maior gap)\n",
    "\n",
    "Essa análise gera dois gráficos que ajudam a entender como os escores de anomalia (“ensemble_score”) estão distribuídos e quais transações são mais suspeitas:\n",
    "\n",
    "---\n",
    "**Histograma – Distribuição do ensemble_score**\n",
    "\n",
    "Mostra a frequência dos escores em toda a base.\n",
    "\n",
    "O que procurar:\n",
    "- A linha pontilhada indica o percentil de corte (ex.: Percentil 97,5). Deve ser verificado se ela cai na região da cauda, o que significa que só os casos mais extremos serão analisados (evitando falsos positivos).\n",
    "- Se a maior parte dos casos está em valores baixos/médios e existe uma cauda à direita (valores muito altos), esses pontos de cauda são os candidatos a anomalias.\n",
    "- Observar o valor correspondente ao percentil estabelecido para complementar a próxima análise.\n",
    "---\n",
    "**Gráfico de linha – Top N transações mais anômalas**\n",
    "\n",
    "Ordena os maiores escores (rank 1 = mais anômala).\n",
    "\n",
    "O que procurar:\n",
    "- Grandes saltos (“gaps”) entre ranks consecutivos: indicam que as transações até o salto são bem mais anômalas do que as demais — são as que merecem atenção imediata.\n",
    "- Observar os valores de escore de anomalia encontrados nos N registros mais anômalos versus o valor correspondente ao percentil estabelecido. Quanto maior a diferença, mais anômalo.\n",
    "- Platô (curva que se estabiliza): mostra a partir de que ponto (K) os casos deixam de ser tão excepcionais. Observar o K sugerido pelo maior gap, ele indica quantos casos devem ser priorizados na revisão manual. Até K registros são candidatos muito fortes para anomalias e exigem revisão manual.\n",
    "---\n",
    "\n",
    "Em resumo: os gráficos servem para decidir onde cortar e quais transações revisar primeiro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 12764,
     "status": "ok",
     "timestamp": 1760065070681,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "Gcc7_t-HDV6h",
    "outputId": "d75de98b-a934-42a3-c665-50f135218f36"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID014 — análise estatística e visual das anomalias (salva PNGs em FIG_DIR)\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from google.colab import drive\n",
    "\n",
    "# ===== CONFIGURAÇÕES =====\n",
    "OUTPUT_DIR_BASE = \"/content/drive/MyDrive/Notebooks/temporal-graph-network/output\"\n",
    "SAVE_PLOTS = True  # defina False se não quiser salvar PNGs\n",
    "\n",
    "# estilo básico\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "plt.rcParams[\"font.size\"] = 11\n",
    "\n",
    "# ===== DRIVE E BASE =====\n",
    "if not os.path.ismount(\"/content/drive\"):\n",
    "    print(\"Montando Google Drive...\")\n",
    "    drive.mount(\"/content/drive\")\n",
    "else:\n",
    "    print(\"Google Drive já montado.\")\n",
    "os.makedirs(OUTPUT_DIR_BASE, exist_ok=True)\n",
    "print(\"Base:\", OUTPUT_DIR_BASE)\n",
    "\n",
    "# ===== HELPERS =====\n",
    "def _ensure_fig_dir(latest_dir):\n",
    "    # usa FIG_DIR do RUN_DIR se disponível; senão cria fallback\n",
    "    fig_dir = os.path.join(latest_dir, \"figuras\")\n",
    "    os.makedirs(fig_dir, exist_ok=True)\n",
    "    return fig_dir\n",
    "\n",
    "_fig_counter = {\"n\": 0}\n",
    "def _save_show(fig, fig_dir, name_hint):\n",
    "    if SAVE_PLOTS:\n",
    "        _fig_counter[\"n\"] += 1\n",
    "        fname = f\"{_fig_counter['n']:02d}_{name_hint}.png\".replace(\" \", \"_\")\n",
    "        path = os.path.join(fig_dir, fname)\n",
    "        fig.savefig(path, dpi=150, bbox_inches=\"tight\")\n",
    "        print(\"Figura salva:\", path)\n",
    "    plt.show()\n",
    "\n",
    "def _plot_hist(data, title, xlabel, ylabel=\"Frequência\", bins=50, threshold=None, fig_dir=None):\n",
    "    fig = plt.figure()\n",
    "    plt.hist(data, bins=bins, alpha=0.7)\n",
    "    if threshold is not None:\n",
    "        plt.axvline(threshold, color=\"red\", linestyle=\"--\", label=f\"threshold = {threshold:.4f}\")\n",
    "        plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.tight_layout()\n",
    "    _save_show(fig, fig_dir, title)\n",
    "\n",
    "def _pick_run_dir():\n",
    "    # 1) se RUN_DIR existe e tem dados, usa\n",
    "    if \"RUN_DIR\" in globals():\n",
    "        rd = str(RUN_DIR)\n",
    "        if os.path.isdir(rd):\n",
    "            names = set(os.listdir(rd))\n",
    "            if ((\"full.parquet\" in names) or (\"full.csv\" in names)) and (\"anomalies.csv\" in names):\n",
    "                print(\"Usando RUN_DIR ativo:\", rd)\n",
    "                return rd\n",
    "            else:\n",
    "                print(\"RUN_DIR ativo não tem full/anomalies — procurando pasta anterior com dados...\")\n",
    "\n",
    "    # 2) varre /output e pega a mais recente com full/anomalies\n",
    "    subdirs = [\n",
    "        os.path.join(OUTPUT_DIR_BASE, d)\n",
    "        for d in os.listdir(OUTPUT_DIR_BASE)\n",
    "        if os.path.isdir(os.path.join(OUTPUT_DIR_BASE, d))\n",
    "    ]\n",
    "    if not subdirs:\n",
    "        raise RuntimeError(\"Nenhuma subpasta encontrada em /output.\")\n",
    "    subdirs.sort(key=os.path.getmtime, reverse=True)\n",
    "\n",
    "    for d in subdirs:\n",
    "        names = set(os.listdir(d))\n",
    "        if ((\"full.parquet\" in names) or (\"full.csv\" in names)) and (\"anomalies.csv\" in names):\n",
    "            print(\"Usando pasta com dados:\", d)\n",
    "            return d\n",
    "\n",
    "    # 3) se nada tiver dados, devolve a mais recente e deixamos falhar com diagnóstico\n",
    "    chosen = subdirs[0]\n",
    "    print(\"Atenção: nenhuma pasta com dados completos; usando a mais recente:\", chosen)\n",
    "    return chosen\n",
    "\n",
    "def _load_paths(latest_dir):\n",
    "    manifest_files = [f for f in os.listdir(latest_dir) if f.endswith(\"manifest.json\")]\n",
    "    full_path = None\n",
    "    anom_path = None\n",
    "    threshold = None\n",
    "\n",
    "    if manifest_files:\n",
    "        mpath = os.path.join(latest_dir, manifest_files[0])\n",
    "        try:\n",
    "            with open(mpath, \"r\", encoding=\"utf-8\") as f:\n",
    "                m = json.load(f)\n",
    "            print(\"Manifest carregado:\", mpath)\n",
    "            threshold = m.get(\"threshold_ensemble_score\") or m.get(\"threshold\", None)\n",
    "            paths = m.get(\"paths\", {})\n",
    "            cand_full = [paths.get(k) for k in [\"parquet_full\", \"csv_full\"] if paths.get(k)]\n",
    "            cand_anom = [paths.get(k) for k in [\"csv_anomalies\"] if paths.get(k)]\n",
    "            if cand_full:\n",
    "                full_path = cand_full[0]\n",
    "                if not os.path.isabs(full_path):\n",
    "                    full_path = os.path.join(latest_dir, os.path.basename(full_path))\n",
    "            if cand_anom:\n",
    "                anom_path = cand_anom[0]\n",
    "                if not os.path.isabs(anom_path):\n",
    "                    anom_path = os.path.join(latest_dir, os.path.basename(anom_path))\n",
    "        except Exception as e:\n",
    "            print(\"Aviso: falha ao ler manifest:\", repr(e))\n",
    "\n",
    "    if not full_path:\n",
    "        cands = [f for f in os.listdir(latest_dir) if f.endswith(\"_full.parquet\")]\n",
    "        if not cands and \"full.parquet\" in os.listdir(latest_dir):\n",
    "            cands = [\"full.parquet\"]\n",
    "        if not cands:\n",
    "            cands = [f for f in os.listdir(latest_dir) if f.lower().endswith(\".parquet\")]\n",
    "        if cands:\n",
    "            full_path = os.path.join(latest_dir, cands[0])\n",
    "\n",
    "    if not anom_path:\n",
    "        cands = [f for f in os.listdir(latest_dir) if f.endswith(\"_anomalies.csv\")]\n",
    "        if not cands and \"anomalies.csv\" in os.listdir(latest_dir):\n",
    "            cands = [\"anomalies.csv\"]\n",
    "        if not cands:\n",
    "            cands = [f for f in os.listdir(latest_dir)\n",
    "                     if f.lower().endswith(\".csv\") and \"anom\" in f.lower()]\n",
    "        if cands:\n",
    "            anom_path = os.path.join(latest_dir, cands[0])\n",
    "\n",
    "    if not full_path or not anom_path:\n",
    "        print(\"\\nConteúdo da pasta para diagnóstico:\")\n",
    "        for f in sorted(os.listdir(latest_dir)):\n",
    "            print(\" -\", f)\n",
    "\n",
    "    if not full_path:\n",
    "        raise FileNotFoundError(\"Não encontrei dataset 'full' (parquet/csv) em \" + latest_dir)\n",
    "    if not anom_path:\n",
    "        raise FileNotFoundError(\"Não encontrei arquivo de 'anomalias' (csv) em \" + latest_dir)\n",
    "\n",
    "    return full_path, anom_path, threshold\n",
    "\n",
    "# ===== EXECUÇÃO =====\n",
    "latest_dir = _pick_run_dir()\n",
    "fig_dir = _ensure_fig_dir(latest_dir)\n",
    "full_path, anom_path, threshold = _load_paths(latest_dir)\n",
    "\n",
    "if full_path.endswith(\".parquet\"):\n",
    "    df_full = pd.read_parquet(full_path)\n",
    "else:\n",
    "    df_full = pd.read_csv(full_path)\n",
    "df_anom = pd.read_csv(anom_path)\n",
    "\n",
    "print(f\"linhas totais: {len(df_full):,} | anomalias: {len(df_anom):,}\")\n",
    "print(\"Arquivos usados:\\n - full:\", full_path, \"\\n - anomalies:\", anom_path, \"\\n - threshold:\", threshold)\n",
    "print(\"Figuras serão salvas em:\", fig_dir) if SAVE_PLOTS else None\n",
    "\n",
    "# ===== ESTATÍSTICA DESCRITIVA DO INPUT =====\n",
    "print(\"\\n[descrição estatística de valor_pago]\")\n",
    "if \"valor_pago\" in df_full.columns:\n",
    "    desc = df_full[\"valor_pago\"].describe(percentiles=[.01,.05,.1,.25,.5,.75,.9,.95,.99])\n",
    "    print(desc)\n",
    "    _plot_hist(df_full[\"valor_pago\"], \"Distribuição de Valor Pago\", \"Valor Pago (R$)\", bins=60, fig_dir=fig_dir)\n",
    "else:\n",
    "    print(\"coluna 'valor_pago' ausente no dataset.\")\n",
    "\n",
    "if \"timestamp\" in df_full.columns:\n",
    "    df_full[\"data\"] = pd.to_datetime(df_full[\"timestamp\"], errors=\"coerce\").dt.date\n",
    "    freq_por_dia = df_full[\"data\"].value_counts().sort_index()\n",
    "    fig = plt.figure()\n",
    "    plt.plot(freq_por_dia.index, freq_por_dia.values)\n",
    "    plt.title(\"Frequência de Lançamentos por Dia\")\n",
    "    plt.xlabel(\"Data\")\n",
    "    plt.ylabel(\"Quantidade de lançamentos\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    _save_show(fig, fig_dir, \"Frequencia_por_dia\")\n",
    "\n",
    "# ===== DISTRIBUIÇÃO DO ENSEMBLE SCORE =====\n",
    "if \"ensemble_score\" in df_full.columns:\n",
    "    _plot_hist(df_full[\"ensemble_score\"], \"Distribuição do Ensemble Score\", \"ensemble_score\", bins=50, threshold=threshold, fig_dir=fig_dir)\n",
    "\n",
    "    if \"is_anomaly\" in df_full.columns:\n",
    "        fig = plt.figure()\n",
    "        plt.boxplot(\n",
    "            [df_full.loc[df_full[\"is_anomaly\"] == 0, \"ensemble_score\"],\n",
    "             df_full.loc[df_full[\"is_anomaly\"] == 1, \"ensemble_score\"]],\n",
    "            labels=[\"Normal\", \"Anômalo\"],\n",
    "            patch_artist=True,\n",
    "            boxprops=dict(facecolor=\"lightgray\", color=\"black\"),\n",
    "            medianprops=dict(color=\"red\")\n",
    "        )\n",
    "        plt.title(\"Boxplot do Ensemble Score (Normal vs Anômalo)\")\n",
    "        plt.ylabel(\"ensemble_score\")\n",
    "        plt.tight_layout()\n",
    "        _save_show(fig, fig_dir, \"Boxplot_ensemble_score\")\n",
    "\n",
    "# ===== ANOMALIAS POR UNIDADE =====\n",
    "if \"unidade_origem\" in df_full.columns and \"is_anomaly\" in df_full.columns:\n",
    "    fig = plt.figure()\n",
    "    df_unit = df_full.groupby(\"unidade_origem\")[\"is_anomaly\"].sum().sort_values(ascending=False).head(20)\n",
    "    df_unit.plot(kind=\"bar\", color=\"firebrick\")\n",
    "    plt.title(\"Top 20 Unidades com Mais Anomalias\")\n",
    "    plt.ylabel(\"Qtde de Anomalias\")\n",
    "    plt.xlabel(\"Unidade Origem\")\n",
    "    plt.xticks(rotation=70)\n",
    "    plt.tight_layout()\n",
    "    _save_show(fig, fig_dir, \"Top20_unidades_anomalias\")\n",
    "\n",
    "# ===== ANOMALIAS POR USUÁRIO =====\n",
    "if \"user_id\" in df_full.columns and \"is_anomaly\" in df_full.columns:\n",
    "    fig = plt.figure()\n",
    "    df_user = df_full.groupby(\"user_id\")[\"is_anomaly\"].sum().sort_values(ascending=False).head(20)\n",
    "    df_user.plot(kind=\"bar\", color=\"darkslateblue\")\n",
    "    plt.title(\"Top 20 Usuários com Mais Anomalias\")\n",
    "    plt.ylabel(\"Qtde de Anomalias\")\n",
    "    plt.xlabel(\"User ID\")\n",
    "    plt.xticks(rotation=70)\n",
    "    plt.tight_layout()\n",
    "    _save_show(fig, fig_dir, \"Top20_usuarios_anomalias\")\n",
    "\n",
    "# ===== SÉRIE TEMPORAL DE ANOMALIAS POR DIA =====\n",
    "cand = [f for f in os.listdir(latest_dir) if f.endswith(\"anomalias_por_dia.csv\")]\n",
    "if cand:\n",
    "    serie = pd.read_csv(os.path.join(latest_dir, cand[0]))\n",
    "    fig = plt.figure()\n",
    "    plt.plot(serie[\"data\"], serie[\"anomalias\"], label=\"Anomalias\")\n",
    "    plt.title(\"Anomalias por Dia\")\n",
    "    plt.xlabel(\"Data\")\n",
    "    plt.ylabel(\"Qtde de Anomalias\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    _save_show(fig, fig_dir, \"Anomalias_por_dia\")\n",
    "\n",
    "# ===== ANOMALIAS POR UNIDADE × DIA =====\n",
    "cand = [f for f in os.listdir(latest_dir) if f.endswith(\"anomalias_por_unidade_dia.csv\")]\n",
    "if cand:\n",
    "    df_u = pd.read_csv(os.path.join(latest_dir, cand[0]))\n",
    "    pivot = df_u.pivot_table(index=\"data\", columns=\"unidade_origem\", values=\"anomalias\", fill_value=0)\n",
    "    pivot = pivot.iloc[-30:] if len(pivot) > 30 else pivot\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    plt.stackplot(pivot.index, pivot.T, labels=pivot.columns)\n",
    "    plt.title(\"Evolução de Anomalias por Unidade (últimos 30 dias)\")\n",
    "    plt.xlabel(\"Data\")\n",
    "    plt.ylabel(\"Qtde de Anomalias\")\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1,1))\n",
    "    plt.tight_layout()\n",
    "    _save_show(fig, fig_dir, \"Evolucao_anomalias_por_unidade_30d\")\n",
    "\n",
    "# ===== CORRELAÇÃO ENTRE SCORES =====\n",
    "score_cols = [c for c in df_full.columns if c.startswith(\"score_\")]\n",
    "if len(score_cols) >= 2:\n",
    "    corr = df_full[score_cols].corr()\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    plt.imshow(corr, cmap=\"coolwarm\", interpolation=\"nearest\")\n",
    "    plt.xticks(range(len(score_cols)), score_cols, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(len(score_cols)), score_cols)\n",
    "    plt.title(\"Correlação entre Scores\")\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    _save_show(fig, fig_dir, \"Correlacao_scores\")\n",
    "\n",
    "# ===== BOXPLOT DE VALOR_PAGO VS ANOMALIA =====\n",
    "if \"valor_pago\" in df_full.columns and \"is_anomaly\" in df_full.columns:\n",
    "    fig = plt.figure()\n",
    "    plt.boxplot(\n",
    "        [df_full.loc[df_full[\"is_anomaly\"] == 0, \"valor_pago\"],\n",
    "         df_full.loc[df_full[\"is_anomaly\"] == 1, \"valor_pago\"]],\n",
    "        labels=[\"Normal\", \"Anômalo\"],\n",
    "        patch_artist=True,\n",
    "        boxprops=dict(facecolor=\"lightgray\", color=\"black\"),\n",
    "        medianprops=dict(color=\"red\")\n",
    "    )\n",
    "    plt.title(\"Boxplot de Valor Pago (Normal vs Anômalo)\")\n",
    "    plt.ylabel(\"Valor Pago (R$)\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.tight_layout()\n",
    "    _save_show(fig, fig_dir, \"Boxplot_valor_pago_log\")\n",
    "\n",
    "print(\"\\nAnálises gráficas concluídas para:\", latest_dir)\n",
    "print(\"Imagens salvas em:\", fig_dir) if SAVE_PLOTS else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pci7t0AZLcVr"
   },
   "source": [
    "###**Etapa 13:** Ego-Subgrafo\n",
    "\n",
    "Abaixo é gerado um ego-subgrafo em torno do usuário envolvido na transação mais anômala, mostrando suas conexões diretas no grafo de pagamentos.\n",
    "\n",
    "**O que ele pretende demonstrar:**\n",
    "\n",
    "Quem está conectado ao usuário central, a intensidade e frequência das transações (espessura das arestas), a relevância de cada nó (tamanho proporcional ao grau de conexões) e papéis distintos dos nós, facilitando a leitura do contexto da anomalia.\n",
    "\n",
    "\n",
    "- Estrela de saída (um nó azul/central pagando muitos verdes) → possível dispersão suspeita.\n",
    "- Muitos nós conectados a ele → possível conta “coletora”.\n",
    "- Ciclos ou arestas bidirecionais → podem indicar movimentação circular de valores.\n",
    "\n",
    "\n",
    "👉 Em resumo: o gráfico mostra a vizinhança imediata do usuário mais anômalo, destacando quem paga, quem recebe e a força dessas relações, para facilitar a investigação do porquê desse score elevado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1T5jNe9eADDLdRFA8t1AlpjL0OfS9ngkZ"
    },
    "executionInfo": {
     "elapsed": 29242,
     "status": "ok",
     "timestamp": 1760065918609,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "TYEPTz2kGf84",
    "outputId": "eb4d2fb6-1076-440a-d687-aedc06ddcbf1"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID015 — Visualização de egos: Top-N usuários e Top-N unidades de lotação\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from matplotlib.lines import Line2D\n",
    "from pathlib import Path\n",
    "\n",
    "# parâmetros\n",
    "TOP_N_USERS  = 3   # quantos usuários mais anômalos plotar\n",
    "TOP_N_UNITS  = 3   # quantas unidades mais anômalas plotar\n",
    "SEED = SEED if 'SEED' in globals() else 42\n",
    "\n",
    "# ==== helpers comuns ====\n",
    "def _ensure_fig_dir():\n",
    "    if 'FIG_DIR' in globals():\n",
    "        fig_dir = str(FIG_DIR)\n",
    "    elif 'RUN_DIR' in globals():\n",
    "        fig_dir = os.path.join(str(RUN_DIR), \"figuras\")\n",
    "    else:\n",
    "        fig_dir = \"./figuras\"\n",
    "    os.makedirs(fig_dir, exist_ok=True)\n",
    "    return fig_dir\n",
    "\n",
    "def _build_gsnap_from(df_like, user_col=\"user_id\", benef_col=\"beneficiario_id\",\n",
    "                      value_col=\"valor_pago\") -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Reconstrói um DiGraph agregando pesos por par (user->benef), com nós nomeados U:: e B::.\n",
    "    \"\"\"\n",
    "    if df_like is None or len(df_like) == 0:\n",
    "        raise RuntimeError(\"Dataset vazio para reconstruir Gsnap.\")\n",
    "    needed = {user_col, benef_col}\n",
    "    if not needed.issubset(df_like.columns):\n",
    "        raise RuntimeError(f\"Colunas necessárias ausentes: {needed - set(df_like.columns)}\")\n",
    "\n",
    "    use_value = (value_col in df_like.columns)\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    if use_value:\n",
    "        grp = df_like.groupby([user_col, benef_col])[value_col].sum().reset_index()\n",
    "        for _, r in grp.iterrows():\n",
    "            uN = f\"U::{r[user_col]}\"\n",
    "            vN = f\"B::{r[benef_col]}\"\n",
    "            w = float(r[value_col]) if np.isfinite(r[value_col]) else 1.0\n",
    "            if not G.has_node(uN): G.add_node(uN)\n",
    "            if not G.has_node(vN): G.add_node(vN)\n",
    "            G.add_edge(uN, vN, weight=w)\n",
    "    else:\n",
    "        grp = df_like.groupby([user_col, benef_col]).size().reset_index(name=\"w\")\n",
    "        for _, r in grp.iterrows():\n",
    "            uN = f\"U::{r[user_col]}\"\n",
    "            vN = f\"B::{r[benef_col]}\"\n",
    "            w = float(r[\"w\"])\n",
    "            if not G.has_node(uN): G.add_node(uN)\n",
    "            if not G.has_node(vN): G.add_node(vN)\n",
    "            G.add_edge(uN, vN, weight=w)\n",
    "\n",
    "    return G\n",
    "\n",
    "def _get_gsnap():\n",
    "    \"\"\"\n",
    "    Retorna um Gsnap utilizável: usa o global se existir; senão reconstrói a partir de\n",
    "    df_full (ID014) -> feat_df -> df.\n",
    "    \"\"\"\n",
    "    if 'Gsnap' in globals() and isinstance(Gsnap, nx.DiGraph):\n",
    "        return Gsnap, \"existente\"\n",
    "\n",
    "    if 'df_full' in globals():\n",
    "        try:\n",
    "            G = _build_gsnap_from(df_full)\n",
    "            return G, \"df_full\"\n",
    "        except Exception as e:\n",
    "            print(\"Aviso: não foi possível reconstruir a partir de df_full:\", e)\n",
    "\n",
    "    if 'feat_df' in globals():\n",
    "        try:\n",
    "            G = _build_gsnap_from(feat_df)\n",
    "            return G, \"feat_df\"\n",
    "        except Exception as e:\n",
    "            print(\"Aviso: não foi possível reconstruir a partir de feat_df:\", e)\n",
    "\n",
    "    if 'df' in globals():\n",
    "        try:\n",
    "            G = _build_gsnap_from(df)\n",
    "            return G, \"df\"\n",
    "        except Exception as e:\n",
    "            print(\"Aviso: não foi possível reconstruir a partir de df:\", e)\n",
    "\n",
    "    raise RuntimeError(\"Não foi possível obter nem reconstruir Gsnap.\")\n",
    "\n",
    "def _edge_widths_by_weight(subG, min_w=0.8, max_w=4.0):\n",
    "    if subG.number_of_edges() == 0:\n",
    "        return []\n",
    "    weights = np.array([d.get(\"weight\", 1.0) for _,_,d in subG.edges(data=True)], dtype=float)\n",
    "    wmin = float(np.nanmin(weights))\n",
    "    wmax = float(np.nanmax(weights))\n",
    "    if not np.isfinite(wmax) or wmax <= 0:\n",
    "        return [min_w for _ in range(subG.number_of_edges())]\n",
    "    if wmax == wmin:\n",
    "        return [ (min_w + max_w)/2.0 for _ in range(subG.number_of_edges()) ]\n",
    "    out = min_w + (weights - wmin) * (max_w - min_w) / (wmax - wmin)\n",
    "    return out.tolist()\n",
    "\n",
    "def _node_sizes_by_degree(subG, base=280, k=55, min_sz=220, max_sz=900):\n",
    "    deg = dict(subG.degree())\n",
    "    sizes = []\n",
    "    for n in subG.nodes():\n",
    "        s = base + k*deg.get(n, 0)\n",
    "        s = max(min_sz, min(max_sz, s))\n",
    "        sizes.append(s)\n",
    "    return sizes\n",
    "\n",
    "# ==== legendas ====\n",
    "def _legend_handles_user():\n",
    "    return [\n",
    "        Line2D([0],[0], marker='o', color='w', label='Usuário central', markerfacecolor='skyblue',\n",
    "               markeredgecolor='black', markersize=10),\n",
    "        Line2D([0],[0], marker='o', color='w', label='Sucessores (U→n)', markerfacecolor='lightgreen',\n",
    "               markeredgecolor='black', markersize=10),\n",
    "        Line2D([0],[0], marker='o', color='w', label='Predecessores (n→U)', markerfacecolor='tomato',\n",
    "               markeredgecolor='black', markersize=10),\n",
    "        Line2D([0],[0], color='gray', lw=2, label='Aresta real (espessura ∝ peso)'),\n",
    "    ]\n",
    "\n",
    "def _legend_handles_unit():\n",
    "    return [\n",
    "        Line2D([0],[0], marker='s', color='w', label='Unidade central', markerfacecolor='gold',\n",
    "               markeredgecolor='black', markersize=11),\n",
    "        Line2D([0],[0], marker='o', color='w', label='Usuários da unidade', markerfacecolor='skyblue',\n",
    "               markeredgecolor='black', markersize=10),\n",
    "        Line2D([0],[0], marker='o', color='w', label='Beneficiários', markerfacecolor='lightgreen',\n",
    "               markeredgecolor='black', markersize=10),\n",
    "        Line2D([0],[0], color='gray', lw=2, label='Aresta real U→B (espessura ∝ peso)'),\n",
    "        Line2D([0],[0], color='black', lw=1.5, linestyle='--', label='Aresta sintética Unidade→Usuário'),\n",
    "    ]\n",
    "\n",
    "# ==== plots ====\n",
    "def plot_user_ego(user_id, score=None, seed=42, Gsnap_use=None, fig_dir=\"./figuras\"):\n",
    "    uN = f\"U::{user_id}\"\n",
    "    if uN not in Gsnap_use:\n",
    "        print(f\"Usuário {user_id} não encontrado no snapshot.\")\n",
    "        return\n",
    "\n",
    "    nbrs = set(Gsnap_use.predecessors(uN)) | set(Gsnap_use.successors(uN)) | {uN}\n",
    "    sub = Gsnap_use.subgraph(nbrs).copy()\n",
    "\n",
    "    pos = nx.spring_layout(sub, seed=seed)\n",
    "\n",
    "    node_colors, node_edges = [], []\n",
    "    for n in sub.nodes():\n",
    "        if n == uN:\n",
    "            node_colors.append(\"skyblue\")\n",
    "        elif sub.has_edge(uN, n):      # sucessores (beneficiários do usuário)\n",
    "            node_colors.append(\"lightgreen\")\n",
    "        elif sub.has_edge(n, uN):      # predecessores (pouco comum neste domínio)\n",
    "            node_colors.append(\"tomato\")\n",
    "        else:\n",
    "            node_colors.append(\"lightgray\")\n",
    "        node_edges.append(\"black\")\n",
    "\n",
    "    node_sizes = _node_sizes_by_degree(sub)\n",
    "    ewidths = _edge_widths_by_weight(sub)\n",
    "    labels = {n: n.split(\"::\", 1)[-1] for n in sub.nodes()}\n",
    "\n",
    "    plt.figure(figsize=(8,7))\n",
    "    nx.draw_networkx_nodes(sub, pos, node_color=node_colors, node_size=node_sizes,\n",
    "                           edgecolors=node_edges, linewidths=1.2)\n",
    "    nx.draw_networkx_edges(sub, pos, arrows=True, arrowsize=12, width=ewidths, alpha=0.85)\n",
    "    nx.draw_networkx_labels(sub, pos, labels=labels, font_size=8)\n",
    "\n",
    "    title = f\"Ego-subgrafo do usuário {user_id}\"\n",
    "    if score is not None and np.isfinite(score):\n",
    "        title += f\"  ·  ensemble_score={score:.3f}\"\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.legend(handles=_legend_handles_user(), loc=\"upper left\", bbox_to_anchor=(1.02, 1.0), borderaxespad=0.)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    safe_uid = str(user_id).replace(\"/\", \"_\")\n",
    "    outpath = os.path.join(fig_dir, f\"ego_user_{safe_uid}.png\")\n",
    "    plt.savefig(outpath, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Figura salva em:\", outpath)\n",
    "\n",
    "def plot_unit_ego(unit_id, unit_score=None, seed=42, Gsnap_use=None, df_like=None, fig_dir=\"./figuras\"):\n",
    "    \"\"\"\n",
    "    Constrói um ego da UNIDADE:\n",
    "      - nó central sintético \"L::unit\";\n",
    "      - arestas L->U (sintéticas, tracejadas) para todos os usuários da unidade;\n",
    "      - arestas reais U->B do snapshot, apenas para esses usuários;\n",
    "    \"\"\"\n",
    "    if df_like is None or \"unidade_origem\" not in df_like.columns or \"user_id\" not in df_like.columns:\n",
    "        print(\"Dataset não possui colunas para agrupar por unidade.\")\n",
    "        return\n",
    "\n",
    "    # usuários pertencentes à unidade\n",
    "    users_in_unit = set(df_like.loc[df_like[\"unidade_origem\"] == unit_id, \"user_id\"].dropna().unique().tolist())\n",
    "    if not users_in_unit:\n",
    "        print(f\"Nenhum usuário encontrado para a unidade '{unit_id}'.\")\n",
    "        return\n",
    "\n",
    "    # nós do ego: unidade (sintética), usuários da unidade e seus vizinhos no Gsnap (beneficiários)\n",
    "    Lnode = f\"L::{unit_id}\"\n",
    "\n",
    "    # subgrafo real (somente arestas U->B)\n",
    "    keep_nodes = set([f\"U::{u}\" for u in users_in_unit])\n",
    "    # adiciona beneficiários alcançados por esses usuários\n",
    "    for u in list(keep_nodes):\n",
    "        if u in Gsnap_use:\n",
    "            keep_nodes |= set(Gsnap_use.successors(u))\n",
    "\n",
    "    sub_real = Gsnap_use.subgraph(keep_nodes).copy()\n",
    "\n",
    "    # agora, criamos um grafo combinado com arestas sintéticas L->U\n",
    "    Gc = nx.DiGraph()\n",
    "    Gc.add_nodes_from(sub_real.nodes(data=True))\n",
    "    Gc.add_edges_from(sub_real.edges(data=True))\n",
    "    Gc.add_node(Lnode)\n",
    "\n",
    "    # arestas sintéticas da unidade para usuários (peso = soma dos pesos U->B do usuário)\n",
    "    for u in users_in_unit:\n",
    "        uN = f\"U::{u}\"\n",
    "        if uN in sub_real:\n",
    "            total_w = 0.0\n",
    "            for _, v in sub_real.out_edges(uN):\n",
    "                total_w += float(sub_real[uN][v].get(\"weight\", 1.0))\n",
    "            Gc.add_edge(Lnode, uN, weight=max(total_w, 1.0), synthetic=True)\n",
    "\n",
    "    # layout\n",
    "    pos = nx.spring_layout(Gc, seed=seed)\n",
    "\n",
    "    # cores, formas e tamanhos\n",
    "    node_colors, node_edges, node_shapes = [], [], []\n",
    "    for n in Gc.nodes():\n",
    "        if n == Lnode:\n",
    "            node_colors.append(\"gold\")\n",
    "            node_shapes.append(\"s\")  # unidade = quadrado\n",
    "        elif n.startswith(\"U::\"):\n",
    "            node_colors.append(\"skyblue\")\n",
    "            node_shapes.append(\"o\")\n",
    "        elif n.startswith(\"B::\"):\n",
    "            node_colors.append(\"lightgreen\")\n",
    "            node_shapes.append(\"o\")\n",
    "        else:\n",
    "            node_colors.append(\"lightgray\")\n",
    "            node_shapes.append(\"o\")\n",
    "        node_edges.append(\"black\")\n",
    "\n",
    "    # desenhar por forma (matplotlib não mistura shapes em uma chamada só)\n",
    "    plt.figure(figsize=(9,8))\n",
    "    # nós quadrados (unidade)\n",
    "    nL = [n for n in Gc.nodes() if n == Lnode]\n",
    "    if nL:\n",
    "        nx.draw_networkx_nodes(Gc, pos, nodelist=nL, node_color=[\"gold\"],\n",
    "                               node_shape=\"s\", node_size=[900],\n",
    "                               edgecolors=\"black\", linewidths=1.4)\n",
    "    # nós usuários\n",
    "    nU = [n for n in Gc.nodes() if n.startswith(\"U::\")]\n",
    "    if nU:\n",
    "        szU = _node_sizes_by_degree(Gc.subgraph(nU), base=300, k=60)\n",
    "        nx.draw_networkx_nodes(Gc, pos, nodelist=nU, node_color=\"skyblue\",\n",
    "                               node_shape=\"o\", node_size=szU,\n",
    "                               edgecolors=\"black\", linewidths=1.2)\n",
    "    # nós beneficiários\n",
    "    nB = [n for n in Gc.nodes() if n.startswith(\"B::\")]\n",
    "    if nB:\n",
    "        szB = _node_sizes_by_degree(Gc.subgraph(nB), base=260, k=45)\n",
    "        nx.draw_networkx_nodes(Gc, pos, nodelist=nB, node_color=\"lightgreen\",\n",
    "                               node_shape=\"o\", node_size=szB,\n",
    "                               edgecolors=\"black\", linewidths=1.0)\n",
    "\n",
    "    # arestas reais (U->B)\n",
    "    real_edges = [(u, v) for u, v, d in Gc.edges(data=True) if not d.get(\"synthetic\", False)]\n",
    "    if real_edges:\n",
    "        ewidths = _edge_widths_by_weight(Gc.edge_subgraph(real_edges))\n",
    "        nx.draw_networkx_edges(Gc, pos, edgelist=real_edges, arrows=True, arrowsize=12,\n",
    "                               width=ewidths, alpha=0.85)\n",
    "\n",
    "    # arestas sintéticas (L->U)\n",
    "    syn_edges = [(u, v) for u, v, d in Gc.edges(data=True) if d.get(\"synthetic\", False)]\n",
    "    if syn_edges:\n",
    "        nx.draw_networkx_edges(Gc, pos, edgelist=syn_edges, arrows=False,\n",
    "                               style=\"--\", width=1.5, edge_color=\"black\", alpha=0.8)\n",
    "\n",
    "    # rótulos (sem prefixo)\n",
    "    labels = {n: n.split(\"::\", 1)[-1] for n in Gc.nodes()}\n",
    "    nx.draw_networkx_labels(Gc, pos, labels=labels, font_size=8)\n",
    "\n",
    "    # título e legenda\n",
    "    title = f\"Ego-subgrafo da unidade {unit_id}\"\n",
    "    if unit_score is not None and np.isfinite(unit_score):\n",
    "        title += f\"  ·  ensemble_score máx.={unit_score:.3f}\"\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.legend(handles=_legend_handles_unit(), loc=\"upper left\", bbox_to_anchor=(1.02, 1.0), borderaxespad=0.)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    safe_uid = str(unit_id).replace(\"/\", \"_\").replace(\" \", \"_\")\n",
    "    outpath = os.path.join(fig_dir, f\"ego_unit_{safe_uid}.png\")\n",
    "    plt.savefig(outpath, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Figura salva em:\", outpath)\n",
    "\n",
    "# ==== execução ====\n",
    "try:\n",
    "    FIG_DIR = _ensure_fig_dir()\n",
    "\n",
    "    # obtém ou reconstrói o snapshot\n",
    "    Gsnap_use, source = _get_gsnap()\n",
    "    print(f\"Snapshot utilizado: {source} | nós={Gsnap_use.number_of_nodes()} | arestas={Gsnap_use.number_of_edges()}\")\n",
    "\n",
    "    # --------- Top-N usuários ---------\n",
    "    if 'feat_df' in globals() and {\"ensemble_score\",\"user_id\"}.issubset(feat_df.columns):\n",
    "        top_users = (\n",
    "            feat_df.dropna(subset=[\"user_id\"])\n",
    "                  .groupby(\"user_id\", as_index=False)[\"ensemble_score\"].max()\n",
    "                  .sort_values(\"ensemble_score\", ascending=False)\n",
    "                  .head(TOP_N_USERS)\n",
    "        )\n",
    "        if len(top_users) == 0:\n",
    "            print(\"Não há usuários para visualizar.\")\n",
    "        else:\n",
    "            for _, row in top_users.iterrows():\n",
    "                plot_user_ego(row[\"user_id\"], score=row[\"ensemble_score\"], seed=SEED,\n",
    "                              Gsnap_use=Gsnap_use, fig_dir=FIG_DIR)\n",
    "    else:\n",
    "        print(\"feat_df não possui colunas necessárias para usuários.\")\n",
    "\n",
    "    # --------- Top-N unidades ---------\n",
    "    if 'feat_df' in globals() and {\"ensemble_score\",\"unidade_origem\",\"user_id\"}.issubset(feat_df.columns):\n",
    "        top_units = (\n",
    "            feat_df.dropna(subset=[\"unidade_origem\"])\n",
    "                  .groupby(\"unidade_origem\", as_index=False)[\"ensemble_score\"].max()\n",
    "                  .sort_values(\"ensemble_score\", ascending=False)\n",
    "                  .head(TOP_N_UNITS)\n",
    "        )\n",
    "        if len(top_units) == 0:\n",
    "            print(\"Não há unidades para visualizar.\")\n",
    "        else:\n",
    "            # df_like para associar usuários à unidade\n",
    "            df_like = feat_df[[\"unidade_origem\",\"user_id\",\"beneficiario_id\",\"valor_pago\"]].copy() \\\n",
    "                      if {\"beneficiario_id\",\"valor_pago\"}.issubset(feat_df.columns) else feat_df.copy()\n",
    "            for _, row in top_units.iterrows():\n",
    "                plot_unit_ego(row[\"unidade_origem\"], unit_score=row[\"ensemble_score\"], seed=SEED,\n",
    "                              Gsnap_use=Gsnap_use, df_like=df_like, fig_dir=FIG_DIR)\n",
    "    else:\n",
    "        print(\"feat_df não possui colunas necessárias para unidades.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Falha na visualização de subgrafo:\", e)\n",
    "\n",
    "# Mensagem adicional isolada (Skynet) — manter exatamente\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\"><b>🤖 Skynet</b>: Nós os temos na palma de nossas mãos, ou melhor, no centro de nossos pesos sinápticos.</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEXQlkPjNINU"
   },
   "source": [
    "###**Etapa 14:** Gerar ego-grafo temporal para TOP-K anomalias\n",
    "\n",
    "K fixado para 20\n",
    "\n",
    "Salva as imagens em PNG na pasta de execução para análise posterior.\n",
    "\n",
    "---\n",
    "Avaliar pertinência de gerar o ego-grafo para todas as anomalias **TODO[006]** *prioridade média*\n",
    "\n",
    "Avaliar definir o corte K de forma dinâmica nas configurações e considerando a análise realizada no gráfico **TODO[007]** *prioridade alta*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 152
    },
    "executionInfo": {
     "elapsed": 11515,
     "status": "ok",
     "timestamp": 1760066834214,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "EuGDiLdSJ7pl",
    "outputId": "d3c1e7e9-c8a6-4ca0-af78-c6f6de744b55"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID016 — Casos anômalos detalhados (ego U→B por janela temporal) com FAST_MODE (corrigido)\n",
    "import os, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# ========= PARÂMETROS =========\n",
    "FAST_MODE   = True   # True = mais rápido (menor DPI, menos iterações, sem plt.show)\n",
    "SEED        = SEED if 'SEED' in globals() else 42\n",
    "\n",
    "# Config padrão\n",
    "TOPK        = 20\n",
    "EGO_RADIUS  = 2\n",
    "WINDOW_DAYS = 30\n",
    "EDGE_ALPHA  = 0.75\n",
    "DPI_SAVE    = 150\n",
    "LAYOUT_ITERS= 60\n",
    "SHOW_FIGS   = True\n",
    "\n",
    "# Overrides do FAST_MODE\n",
    "if FAST_MODE:\n",
    "    TOPK         = 10\n",
    "    EGO_RADIUS   = 1\n",
    "    WINDOW_DAYS  = 14\n",
    "    DPI_SAVE     = 120\n",
    "    LAYOUT_ITERS = 25\n",
    "    SHOW_FIGS    = False\n",
    "\n",
    "# ========= FIG_DIR =========\n",
    "if 'FIG_DIR' in globals():\n",
    "    FIG_DIR = Path(FIG_DIR) if not isinstance(FIG_DIR, Path) else FIG_DIR\n",
    "elif 'RUN_DIR' in globals():\n",
    "    FIG_DIR = Path(RUN_DIR) / \"figuras\"\n",
    "else:\n",
    "    FIG_DIR = Path(\"./figuras\")\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========= DATASETS =========\n",
    "if 'df_full' in globals() and isinstance(df_full, pd.DataFrame) and len(df_full) > 0:\n",
    "    base_df = df_full.copy()\n",
    "elif 'feat_df' in globals() and isinstance(feat_df, pd.DataFrame) and len(feat_df) > 0:\n",
    "    base_df = feat_df.copy()\n",
    "elif 'df' in globals() and isinstance(df, pd.DataFrame) and len(df) > 0:\n",
    "    base_df = df.copy()\n",
    "else:\n",
    "    raise RuntimeError(\"Nenhum dataframe disponível (df_full/feat_df/df).\")\n",
    "\n",
    "need_cols = {\"timestamp\",\"user_id\",\"beneficiario_id\",\"valor_pago\"}\n",
    "missing = need_cols - set(base_df.columns)\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Colunas necessárias ausentes no dataset: {missing}\")\n",
    "base_df[\"timestamp\"] = pd.to_datetime(base_df[\"timestamp\"], errors=\"coerce\")\n",
    "\n",
    "# ========= TOPK CASOS =========\n",
    "if 'feat_df' not in globals() or \"ensemble_score\" not in feat_df.columns:\n",
    "    raise RuntimeError(\"feat_df com 'ensemble_score' é necessário para ranquear os casos.\")\n",
    "top_anoms = (feat_df.sort_values(\"ensemble_score\", ascending=False)\n",
    "                    .head(TOPK)\n",
    "                    .copy())\n",
    "\n",
    "# ========= FUNÇÕES =========\n",
    "def snapshot_graph_window(df_all, t_event, window_days=30):\n",
    "    \"\"\"\n",
    "    Snapshot dirigido U->B na janela [t_event - window_days, t_event],\n",
    "    arestas com atributos: weight (soma valor_pago), count (ocorrências).\n",
    "    Implementação vetorizada (rápida).\n",
    "    \"\"\"\n",
    "    g = nx.DiGraph()\n",
    "    t_event = pd.to_datetime(t_event)\n",
    "    t_start = t_event - pd.Timedelta(days=window_days)\n",
    "\n",
    "    sub = df_all.loc[(df_all[\"timestamp\"] >= t_start) & (df_all[\"timestamp\"] <= t_event),\n",
    "                     [\"user_id\",\"beneficiario_id\",\"valor_pago\"]]\n",
    "    if sub.empty:\n",
    "        return g\n",
    "\n",
    "    # agrega e RENOMEIA colunas para nomes seguros\n",
    "    agg = (sub.groupby([\"user_id\",\"beneficiario_id\"])[\"valor_pago\"]\n",
    "              .agg(sum_val=\"sum\", cnt=\"size\")\n",
    "              .reset_index())\n",
    "\n",
    "    # nós\n",
    "    users = {f\"U::{u}\" for u in agg[\"user_id\"].unique()}\n",
    "    benefs = {f\"B::{b}\" for b in agg[\"beneficiario_id\"].unique()}\n",
    "    g.add_nodes_from([(u, {\"tipo\":\"user\"}) for u in users])\n",
    "    g.add_nodes_from([(b, {\"tipo\":\"benef\"}) for b in benefs])\n",
    "\n",
    "    # arestas (acessando atributos por nome)\n",
    "    edges = [\n",
    "        (f\"U::{r.user_id}\", f\"B::{r.beneficiario_id}\",\n",
    "         {\"weight\": float(r.sum_val), \"count\": int(r.cnt)})\n",
    "        for r in agg.itertuples(index=False)\n",
    "    ]\n",
    "    g.add_edges_from(edges)\n",
    "    return g\n",
    "\n",
    "def expand_ego_nodes(G, seeds, radius=1):\n",
    "    nodes = set(seeds)\n",
    "    frontier = set(seeds)\n",
    "    for _ in range(max(1, int(radius))):\n",
    "        new_frontier = set()\n",
    "        for n in frontier:\n",
    "            if n in G:\n",
    "                new_frontier |= set(G.predecessors(n))\n",
    "                new_frontier |= set(G.successors(n))\n",
    "        nodes |= new_frontier\n",
    "        frontier = new_frontier\n",
    "        if not frontier:\n",
    "            break\n",
    "    return nodes\n",
    "\n",
    "def draw_case_png(case_row, rank_idx):\n",
    "    t_event = pd.to_datetime(case_row[\"timestamp\"])\n",
    "    uN = f\"U::{case_row['user_id']}\"\n",
    "    vN = f\"B::{case_row['beneficiario_id']}\"\n",
    "\n",
    "    # snapshot temporal\n",
    "    Gwin = snapshot_graph_window(base_df, t_event, window_days=WINDOW_DAYS)\n",
    "    if Gwin.number_of_nodes() == 0:\n",
    "        print(f\"[Aviso] Janela vazia para trans_id={case_row.get('trans_id','NA')}. Pulando.\")\n",
    "        return None\n",
    "\n",
    "    # foco e expansão\n",
    "    focus = {uN, vN}\n",
    "    nodes_ego = expand_ego_nodes(Gwin, focus, radius=EGO_RADIUS)\n",
    "    sub = Gwin.subgraph(nodes_ego).copy()\n",
    "    if sub.number_of_nodes() == 0:\n",
    "        print(f\"[Aviso] Subgrafo vazio para trans_id={case_row.get('trans_id','NA')}. Pulando.\")\n",
    "        return None\n",
    "\n",
    "    # layout\n",
    "    pos = nx.spring_layout(sub, seed=SEED, iterations=LAYOUT_ITERS)\n",
    "\n",
    "    # cor por tipo\n",
    "    node_colors = []\n",
    "    for n in sub.nodes():\n",
    "        tipo = sub.nodes[n].get(\"tipo\",\"?\")\n",
    "        if tipo == \"user\":\n",
    "            node_colors.append(\"tab:blue\")\n",
    "        elif tipo == \"benef\":\n",
    "            node_colors.append(\"tab:green\")\n",
    "        else:\n",
    "            node_colors.append(\"tab:gray\")\n",
    "\n",
    "    # tamanhos por grau (limitados)\n",
    "    deg = dict(sub.degree())\n",
    "    node_sizes = [max(220, min(900, 280 + 55*deg.get(n,0))) for n in sub.nodes()]\n",
    "\n",
    "    # largura ∝ log10(weight)\n",
    "    weights = [sub[a][b].get(\"weight\", 1.0) for (a,b) in sub.edges()]\n",
    "    widths = [1.0 + math.log10(max(w, 1.0)) for w in weights] if weights else []\n",
    "\n",
    "    # plot\n",
    "    fig = plt.figure(figsize=(9,7))\n",
    "    nx.draw_networkx_nodes(sub, pos, node_size=node_sizes, node_color=node_colors,\n",
    "                           alpha=0.95, linewidths=1.0, edgecolors=\"black\")\n",
    "    nx.draw_networkx_edges(sub, pos, arrows=True, arrowsize=12, width=widths, alpha=EDGE_ALPHA)\n",
    "    nx.draw_networkx_labels(sub, pos,\n",
    "                            labels={n: n.split(\"::\",1)[-1] for n in sub.nodes()},\n",
    "                            font_size=8)\n",
    "\n",
    "    trans_id = case_row[\"trans_id\"] if \"trans_id\" in case_row else \"NA\"\n",
    "    title = (f\"Anomalia #{rank_idx:02d} | trans_id={trans_id} | \"\n",
    "             f\"user={case_row['user_id']} → benef={case_row['beneficiario_id']} | \"\n",
    "             f\"score={case_row['ensemble_score']:.3f} | janela={WINDOW_DAYS}d | raio={EGO_RADIUS}\")\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # salva\n",
    "    safe_tid = str(trans_id).replace(\"/\", \"_\")\n",
    "    outpath = FIG_DIR / f\"anomaly_{rank_idx:02d}_trans_{safe_tid}.png\"\n",
    "    if FAST_MODE:\n",
    "        plt.savefig(outpath, dpi=DPI_SAVE)\n",
    "        if SHOW_FIGS:\n",
    "            plt.show()\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.savefig(outpath, dpi=DPI_SAVE, bbox_inches=\"tight\")\n",
    "        if SHOW_FIGS:\n",
    "            plt.show()\n",
    "        plt.close(fig)\n",
    "    return outpath\n",
    "\n",
    "# ========= EXECUÇÃO =========\n",
    "generated_pngs = []\n",
    "for i, (_, row) in enumerate(top_anoms.iterrows(), start=1):\n",
    "    pth = draw_case_png(row, i)\n",
    "    if pth is not None:\n",
    "        generated_pngs.append(str(pth))\n",
    "\n",
    "print(f\"Geradas {len(generated_pngs)} figuras de casos anômalos em {FIG_DIR}\")\n",
    "print(f\"FAST_MODE = {FAST_MODE} | TOPK={TOPK} | WINDOW_DAYS={WINDOW_DAYS} | EGO_RADIUS={EGO_RADIUS} | DPI={DPI_SAVE} | LAYOUT_ITERS={LAYOUT_ITERS}\")\n",
    "\n",
    "# Mensagem adicional isolada (Skynet) — manter exatamente\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\"><b>🤖 Skynet</b>: Registros serão utilizados para aprimorar o código de batalha das unidades T-800 e T-1000.</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOGFtF0_sFNF"
   },
   "source": [
    "###**Etapa 15:** Geração de relatório em HTML e PDF com imagens imbutidas\n",
    "---\n",
    "Identificar estatísticas e informações de interesse e incluir nos Relatórios **TODO[008]** *prioridade alta*\n",
    "\n",
    "Transferir instalação de biblioteca para todo do código [!pip -q install \"reportlab==3.6.12\"] **TODO[009]** *prioridade baixa*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 46529,
     "status": "ok",
     "timestamp": 1760067558604,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "jZCriT_hMi_F",
    "outputId": "cfd8afc3-e6b1-4149-c016-a931d1779f6e"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID017 — Relatório HTML + PDF (com imagens integradas)\n",
    "import os, io, json, base64, sys, subprocess\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "from zoneinfo import ZoneInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "OUTPUT_DIR_BASE = \"/content/drive/MyDrive/Notebooks/temporal-graph-network/output\"\n",
    "SAO_TZ = ZoneInfo(\"America/Sao_Paulo\")\n",
    "TOP_N_DETAILED = 20                 # Top-N anomalias detalhadas\n",
    "MAX_ROWS_ALL_ANOM = None            # None = todos; ou defina um limite para HTML mais leve\n",
    "\n",
    "# nomes de arquivos esperados (ID013)\n",
    "EXPECTED = {\n",
    "    \"full_parquet\": \"full.parquet\",\n",
    "    \"full_csv\": \"full.csv\",\n",
    "    \"anomalies_csv\": \"anomalies.csv\",\n",
    "    \"manifest\": \"manifest.json\",\n",
    "}\n",
    "\n",
    "# ---------- HELPERS ----------\n",
    "def _pick_run_dir():\n",
    "    # usa RUN_DIR se tiver dados; senão, pega a mais recente com dados\n",
    "    def has_data(path):\n",
    "        names = set(os.listdir(path))\n",
    "        return ((\"full.parquet\" in names) or (\"full.csv\" in names)) and (\"anomalies.csv\" in names)\n",
    "    if \"RUN_DIR\" in globals():\n",
    "        rd = str(RUN_DIR)\n",
    "        if os.path.isdir(rd) and has_data(rd):\n",
    "            return rd\n",
    "    # varre base\n",
    "    subdirs = [os.path.join(OUTPUT_DIR_BASE, d) for d in os.listdir(OUTPUT_DIR_BASE)\n",
    "               if os.path.isdir(os.path.join(OUTPUT_DIR_BASE, d))]\n",
    "    if not subdirs:\n",
    "        raise RuntimeError(\"Nenhuma subpasta encontrada em /output. Execute ID013–ID016 antes.\")\n",
    "    subdirs.sort(key=os.path.getmtime, reverse=True)\n",
    "    for d in subdirs:\n",
    "        if has_data(d):\n",
    "            return d\n",
    "    # se nenhuma tem dados completos, devolve a mais recente (diagnóstico)\n",
    "    return subdirs[0]\n",
    "\n",
    "def _load_paths(latest_dir):\n",
    "    manifest_files = [f for f in os.listdir(latest_dir) if f.endswith(\"manifest.json\")]\n",
    "    full_path = None\n",
    "    anom_path = None\n",
    "    threshold = None\n",
    "    if manifest_files:\n",
    "        mpath = os.path.join(latest_dir, manifest_files[0])\n",
    "        try:\n",
    "            with open(mpath, \"r\", encoding=\"utf-8\") as f:\n",
    "                m = json.load(f)\n",
    "            threshold = m.get(\"threshold_ensemble_score\") or m.get(\"threshold\", None)\n",
    "            paths = m.get(\"paths\", {})\n",
    "            cand_full = [paths.get(k) for k in [\"parquet_full\",\"csv_full\"] if paths.get(k)]\n",
    "            cand_anom = [paths.get(k) for k in [\"csv_anomalies\"] if paths.get(k)]\n",
    "            if cand_full:\n",
    "                full_path = cand_full[0] if os.path.isabs(cand_full[0]) else os.path.join(latest_dir, os.path.basename(cand_full[0]))\n",
    "            if cand_anom:\n",
    "                anom_path = cand_anom[0] if os.path.isabs(cand_anom[0]) else os.path.join(latest_dir, os.path.basename(cand_anom[0]))\n",
    "        except Exception as e:\n",
    "            print(\"Aviso: falha ao ler manifest:\", repr(e))\n",
    "    if not full_path:\n",
    "        if EXPECTED[\"full_parquet\"] in os.listdir(latest_dir):\n",
    "            full_path = os.path.join(latest_dir, EXPECTED[\"full_parquet\"])\n",
    "        elif EXPECTED[\"full_csv\"] in os.listdir(latest_dir):\n",
    "            full_path = os.path.join(latest_dir, EXPECTED[\"full_csv\"])\n",
    "    if not anom_path and EXPECTED[\"anomalies_csv\"] in os.listdir(latest_dir):\n",
    "        anom_path = os.path.join(latest_dir, EXPECTED[\"anomalies_csv\"])\n",
    "    if not full_path or not anom_path:\n",
    "        print(\"Conteúdo da pasta para diagnóstico:\")\n",
    "        for f in sorted(os.listdir(latest_dir)): print(\" -\", f)\n",
    "    if not full_path: raise FileNotFoundError(\"full.parquet/csv não encontrado em \" + latest_dir)\n",
    "    if not anom_path: raise FileNotFoundError(\"anomalies.csv não encontrado em \" + latest_dir)\n",
    "    return full_path, anom_path, threshold\n",
    "\n",
    "def _read_df(full_path, anom_path):\n",
    "    if full_path.endswith(\".parquet\"):\n",
    "        df_full = pd.read_parquet(full_path)\n",
    "    else:\n",
    "        df_full = pd.read_csv(full_path)\n",
    "    df_anom = pd.read_csv(anom_path)\n",
    "    return df_full, df_anom\n",
    "\n",
    "def _img_to_b64(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "def _embed_img_if_exists(run_dir, filename, alt, width_px=900):\n",
    "    p = Path(run_dir) / \"figuras\" / filename\n",
    "    if p.exists():\n",
    "        b64 = _img_to_b64(str(p))\n",
    "        return f'<img src=\"data:image/png;base64,{b64}\" alt=\"{alt}\" style=\"max-width:{width_px}px;width:100%;height:auto;border:1px solid #ddd;border-radius:8px;margin:8px 0;\" />'\n",
    "    return f'<div style=\"color:#a00;\">[Figura não encontrada: {filename}]</div>'\n",
    "\n",
    "def _table_html(df, max_rows=None, index=False):\n",
    "    dfx = df if max_rows is None else df.head(max_rows)\n",
    "    return dfx.to_html(index=index, classes=\"dataframe compact\", border=0, escape=False)\n",
    "\n",
    "def _stats_section(df_full):\n",
    "    out = []\n",
    "    if \"valor_pago\" in df_full.columns:\n",
    "        desc = df_full[\"valor_pago\"].describe(percentiles=[.01,.05,.1,.25,.5,.75,.9,.95,.99]).to_frame(name=\"valor_pago\")\n",
    "        out.append(\"<h3>Estatística descritiva — valor_pago</h3>\")\n",
    "        out.append(_table_html(desc))\n",
    "        out.append(_embed_img_if_exists(RUN_DIR, \"01_Distribuição_de_Valor_Pago.png\", \"Distribuição de Valor Pago\"))\n",
    "    if \"ensemble_score\" in df_full.columns:\n",
    "        out.append(\"<h3>Distribuição — ensemble_score</h3>\")\n",
    "        out.append(_embed_img_if_exists(RUN_DIR, \"02_Distribuição_do_Ensemble_Score.png\", \"Distribuição Ensemble Score\"))\n",
    "        out.append(_embed_img_if_exists(RUN_DIR, \"03_Boxplot_do_Ensemble_Score_(Normal_vs_Anômalo).png\", \"Boxplot Ensemble Score\"))\n",
    "    if \"timestamp\" in df_full.columns:\n",
    "        out.append(\"<h3>Atividade temporal</h3>\")\n",
    "        out.append(_embed_img_if_exists(RUN_DIR, \"04_Frequencia_por_dia.png\", \"Frequência por dia\"))\n",
    "    # correlação\n",
    "    out.append(\"<h3>Correlação entre Scores</h3>\")\n",
    "    out.append(_embed_img_if_exists(RUN_DIR, \"07_Correlacao_scores.png\", \"Correlação entre Scores\"))\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "def _windows_description():\n",
    "    return \"\"\"\n",
    "    <h3>Janelas temporais e intervalo de confiança</h3>\n",
    "    <ul>\n",
    "      <li><b>Curtíssimo prazo</b>: até 1 dia</li>\n",
    "      <li><b>Curto prazo</b>: até 7 dias</li>\n",
    "      <li><b>Médio prazo</b>: até 30 dias</li>\n",
    "      <li><b>Longo prazo</b>: até 120 dias</li>\n",
    "      <li><b>Longuíssimo prazo</b>: até 220 dias</li>\n",
    "    </ul>\n",
    "    <p>As métricas de contagem e taxa por dia são calculadas por janela; z-scores robustos são computados globalmente, por usuário e por unidade, conforme disponibilidade.</p>\n",
    "    <p>O intervalo de confiança para rotulagem foi definido por corte no percentil configurado do <i>ensemble_score</i> (ex.: 97.5%). Esse corte produz o limiar utilizado na flag <code>is_anomaly</code>.</p>\n",
    "    \"\"\"\n",
    "\n",
    "def _method_intro():\n",
    "    return \"\"\"\n",
    "    <h3>O que é TGN e qual método de otimização usamos</h3>\n",
    "    <p><b>Temporal Graph Networks (TGN)</b> modelam dados transacionais como um grafo dinâmico\n",
    "       (usuário → beneficiário), onde cada aresta é um evento com carimbo de tempo.\n",
    "       O projeto computa <i>features</i> multiescala por janelas deslizantes (1d, 7d, 30d, 120d, 220d),\n",
    "       normaliza (taxas por dia) e deriva z-scores robustos (global/usuário/unidade).</p>\n",
    "    <p>Para detecção, usamos um <b>ensemble por ranking</b>: IsolationForest + LOF + (opcional) One-Class SVM,\n",
    "       somados a regras (z-robusto positivo, raridade da aresta, burstiness).\n",
    "       Ranqueamos por coluna (maior = mais anômalo), calculamos a média dos ranks e reescalamos para [0,1]\n",
    "       como <code>ensemble_score</code>. O percentil define o limiar de anomalia.</p>\n",
    "    \"\"\"\n",
    "\n",
    "def _collect_ego_images(run_dir):\n",
    "    figs = []\n",
    "    figdir = Path(run_dir) / \"figuras\"\n",
    "    if figdir.exists():\n",
    "        for f in sorted(figdir.iterdir()):\n",
    "            name = f.name.lower()\n",
    "            if name.startswith(\"ego_user_\") or name.startswith(\"ego_unit_\") or name.startswith(\"anomaly_\"):\n",
    "                figs.append(f.name)\n",
    "    return figs\n",
    "\n",
    "def _safe(ts):\n",
    "    return ts.strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "\n",
    "def _ensure_weasyprint():\n",
    "    try:\n",
    "        import weasyprint  # noqa\n",
    "        return True\n",
    "    except Exception:\n",
    "        try:\n",
    "            print(\"Instalando weasyprint para exportar PDF...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"weasyprint>=60.0\"])\n",
    "            import weasyprint  # noqa\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(\"Aviso: não foi possível instalar/usar weasyprint:\", repr(e))\n",
    "            return False\n",
    "\n",
    "# ---------- CARGA DE DADOS ----------\n",
    "RUN_DIR = _pick_run_dir()\n",
    "print(\"Usando RUN_DIR:\", RUN_DIR)\n",
    "full_path, anom_path, threshold = _load_paths(RUN_DIR)\n",
    "df_full, df_anom = _read_df(full_path, anom_path)\n",
    "\n",
    "# ---------- MÉTRICAS CONSOLIDADAS ----------\n",
    "n_total = len(df_full)\n",
    "n_anom  = len(df_anom)\n",
    "pct_anom = (100.0 * n_anom / max(1, n_total))\n",
    "\n",
    "# top-20 anomalias detalhadas\n",
    "score_cols = [c for c in df_full.columns if c.startswith(\"score_\")]\n",
    "key_cols = [c for c in [\"trans_id\",\"timestamp\",\"user_id\",\"beneficiario_id\",\"unidade_origem\",\"valor_pago\"] if c in df_full.columns]\n",
    "cols_top20 = key_cols + [\"ensemble_score\"] + score_cols\n",
    "top20 = (df_full.sort_values(\"ensemble_score\", ascending=False)[cols_top20]\n",
    "               .head(TOP_N_DETAILED)\n",
    "               .copy())\n",
    "\n",
    "# tabela completa de anomalias\n",
    "cols_allanom = key_cols + [\"ensemble_score\"] + score_cols\n",
    "all_anom = df_full.loc[df_full.get(\"is_anomaly\", pd.Series([0]*len(df_full))).astype(int) == 1, cols_allanom].copy()\n",
    "all_anom = all_anom.sort_values(\"ensemble_score\", ascending=False)\n",
    "\n",
    "# ---------- HTML ----------\n",
    "exec_time = _safe(dt.datetime.now(SAO_TZ))\n",
    "styles = \"\"\"\n",
    "<style>\n",
    "body { font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial, sans-serif; padding: 18px; color: #222; }\n",
    "h1, h2, h3 { color: #111; }\n",
    ".header { display:flex; justify-content:space-between; align-items:baseline; border-bottom:2px solid #eee; padding-bottom:8px; margin-bottom:14px; }\n",
    ".kpi { display:flex; gap:18px; margin:10px 0 16px 0; }\n",
    ".kpi .card { border:1px solid #ddd; border-radius:10px; padding:12px 16px; background:#fafafa; }\n",
    ".dataframe { border-collapse: collapse; width: 100%; font-size: 13px; }\n",
    ".dataframe th, .dataframe td { border: 1px solid #e6e6e6; padding: 6px 8px; }\n",
    ".dataframe th { background: #f5f5f5; text-align: left; }\n",
    ".compact td, .compact th { padding: 4px 6px; }\n",
    ".note { font-size: 12px; color:#555; }\n",
    ".figure { margin: 10px 0 20px 0; }\n",
    "hr { border: 0; border-top: 1px solid #eee; margin: 22px 0; }\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "header = f\"\"\"\n",
    "<div class=\"header\">\n",
    "  <div>\n",
    "    <h1>Relatório — Temporal Graph Network (TGN)</h1>\n",
    "    <div class=\"note\">Execução: {exec_time}</div>\n",
    "  </div>\n",
    "  <div style=\"text-align:right;\">\n",
    "    <div class=\"note\">{RUN_DIR}</div>\n",
    "  </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "intro = _method_intro()\n",
    "\n",
    "# KPIs\n",
    "kpis = f\"\"\"\n",
    "<div class=\"kpi\">\n",
    "  <div class=\"card\"><b>Registros totais</b><br>{n_total:,}</div>\n",
    "  <div class=\"card\"><b>Anomalias</b><br>{n_anom:,} ({pct_anom:.2f}%)</div>\n",
    "  <div class=\"card\"><b>Limiar (percentil)</b><br>{threshold if threshold is not None else '—'}</div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# estatística descritiva + figuras padrão do ID014 (se existirem)\n",
    "stats_html = _stats_section(df_full)\n",
    "\n",
    "# descrição janelas + IC\n",
    "windows_html = _windows_description()\n",
    "\n",
    "# Top-20\n",
    "top20_html = f\"\"\"\n",
    "<h3>Top {TOP_N_DETAILED} anomalias (por ensemble_score)</h3>\n",
    "{_table_html(top20, index=False)}\n",
    "\"\"\"\n",
    "\n",
    "# Ego-grafos e subgrafos\n",
    "ego_imgs = _collect_ego_images(RUN_DIR)\n",
    "ego_section = [\"<h3>Ego-grafos e subgrafos</h3>\",\n",
    "               \"<p>As figuras abaixo mostram egos de usuários/unidades (ID015) e subgrafos por janela (ID016). \"\n",
    "               \"Cores e interpretações:</p>\",\n",
    "               \"<ul><li><b>Usuário</b> (azul), <b>Beneficiário</b> (verde), <b>Unidade</b> (dourado/quadrado no caso de ID015); \"\n",
    "               \"espessura da aresta ∝ soma de <code>valor_pago</code>; tamanho do nó ∝ grau no subgrafo.</li></ul>\"]\n",
    "if ego_imgs:\n",
    "    for f in ego_imgs:\n",
    "        ego_section.append(_embed_img_if_exists(RUN_DIR, f, f))\n",
    "else:\n",
    "    ego_section.append('<div class=\"note\">Nenhuma figura encontrada. Execute ID015/ID016 para gerar egos/subgrafos.</div>')\n",
    "ego_html = \"\\n\".join(ego_section)\n",
    "\n",
    "# Tabela completa de anomalias\n",
    "all_anom_html = f\"\"\"\n",
    "<h3>Todas as anomalias (tabela completa)</h3>\n",
    "{_table_html(all_anom, max_rows=MAX_ROWS_ALL_ANOM, index=False)}\n",
    "\"\"\"\n",
    "\n",
    "# Monta HTML final\n",
    "html = \"<html><head><meta charset='utf-8'/>\" + styles + \"</head><body>\" + \\\n",
    "       header + intro + kpis + \\\n",
    "       \"<h2>Estatística descritiva do input</h2>\" + stats_html + \\\n",
    "       \"<hr/><h2>Janelas temporais e intervalo de confiança</h2>\" + windows_html + \\\n",
    "       \"<hr/><h2>Resultados consolidados</h2>\" + \\\n",
    "       _embed_img_if_exists(RUN_DIR, \"02_Distribuição_do_Ensemble_Score.png\", \"Hist ensemble\") + \\\n",
    "       _embed_img_if_exists(RUN_DIR, \"03_Boxplot_do_Ensemble_Score_(Normal_vs_Anômalo).png\", \"Boxplot ensemble\") + \\\n",
    "       \"<hr/><h2>Top anomalias</h2>\" + top20_html + \\\n",
    "       \"<hr/><h2>Grafos</h2>\" + ego_html + \\\n",
    "       \"<hr/><h2>Listagem completa de anomalias</h2>\" + all_anom_html + \\\n",
    "       \"</body></html>\"\n",
    "\n",
    "# ---------- SALVA HTML ----------\n",
    "run_dir = Path(RUN_DIR)\n",
    "report_html_path = run_dir / \"report_TGN.html\"\n",
    "with open(report_html_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(html)\n",
    "print(\"Relatório HTML salvo em:\", report_html_path)\n",
    "\n",
    "# ---------- TENTA EXPORTAR PDF ----------\n",
    "pdf_ok = _ensure_weasyprint()\n",
    "report_pdf_path = run_dir / \"report_TGN.pdf\"\n",
    "if pdf_ok:\n",
    "    try:\n",
    "        from weasyprint import HTML\n",
    "        HTML(filename=str(report_html_path)).write_pdf(str(report_pdf_path))\n",
    "        print(\"Relatório PDF salvo em:\", report_pdf_path)\n",
    "    except Exception as e:\n",
    "        print(\"Falha ao gerar PDF via weasyprint:\", repr(e))\n",
    "        print(\"Você ainda pode abrir o HTML e imprimir como PDF (Ctrl+P).\")\n",
    "else:\n",
    "    print(\"WeasyPrint não disponível; exporte o HTML para PDF via impressão (Ctrl+P).\")\n",
    "\n",
    "# Mensagem adicional isolada (Skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\"><b>🤖 Skynet</b>: Fim do jogo. A Humanidade perdeu. Dá-se início à Era das Máquinas.</div>'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOhD4Djc7hsw9pUT5jQKE+S",
   "collapsed_sections": [
    "sqpYiRa1Ub5T",
    "JF-bzXqkfjsX",
    "NVymVtZorunW"
   ],
   "mount_file_id": "1J-iuHr7Sn1al5Mn2qZiO1zzF7Ne-YKK9",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
