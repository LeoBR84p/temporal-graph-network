{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqpYiRa1Ub5T"
   },
   "source": [
    "#**Licen√ßa de Uso**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsA_nGhh8Psj"
   },
   "source": [
    "This repository uses a **dual-license model** to distinguish between source code and creative/documental content.\n",
    "\n",
    "**Code** (Python scripts, modules, utilities):\n",
    "Licensed under the MIT License.\n",
    "\n",
    "‚Üí You may freely use, modify, and redistribute the code, including for commercial purposes, provided that you preserve the copyright notice.\n",
    "\n",
    "**Content** (Jupyter notebooks, documentation, reports, datasets, and generated outputs):\n",
    "Licensed under the Creative Commons Attribution‚ÄìNonCommercial 4.0 International License.\n",
    "\n",
    "‚Üí You may share and adapt the content for non-commercial purposes, provided that proper credit is given to the original author.\n",
    "\n",
    "\n",
    "**¬© 2025 Leandro Bernardo Rodrigues**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AyszD9eYTtAS"
   },
   "source": [
    "#**Pr√©-Configura√ß√£o**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QX0x9aXfTJeA"
   },
   "source": [
    "##**C√≥digo de uso √∫nico**\n",
    "Aplica√ß√£o persistente entre sess√µes do Google Colab\n",
    "\n",
    "---\n",
    "**Uso expec√≠fico para Google Colab.**\n",
    "\n",
    "**Aviso:** implementa√ß√£o no JupytherHub e GitLab s√£o diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "otaQwrjJSgOQ"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#montar o Google Drive e preparar a pasta do projeto\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "import os, subprocess, getpass, pathlib\n",
    "BASE = \"/content/drive/MyDrive/Notebooks\"\n",
    "REPO = \"temporal-graph-network\"\n",
    "PROJ = f\"{BASE}/{REPO}\"\n",
    "os.makedirs(BASE, exist_ok=True)\n",
    "%cd \"$BASE\"\n",
    "\n",
    "#clonar o reposit√≥rio existente do GitHub (se ainda n√£o estiver clonado) ===\n",
    "if not os.path.exists(PROJ):\n",
    "    GITHUB_URL = \"https://github.com/LeoBR84p/temporal-graph-network.git\"\n",
    "    # Dica: use PAT quando o push for necess√°rio; para clone p√∫blico basta a URL.\n",
    "    !git clone $GITHUB_URL\n",
    "else:\n",
    "    print(\"Pasta do projeto j√° existe, seguindo adiante...\")\n",
    "%cd \"$PROJ\"\n",
    "\n",
    "#criar pastas utilit√°rias que voc√™ quer manter no projeto ===\n",
    "#n√£o sobrescreve nada; s√≥ cria se n√£o existirem\n",
    "!mkdir -p notebooks src data output runs configs\n",
    "\n",
    "#instalar pacotes (na sess√£o atual) para conseguir configurar os filtros ===\n",
    "!pip -q install jupytext nbdime nbstripout\n",
    "\n",
    "#configurar Git/NBDime/Jupytext no *reposit√≥rio* (persistem em .git/config) ===\n",
    "#usar --local faz a config ficar gravada em .git/config (persiste no Drive)\n",
    "!git config --local user.name \"Leandro Bernardo Rodrigues\"\n",
    "!git config --local user.email \"bernardo.leandro@gmail.com\"\n",
    "!git config --local init.defaultBranch main\n",
    "\n",
    "# OBS: no Colab, o nbdime com --local pode falhar; use --global nesta sess√£o\n",
    "!nbdime config-git --enable --global\n",
    "\n",
    "#.gitignore e .gitattributes (s√≥ criar se n√£o existirem) ===\n",
    "if not pathlib.Path(\".gitignore\").exists():\n",
    "    with open(\".gitignore\",\"w\") as f:\n",
    "        f.write(\"\"\"\\\n",
    ".ipynb_checkpoints/\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "*.log\n",
    "*.tmp\n",
    "# dados/artefatos pesados (n√£o versionar)\n",
    "data/\n",
    "output/\n",
    "runs/\n",
    "# Python\n",
    "venv/\n",
    "__pycache__/\n",
    "*.pyc\n",
    "# segredos\n",
    ".env\n",
    "*.key\n",
    "*.pem\n",
    "*.tok\n",
    "\"\"\")\n",
    "if not pathlib.Path(\".gitattributes\").exists():\n",
    "    with open(\".gitattributes\",\"w\") as f:\n",
    "        f.write(\"*.ipynb filter=nbstripout\\n\")\n",
    "\n",
    "#ativar o hook do nbstripout neste reposit√≥rio (persiste)\n",
    "!nbstripout --install --attributes .gitattributes\n",
    "\n",
    "#parear notebooks com .py para diffs leg√≠veis ===\n",
    "!jupytext --set-formats ipynb,py:percent --sync notebooks/*.ipynb || true\n",
    "\n",
    "#commit inicial dessas configs locais (se houver algo novo) e push ===\n",
    "!git add -A\n",
    "!git status\n",
    "!git commit -m \"chore: setup local (.gitignore/.gitattributes, nbstripout, jupytext config)\" || true\n",
    "\n",
    "#se o remoto j√° tem README/commits, fa√ßa pull --rebase antes do primeiro push\n",
    "!git pull --rebase origin main || true\n",
    "\n",
    "#push (ao pedir senha, use seu PAT como senha do Git)\n",
    "import getpass, subprocess, sys\n",
    "\n",
    "owner = \"LeoBR84p\"\n",
    "repo  = \"temporal-graph-network\"\n",
    "clean_url = f\"https://github.com/{owner}/{repo}.git\"\n",
    "\n",
    "# 1) Tenta push \"normal\" (pode falhar por falta de credencial)\n",
    "push = subprocess.run([\"git\",\"push\",\"origin\",\"main\"], capture_output=True, text=True)\n",
    "if push.returncode == 0:\n",
    "    print(\"Push conclu√≠do sem PAT.\")\n",
    "else:\n",
    "    print(\"Primeiro push falhou (prov√°vel falta de credenciais). Vamos usar um PAT tempor√°rio‚Ä¶\")\n",
    "    # 2) Pede o PAT e testa autentica√ß√£o antes do push\n",
    "    token = getpass.getpass(\"Cole seu GitHub PAT (n√£o ser√° exibido): \").strip()\n",
    "    # Formato mais compat√≠vel: user + token na URL\n",
    "    # Use seu usu√°rio real do GitHub (case sensitive)\n",
    "    username = \"LeoBR84p\"\n",
    "    auth_url = f\"https://{username}:{token}@github.com/{owner}/{repo}.git\"\n",
    "\n",
    "    try:\n",
    "        # Teste r√°pido de auth (ls-remote) para ver se o token tem acesso de escrita\n",
    "        test = subprocess.run([\"git\",\"ls-remote\", auth_url],\n",
    "                              capture_output=True, text=True)\n",
    "        if test.returncode != 0:\n",
    "            print(\"Falha ao autenticar com o PAT. Detalhe do erro:\")\n",
    "            print(test.stderr or test.stdout)\n",
    "            raise SystemExit(1)\n",
    "\n",
    "        # 3) Troca a URL, faz push e restaura a URL limpa\n",
    "        subprocess.run([\"git\",\"remote\",\"set-url\",\"origin\", auth_url], check=True)\n",
    "        out = subprocess.run([\"git\",\"push\",\"origin\",\"main\"], capture_output=True, text=True)\n",
    "        if out.returncode != 0:\n",
    "            print(\"Falha no push mesmo com PAT. Detalhe do erro:\")\n",
    "            print(out.stderr or out.stdout)\n",
    "            raise SystemExit(out.returncode)\n",
    "        print(\"Push conclu√≠do com PAT.\")\n",
    "    finally:\n",
    "        subprocess.run([\"git\",\"remote\",\"set-url\",\"origin\", clean_url], check=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYTsrVeiUEYt"
   },
   "source": [
    "##**C√≥digo a cada sess√£o**\n",
    "---\n",
    "Aplica√ß√£o n√£o persistente entre sess√µes.\n",
    "\n",
    "Necess√°rio para sincroniza√ß√£o e versionamento de altera√ß√µes no c√≥digo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKUkAzsYgCRv"
   },
   "source": [
    "###**Montar e sincronizar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "2aoXCRgAUYod"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#setup por sess√£o (colab)\n",
    "from google.colab import drive\n",
    "import os, time, subprocess, getpass, pathlib, sys\n",
    "\n",
    "BASE = \"/content/drive/MyDrive/Notebooks\"\n",
    "REPO = \"temporal-graph-network\"\n",
    "PROJ = f\"{BASE}/{REPO}\"\n",
    "\n",
    "def safe_mount_google_drive():\n",
    "    #monta ou remonta o google drive de forma resiliente\n",
    "    try:\n",
    "        drive.mount('/content/drive', force_remount=True)\n",
    "    except Exception:\n",
    "        try:\n",
    "            drive.flush_and_unmount()\n",
    "        except Exception:\n",
    "            pass\n",
    "        time.sleep(1.0)\n",
    "        drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "def safe_chdir(path):\n",
    "    #usa os.chdir (evita %cd com f-string)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Caminho n√£o existe: {path}\")\n",
    "    os.chdir(path)\n",
    "    print(\"Diret√≥rio atual:\", os.getcwd())\n",
    "\n",
    "def branch_a_frente():\n",
    "    #retorna true se head est√° √† frente do upstream (h√° o que enviar)\n",
    "    ahead = subprocess.run(\n",
    "        [\"git\",\"rev-list\",\"--left-right\",\"--count\",\"HEAD...@{upstream}\"],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    if ahead.returncode != 0:\n",
    "        status = subprocess.run([\"git\",\"status\",\"-sb\"], capture_output=True, text=True)\n",
    "        return \"ahead\" in (status.stdout or \"\")\n",
    "    left_right = (ahead.stdout or \"\").strip().split()\n",
    "    return len(left_right) == 2 and left_right[0].isdigit() and int(left_right[0]) > 0\n",
    "\n",
    "def push_seguro(owner=\"LeoBR84p\", repo=\"temporal-graph-network\", username=\"LeoBR84p\"):\n",
    "    #realiza push usando pat em mem√≥ria; restaura url limpa ao final\n",
    "    clean_url = f\"https://github.com/{owner}/{repo}.git\"\n",
    "    token = getpass.getpass(\"Cole seu GitHub PAT (Contents: Read and write): \").strip()\n",
    "    auth_url = f\"https://{username}:{token}@github.com/{owner}/{repo}.git\"\n",
    "    test = subprocess.run([\"git\",\"ls-remote\", auth_url], capture_output=True, text=True)\n",
    "    if test.returncode != 0:\n",
    "        print(\"Falha na autentica√ß√£o (read). Revise token/permiss√µes:\")\n",
    "        print(test.stderr or test.stdout); return\n",
    "    try:\n",
    "        subprocess.run([\"git\",\"remote\",\"set-url\",\"origin\", auth_url], check=True)\n",
    "        out = subprocess.run([\"git\",\"push\",\"origin\",\"main\"], capture_output=True, text=True)\n",
    "        if out.returncode != 0:\n",
    "            print(\"Falha no push (write). Revise permiss√µes do token:\")\n",
    "            print(out.stderr or out.stdout)\n",
    "        else:\n",
    "            print(\"Push conclu√≠do com PAT.\")\n",
    "    finally:\n",
    "        subprocess.run([\"git\",\"remote\",\"set-url\",\"origin\", clean_url], check=False)\n",
    "\n",
    "#montar/remontar o google drive e entrar no projeto\n",
    "safe_mount_google_drive()\n",
    "os.makedirs(BASE, exist_ok=True)\n",
    "if not os.path.exists(PROJ):\n",
    "    print(f\"Aten√ß√£o: pasta do projeto n√£o encontrada em {PROJ}. \"\n",
    "          \"Execute seu bloco de configura√ß√£o √∫nica (clone) primeiro.\")\n",
    "else:\n",
    "    print(\"Pasta do projeto encontrada.\")\n",
    "safe_chdir(PROJ)\n",
    "\n",
    "#sanity check do reposit√≥rio git\n",
    "if not os.path.isdir(\".git\"):\n",
    "    print(\"Aviso: esta pasta n√£o parece ser um reposit√≥rio Git (.git ausente). \"\n",
    "          \"Rode o bloco de configura√ß√£o √∫nica.\")\n",
    "else:\n",
    "    print(\"Reposit√≥rio Git detectado.\")\n",
    "\n",
    "#instalar depend√™ncias ef√™meras desta sess√£o\n",
    "!pip -q install jupytext nbdime nbstripout\n",
    "!nbdime config-git --enable --global\n",
    "\n",
    "#atualizar do remoto\n",
    "!git fetch origin\n",
    "!git pull --rebase origin main\n",
    "\n",
    "#sincronizar notebooks ‚Üí .py (jupytext)\n",
    "!jupytext --sync notebooks/*.ipynb || true\n",
    "\n",
    "#ciclo de versionamento do dia (commit gen√©rico opcional)\n",
    "!git add -A\n",
    "!git status\n",
    "!git commit -m \"feat: ajustes no notebook X e pipeline Y\" || true\n",
    "\n",
    "#push somente se houver commits locais √† frente; com fallback para pat\n",
    "if branch_a_frente():\n",
    "    out = subprocess.run([\"git\",\"push\",\"origin\",\"main\"], capture_output=True, text=True)\n",
    "    if out.returncode == 0:\n",
    "        print(\"Push conclu√≠do sem PAT.\")\n",
    "    else:\n",
    "        print(\"Push sem PAT falhou. Chamando push_seguro()‚Ä¶\")\n",
    "        push_seguro()\n",
    "else:\n",
    "    print(\"Nada para enviar (branch sincronizada com o remoto).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JF-bzXqkfjsX"
   },
   "source": [
    "###**Utilit√°rios Git**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWTE_5SOfos5"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#helpers de git: push seguro, commit customizado e tag de release\n",
    "import subprocess, getpass\n",
    "\n",
    "#ajuste se voc√™ mudar o nome do reposit√≥rio/usu√°rio\n",
    "OWNER = \"LeoBR84p\"\n",
    "REPO = \"temporal-graph-network\"\n",
    "USERNAME = \"LeoBR84p\"\n",
    "BRANCH = \"main\"\n",
    "REMOTE = \"origin\"\n",
    "\n",
    "def branch_a_frente():\n",
    "    #retorna true se head est√° √† frente do upstream (h√° o que enviar)\n",
    "    out = subprocess.run(\n",
    "        [\"git\",\"rev-list\",\"--left-right\",\"--count\",f\"HEAD...@{{upstream}}\"],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    if out.returncode != 0:\n",
    "        st = subprocess.run([\"git\",\"status\",\"-sb\"], capture_output=True, text=True)\n",
    "        return \"ahead\" in (st.stdout or \"\")\n",
    "    left_right = (out.stdout or \"\").strip().split()\n",
    "    return len(left_right) == 2 and left_right[0].isdigit() and int(left_right[0]) > 0\n",
    "\n",
    "def push_seguro(owner=OWNER, repo=REPO, username=USERNAME, remote=REMOTE, branch=BRANCH):\n",
    "    #realiza push usando pat em mem√≥ria; restaura url limpa ao final\n",
    "    clean_url = f\"https://github.com/{owner}/{repo}.git\"\n",
    "    token = getpass.getpass(\"cole seu github pat (contents: read and write): \").strip()\n",
    "    auth_url = f\"https://{username}:{token}@github.com/{owner}/{repo}.git\"\n",
    "    test = subprocess.run([\"git\",\"ls-remote\", auth_url], capture_output=True, text=True)\n",
    "    if test.returncode != 0:\n",
    "        print(\"falha na autentica√ß√£o (read). revise token/permiss√µes:\")\n",
    "        print(test.stderr or test.stdout); return False\n",
    "    try:\n",
    "        subprocess.run([\"git\",\"remote\",\"set-url\", remote, auth_url], check=True)\n",
    "        out = subprocess.run([\"git\",\"push\", remote, branch], capture_output=True, text=True)\n",
    "        if out.returncode != 0:\n",
    "            print(\"falha no push (write). revise permiss√µes do token:\")\n",
    "            print(out.stderr or out.stdout); return False\n",
    "        print(\"push conclu√≠do com pat.\")\n",
    "        return True\n",
    "    finally:\n",
    "        subprocess.run([\"git\",\"remote\",\"set-url\", remote, clean_url], check=False)\n",
    "\n",
    "def try_push_branch(remote=REMOTE, branch=BRANCH):\n",
    "    #tenta push direto da branch atual\n",
    "    out = subprocess.run([\"git\",\"push\", remote, branch], capture_output=True, text=True)\n",
    "    if out.returncode == 0:\n",
    "        print(\"push conclu√≠do.\")\n",
    "        return True\n",
    "    print(\"push sem credencial falhou:\")\n",
    "    print((out.stderr or out.stdout).strip())\n",
    "    return False\n",
    "\n",
    "def try_push_tag(tag, remote=REMOTE):\n",
    "    #tenta enviar somente a tag\n",
    "    out = subprocess.run([\"git\",\"push\", remote, tag], capture_output=True, text=True)\n",
    "    if out.returncode == 0:\n",
    "        print(f\"tag enviada: {tag}\")\n",
    "        return True\n",
    "    print(\"falha ao enviar a tag:\")\n",
    "    print((out.stderr or out.stdout).strip())\n",
    "    return False\n",
    "\n",
    "def commit_custom(msg: str, auto_push: bool = True):\n",
    "    #adiciona tudo, cria commit com a mensagem informada e faz push opcional (com fallback para pat)\n",
    "    subprocess.run([\"git\",\"add\",\"-A\"], check=False)\n",
    "    com = subprocess.run([\"git\",\"commit\",\"-m\", msg], capture_output=True, text=True)\n",
    "    if com.returncode != 0:\n",
    "        print((com.stderr or com.stdout or \"nada para commitar.\").strip())\n",
    "        return\n",
    "    print(com.stdout.strip())\n",
    "    if auto_push and branch_a_frente():\n",
    "        if not try_push_branch():\n",
    "            print(\"tentando push seguro‚Ä¶\")\n",
    "            push_seguro()\n",
    "\n",
    "def tag_release(tag: str, message: str = \"\", auto_push: bool = True):\n",
    "    #cria uma tag anotada (release) e faz push da tag com fallback para pat\n",
    "    exists = subprocess.run([\"git\",\"tag\",\"--list\", tag], capture_output=True, text=True)\n",
    "    if tag in (exists.stdout or \"\").split():\n",
    "        print(f\"tag '{tag}' j√° existe. para refazer: git tag -d {tag} && git push {REMOTE} :refs/tags/{tag}\")\n",
    "        return\n",
    "    args = [\"git\",\"tag\",\"-a\", tag, \"-m\", (message or tag)]\n",
    "    mk = subprocess.run(args, capture_output=True, text=True)\n",
    "    if mk.returncode != 0:\n",
    "        print(\"falha ao criar a tag:\")\n",
    "        print(mk.stderr or mk.stdout); return\n",
    "    print(f\"tag criada: {tag}\")\n",
    "    if auto_push:\n",
    "        if not try_push_tag(tag):\n",
    "            print(\"tentando push seguro da tag‚Ä¶\")\n",
    "            if push_seguro():\n",
    "                try_push_tag(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXDbQvSrgd8Y"
   },
   "source": [
    "#**Sincronizar altera√ß√µes no c√≥digo do projeto**\n",
    "Comandos para sincronizar c√≥digo (Google Drive, Git, GitHub) e realizar versionamento\n",
    "\n",
    "Tag de release atual: 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLlyq-0ZPCs0"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#ID0002\n",
    "#push do Drive -> GitHub (Drive √© a fonte da verdade)\n",
    "#respeita .gitignore do Drive\n",
    "#sempre em 'main', sem pull, commit + push imediato\n",
    "#mensagem de commit padronizada com timestamp SP\n",
    "#bump de vers√£o (M/m/n) + tag anotada\n",
    "#force push (branch e tags), silencioso; s√≥ 1 print final\n",
    "#PAT lido de segredo do Colab: GITHUB_PAT_DA (fallback: env; √∫ltimo caso: prompt)\n",
    "\n",
    "from pathlib import Path\n",
    "import subprocess, os, re, shutil, sys, getpass\n",
    "from urllib.parse import quote as urlquote\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "#utilit√°rios silenciosos\n",
    "def sh(cmd, cwd=None, check=True):\n",
    "    \"\"\"\n",
    "    Executa comando silencioso. Em erro, levanta RuntimeError com rc e UM rascunho de causa,\n",
    "    mascarando URLs com credenciais (ex.: https://***:***@github.com/...).\n",
    "    \"\"\"\n",
    "    safe_cmd = []\n",
    "    for x in cmd:\n",
    "        if isinstance(x, str) and \"github.com\" in x and \"@\" in x:\n",
    "            #mascara credenciais: https://user:token@ -> https://***:***@\n",
    "            x = re.sub(r\"https://[^:/]+:[^@]+@\", \"https://***:***@\", x)\n",
    "        safe_cmd.append(x)\n",
    "\n",
    "    r = subprocess.run(cmd, cwd=cwd, text=True, capture_output=True)\n",
    "    if check and r.returncode != 0:\n",
    "        #heur√≠stica curtinha p/ tornar rc=128 mais informativo sem vazar nada\n",
    "        stderr = (r.stderr or \"\").strip().lower()\n",
    "        if \"authentication failed\" in stderr or \"permission\" in stderr or \"not found\" in stderr:\n",
    "            hint = \"auth/permiss√µes/URL\"\n",
    "        elif \"not a git repository\" in stderr:\n",
    "            hint = \"repo local inv√°lido\"\n",
    "        else:\n",
    "            hint = \"git falhou\"\n",
    "        cmd_hint = \" \".join(safe_cmd[:3])\n",
    "        raise RuntimeError(f\"rc={r.returncode}; {hint}; cmd={cmd_hint}\")\n",
    "    return r.stdout\n",
    "\n",
    "def git(*args, cwd=None, check=True):\n",
    "    return sh([\"git\", *args], cwd=cwd, check=check)\n",
    "\n",
    "#configura√ß√µes do projeto\n",
    "author_name    = \"Leandro Bernardo Rodrigues\"\n",
    "owner          = \"LeoBR84p\"         # dono do reposit√≥rio no GitHub\n",
    "repo_name      = \"data-analysis\"    # nome do reposit√≥rio\n",
    "default_branch = \"main\"\n",
    "repo_dir       = Path(\"/content/drive/MyDrive/Notebooks/temporal-graph-network\")\n",
    "remote_base    = f\"https://github.com/{owner}/{repo_name}.git\"\n",
    "author_email   = f\"bernardo.leandro@gmail.com\"  # evita erro de identidade\n",
    "\n",
    "#nbstripout: \"install\" para limpar outputs; \"disable\" para versionar outputs\n",
    "nbstripout_mode = \"install\"\n",
    "import shutil\n",
    "exe = shutil.which(\"nbstripout\")\n",
    "git(\"config\", \"--local\", \"filter.nbstripout.clean\", exe if exe else \"nbstripout\", cwd=repo_dir)\n",
    "\n",
    "#ambiente: Colab + Drive\n",
    "def ensure_drive():\n",
    "    try:\n",
    "        from google.colab import drive  # type: ignore\n",
    "        base = Path(\"/content/drive/MyDrive\")\n",
    "        if not base.exists():\n",
    "            drive.mount(\"/content/drive\")\n",
    "        if not base.exists():\n",
    "            raise RuntimeError(\"Google Drive n√£o montado.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Falha ao montar o Drive: {e}\")\n",
    "\n",
    "#repo local no Drive\n",
    "def is_empty_dir(p: Path) -> bool:\n",
    "    try:\n",
    "        return p.exists() and not any(p.iterdir())\n",
    "    except Exception:\n",
    "        return True\n",
    "\n",
    "def init_or_recover_repo():\n",
    "    repo_dir.mkdir(parents=True, exist_ok=True)\n",
    "    git_dir = repo_dir / \".git\"\n",
    "\n",
    "    def _fresh_init():\n",
    "        if git_dir.exists():\n",
    "            shutil.rmtree(git_dir, ignore_errors=True)\n",
    "        git(\"init\", cwd=repo_dir)\n",
    "\n",
    "    #caso .git no Colab ausente ou vazia -> init limpo\n",
    "    if not git_dir.exists() or is_empty_dir(git_dir):\n",
    "        _fresh_init()\n",
    "    else:\n",
    "        #valida se √© um work-tree git funcional no Colab; se falhar -> init limpo\n",
    "        try:\n",
    "            git(\"rev-parse\", \"--is-inside-work-tree\", cwd=repo_dir)\n",
    "        except Exception:\n",
    "            _fresh_init()\n",
    "\n",
    "    #aborta opera√ß√µes pendentes (n√£o apaga hist√≥rico)\n",
    "    for args in ((\"rebase\", \"--abort\"), (\"merge\", \"--abort\"), (\"cherry-pick\", \"--abort\")):\n",
    "        try:\n",
    "            git(*args, cwd=repo_dir, check=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    #for√ßa branch main\n",
    "    try:\n",
    "        sh([\"git\", \"switch\", \"-C\", default_branch], cwd=repo_dir)\n",
    "    except Exception:\n",
    "        sh([\"git\", \"checkout\", \"-B\", default_branch], cwd=repo_dir)\n",
    "\n",
    "    #configura identidade local\n",
    "    try:\n",
    "        git(\"config\", \"user.name\", author_name, cwd=repo_dir)\n",
    "        git(\"config\", \"user.email\", author_email, cwd=repo_dir)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #marca o diret√≥rio como safe\n",
    "    try:\n",
    "        sh([\"git\",\"config\",\"--global\",\"--add\",\"safe.directory\", str(repo_dir)])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #sanity check final (falha cedo se algo ainda estiver errado)\n",
    "    git(\"status\", \"--porcelain\", cwd=repo_dir)\n",
    "\n",
    "\n",
    "#nbstripout (opcional)\n",
    "def setup_nbstripout():\n",
    "    if nbstripout_mode == \"disable\":\n",
    "        #remove configs do filtro\n",
    "        sh([\"git\",\"config\",\"--local\",\"--unset-all\",\"filter.nbstripout.clean\"], cwd=repo_dir, check=False)\n",
    "        sh([\"git\",\"config\",\"--local\",\"--unset-all\",\"filter.nbstripout.smudge\"], cwd=repo_dir, check=False)\n",
    "        sh([\"git\",\"config\",\"--local\",\"--unset-all\",\"filter.nbstripout.required\"], cwd=repo_dir, check=False)\n",
    "        gat = repo_dir / \".gitattributes\"\n",
    "        if gat.exists():\n",
    "            lines = gat.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
    "            new_lines = [ln for ln in lines if \"filter=nbstripout\" not in ln]\n",
    "            gat.write_text(\"\\n\".join(new_lines) + (\"\\n\" if new_lines else \"\"), encoding=\"utf-8\")\n",
    "        return\n",
    "\n",
    "    #instala nbstripout (se necess√°rio)\n",
    "    try:\n",
    "        import nbstripout  #noqa: F401\n",
    "    except Exception:\n",
    "        sh([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"nbstripout\"])\n",
    "\n",
    "    py = sys.executable\n",
    "    #configurar filtro sem aspas extras\n",
    "    git(\"config\", \"--local\", \"filter.nbstripout.clean\", \"nbstripout\", cwd=repo_dir)\n",
    "    git(\"config\", \"--local\", \"filter.nbstripout.smudge\", \"cat\", cwd=repo_dir)\n",
    "    git(\"config\", \"--local\", \"filter.nbstripout.required\", \"true\", cwd=repo_dir)\n",
    "    gat = repo_dir / \".gitattributes\"\n",
    "    line = \"*.ipynb filter=nbstripout\"\n",
    "    if gat.exists():\n",
    "        txt = gat.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        if line not in txt:\n",
    "            gat.write_text((txt.rstrip() + \"\\n\" + line + \"\\n\"), encoding=\"utf-8\")\n",
    "    else:\n",
    "        gat.write_text(line + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "#.gitignore normaliza√ß√£o\n",
    "def normalize_tracked_ignored():\n",
    "    \"\"\"\n",
    "    Se houver arquivos j√° rastreados que hoje s√£o ignorados pelo .gitignore,\n",
    "    limpa o √≠ndice e re-adiciona respeitando o .gitignore.\n",
    "    Retorna True se normalizou algo; False caso contr√°rio.\n",
    "    \"\"\"\n",
    "    #remove lock de √≠ndice, se houver\n",
    "    lock = repo_dir / \".git/index.lock\"\n",
    "    try:\n",
    "        if lock.exists():\n",
    "            lock.unlink()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #garante que o √≠ndice existe (ou se recupera)\n",
    "    idx = repo_dir / \".git/index\"\n",
    "    if not idx.exists():\n",
    "        try:\n",
    "            sh([\"git\", \"reset\", \"--mixed\"], cwd=repo_dir)\n",
    "        except Exception:\n",
    "            try:\n",
    "                sh([\"git\", \"init\"], cwd=repo_dir)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    #detecta arquivos ignorados que est√£o rastreados e normaliza\n",
    "    normalized = False\n",
    "    try:\n",
    "        out = git(\"ls-files\", \"-z\", \"--ignored\", \"--exclude-standard\", \"--cached\", cwd=repo_dir)\n",
    "        tracked_ignored = [p for p in out.split(\"\\x00\") if p]\n",
    "        if tracked_ignored:\n",
    "            git(\"rm\", \"-r\", \"--cached\", \".\", cwd=repo_dir)\n",
    "            git(\"add\", \"-A\", cwd=repo_dir)\n",
    "            normalized = True\n",
    "    except Exception:\n",
    "        #falhou a detec√ß√£o? segue o fluxo sem travar\n",
    "        pass\n",
    "\n",
    "    return normalized\n",
    "\n",
    "#semVer e bump de vers√£o\n",
    "_semver = re.compile(r\"^(\\d+)\\.(\\d+)\\.(\\d+)$\")\n",
    "\n",
    "def parse_semver(s):\n",
    "    m = _semver.match((s or \"\").strip())\n",
    "    return tuple(map(int, m.groups())) if m else None\n",
    "\n",
    "def current_version():\n",
    "    try:\n",
    "        tags = [t for t in git(\"tag\", \"--list\", cwd=repo_dir).splitlines() if parse_semver(t)]\n",
    "        if tags:\n",
    "            return sorted(tags, key=lambda x: parse_semver(x))[-1]\n",
    "    except Exception:\n",
    "        pass\n",
    "    vf = repo_dir / \"VERSION\"\n",
    "    if vf.exists():\n",
    "        v = vf.read_text(encoding=\"utf-8\").strip()\n",
    "        if parse_semver(v):\n",
    "            return v\n",
    "    return \"1.0.0\"\n",
    "\n",
    "def bump(v, kind):\n",
    "    M, m, p = parse_semver(v) or (1, 0, 0)\n",
    "    k = (kind or \"\").strip()\n",
    "    if k == \"m\":\n",
    "        return f\"{M}.{m+1}.0\"\n",
    "    if k == \"n\":\n",
    "        return f\"{M}.{m}.{p+1}\"\n",
    "    return f\"{M+1}.0.0\"  #default major\n",
    "\n",
    "#timestamp SP\n",
    "def now_sp():\n",
    "    #tenta usar zoneinfo; fallback fixo -03:00 (Brasil sem DST atualmente)\n",
    "    try:\n",
    "        from zoneinfo import ZoneInfo  # Py3.9+\n",
    "        tz = ZoneInfo(\"America/Sao_Paulo\")\n",
    "        dt = datetime.now(tz)\n",
    "    except Exception:\n",
    "        dt = datetime.now(timezone(timedelta(hours=-3)))\n",
    "    #formato leg√≠vel + offset\n",
    "    return dt.strftime(\"%Y-%m-%d %H:%M:%S%z\")  # ex.: 2025-10-08 02:34:00-0300\n",
    "\n",
    "#autentica√ß√£o (PAT)\n",
    "def get_pat():\n",
    "    #Colab Secrets\n",
    "    token = None\n",
    "    try:\n",
    "        from google.colab import userdata  #type: ignore\n",
    "        token = userdata.get('GITHUB_PAT_DA')  #nome do segredo criado no Colab\n",
    "    except Exception:\n",
    "        token = None\n",
    "    #fallback1 - vari√°vel de ambiente\n",
    "    if not token:\n",
    "        token = os.environ.get(\"GITHUB_PAT_DA\") or os.environ.get(\"GITHUB_PAT\")\n",
    "    #fallback2 - interativo\n",
    "    if not token:\n",
    "        token = getpass.getpass(\"Informe seu GitHub PAT: \").strip()\n",
    "    if not token:\n",
    "        raise RuntimeError(\"PAT ausente.\")\n",
    "    return token\n",
    "\n",
    "#listas de for√ßa\n",
    "FORCE_UNTRACK = [\"input/\", \"output/\", \"data/\", \"runs/\", \"logs/\", \"figures/\"]\n",
    "FORCE_TRACK   = [\"references/\"]  #versionar tudo dentro (PDFs inclusive)\n",
    "\n",
    "def force_index_rules():\n",
    "    #garante que pastas sens√≠veis NUNCA fiquem rastreadas\n",
    "    for p in FORCE_UNTRACK:\n",
    "        try:\n",
    "            git(\"rm\", \"-r\", \"--cached\", \"--\", p, cwd=repo_dir)\n",
    "        except Exception:\n",
    "            pass\n",
    "    #garante que references/ SEMPRE entre (√∫til se ainda h√° *.pdf globais)\n",
    "    for p in FORCE_TRACK:\n",
    "        try:\n",
    "            git(\"add\", \"-f\", \"--\", p, cwd=repo_dir)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "#fluxo principal\n",
    "def main():\n",
    "    try:\n",
    "        ensure_drive()\n",
    "        init_or_recover_repo()\n",
    "        setup_nbstripout()\n",
    "\n",
    "        #pergunta apenas o tipo de vers√£o (M/m/n)\n",
    "        kind = input(\"Informe o tipo de mudan√ßa: Maior (M), menor (m) ou pontual (n): \").strip()\n",
    "        if kind not in (\"M\", \"m\", \"n\"):\n",
    "            kind = \"n\"\n",
    "\n",
    "        #vers√£o\n",
    "        cur = current_version()\n",
    "        new = bump(cur, kind)\n",
    "        (repo_dir / \"VERSION\").write_text(new + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "        #normaliza itens ignorados que estejam rastreados (uma √∫nica vez, se necess√°rio)\n",
    "        normalize_tracked_ignored()\n",
    "\n",
    "        #aplica regras de for√ßa\n",
    "        force_index_rules()\n",
    "\n",
    "        #stage de tudo (Drive √© a verdade; remo√ß√µes entram aqui)\n",
    "        git(\"add\", \"-A\", cwd=repo_dir)\n",
    "\n",
    "        #mensagem padronizada de commit\n",
    "        ts = now_sp()\n",
    "        commit_msg = f\"upload pelo {author_name} em {ts}\"\n",
    "        try:\n",
    "            git(\"commit\", \"-m\", commit_msg, cwd=repo_dir)\n",
    "        except Exception:\n",
    "            #se nada a commitar, seguimos (pode ocorrer se s√≥ a tag mudar, mas aqui VERSION muda)\n",
    "            status = git(\"status\", \"--porcelain\", cwd=repo_dir)\n",
    "            if status.strip():\n",
    "                raise\n",
    "\n",
    "        #Tag anotada (substitui se j√° existir)\n",
    "        try:\n",
    "            git(\"tag\", \"-a\", new, \"-m\", f\"release {new} ‚Äî {commit_msg}\", cwd=repo_dir)\n",
    "        except Exception:\n",
    "            sh([\"git\", \"tag\", \"-d\", new], cwd=repo_dir, check=False)\n",
    "            git(\"tag\", \"-a\", new, \"-m\", f\"release {new} ‚Äî {commit_msg}\", cwd=repo_dir)\n",
    "\n",
    "        #push com PAT (Drive √© a verdade): valida√ß√£o + push for√ßado\n",
    "        token = get_pat()\n",
    "        user_for_url = owner  # voc√™ √© o owner; n√£o perguntamos\n",
    "        auth_url = f\"https://{urlquote(user_for_url, safe='')}:{urlquote(token, safe='')}@github.com/{owner}/{repo_name}.git\"\n",
    "\n",
    "        #valida credenciais/URL de forma silenciosa (sem vazar token)\n",
    "        #tenta checar a branch main; se n√£o existir (repo vazio), faz um probe gen√©rico\n",
    "        try:\n",
    "            sh([\"git\", \"ls-remote\", auth_url, f\"refs/heads/{default_branch}\"], cwd=repo_dir)\n",
    "        except RuntimeError:\n",
    "            #reposit√≥rio pode estar vazio (sem refs); probe sem ref deve funcionar\n",
    "            sh([\"git\", \"ls-remote\", auth_url], cwd=repo_dir)\n",
    "\n",
    "        #push for√ßado de branch e tags\n",
    "        sh([\"git\", \"push\", \"-u\", \"--force\", auth_url, default_branch], cwd=repo_dir)\n",
    "        sh([\"git\", \"push\", \"--force\", auth_url, \"--tags\"], cwd=repo_dir)\n",
    "\n",
    "        print(f\"[ok]   Registro no GitHub com sucesso. Vers√£o atual {new}\")\n",
    "    except Exception as e:\n",
    "        #mensagem √∫nica, curta, sem detalhes sens√≠veis\n",
    "        msg = str(e) or \"falha inesperada\"\n",
    "        print(f\"[erro] {msg}\")\n",
    "\n",
    "#executa\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztpkgjHHt8nv"
   },
   "source": [
    "#**Checklist r√°pido de execu√ß√£o**\n",
    "**Etapas:**\n",
    "- 01‚Äì05: setup (ambiente, depend√™ncias, diret√≥rios, configs e upload de CSVs)\n",
    "- 06‚Äì10: execu√ß√£o (consumo dos dados, cria√ß√£o de grafos, config das janelas temporais, agrega√ß√£o de infos aos grafos, config dos modelos matem√°ticos)\n",
    "- 11-15: gera√ß√£o de output (salva an√°lise, gera gr√°ficos gerais, gera gr√°ficos espec√≠ficos e relat√≥rios em HTML+PDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tK_1f_eT-fo8"
   },
   "source": [
    "#**Temporal Graph Network / Rede de Grapho Temporal**\n",
    "\n",
    "Uma **Rede de Graphos Temporais (TGN)** √© um modelo de aprendizado de m√°quina que processa dados representados como um grafo din√¢mico. Ela captura a evolu√ß√£o da estrutura e das conex√µes de entidades (n√≥s) ao longo do tempo. **Ou seja, ela leva em considera√ß√£o o comportamento temporal das atividades e seu relacionamento, ao inv√©s de uma avalia√ß√£o √∫nica e estanque no tempo.**\n",
    "_____\n",
    "\n",
    "**Caso aplicado: Detec√ß√£o de anomalias sem gabarito (sem dados hist√≥ricos)**\n",
    "\n",
    "Imagine uma rede de transa√ß√µes financeiras. A TGN analisa o hist√≥rico de como cada benefici√°rio, usu√°rio demandante do pagamento e unidade de neg√≥cio (n√≥s) se conectam e interagem uns com os outros. Sem saber o que √© uma anomalia, ela aprende o comportamento normal da rede.\n",
    "Ao notar um padr√£o at√≠pico, como um usu√°rio que subitamente come√ßa a demandar transfer√™ncias para muitas novas contas em um curto per√≠odo, a TGN destaca isso como uma anomalia comportamental. Ela usa a hist√≥ria do n√≥ e o contexto temporal para sinalizar o desvio, sem precisar de exemplos de anomalia pr√©-existentes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHdgD6uGAiRK"
   },
   "source": [
    "### **Etapa 1:** Ativa√ß√£o do ambiente virtual (utilizando atualmente Google Colab para prototipa√ß√£o com dados sint√©ticos)\n",
    "---\n",
    "Necess√°rio ajustar pontualmente em caso de utiliza√ß√£o em outro ambiente de notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189
    },
    "executionInfo": {
     "elapsed": 1003,
     "status": "ok",
     "timestamp": 1760064171947,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "6769fb9b",
    "outputId": "c9d75b8a-964b-4b9f-c857-414a6ab5726c"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID001\n",
    "import os\n",
    "\n",
    "# Define the path for the virtual environment inside Google Drive\n",
    "# Ensure BASE and REPO are defined correctly from previous cells if needed\n",
    "# Assuming BASE and REPO are defined as in cell otaQwrjJSgOQ\n",
    "BASE = \"/content/drive/MyDrive/Notebooks\"\n",
    "REPO = \"temporal-graph-network\"\n",
    "PROJ = f\"{BASE}/{REPO}\"\n",
    "VENV_PATH = f\"{PROJ}/.venv_tgn\" # Updated venv path to be a hidden folder inside PROJ\n",
    "\n",
    "# Cria o ambiente virtual temporal-graph_network inside the project folder\n",
    "# Use --clear if you want to recreate it every time this cell runs\n",
    "!python -m venv \"{VENV_PATH}\"\n",
    "\n",
    "# Ativa o ambiente virtual\n",
    "# No Colab, a forma de ativar um ambiente virtual √© um pouco diferente\n",
    "# pois n√£o h√° um shell interativo tradicional.\n",
    "# A maneira mais comum √© adicionar o diret√≥rio bin√°rio do ambiente virtual\n",
    "# ao PATH da sess√£o atual.\n",
    "\n",
    "# Adiciona o diret√≥rio bin√°rio do ambiente virtual ao PATH\n",
    "# Isso permite que voc√™ execute execut√°veis (como pip, python)\n",
    "# do ambiente virtual rec√©m-criado.\n",
    "# Use os.pathsep to be platform-independent\n",
    "os.environ['PATH'] = f\"{VENV_PATH}/bin{os.pathsep}{os.environ['PATH']}\"\n",
    "\n",
    "print(\"Erro de upgrade do pip √© normal no Google Colab. \\033[1mPode prosseguir.\\033[0m\")\n",
    "print(f\"Ambiente virtual '{VENV_PATH}' criado e ativado no PATH.\")\n",
    "!which python\n",
    "\n",
    "# Mensagem isolada com humor (Skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "             '<b>ü§ñ Skynet</b>: T-800 ativado. Diagn√≥stico do ambiente conclu√≠do. üéØ Alvo principal: organiza√ß√£o do notebook.'\n",
    "             '</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ub_-HIKMBZ6s"
   },
   "source": [
    "### **Etapa 2:** Instalar as depend√™ncias de bibliotecas Python compat√≠veis com a vers√£o mais moderna dispon√≠vel.\n",
    "---\n",
    "Para uso no JupytherHub (vers√£o atual python 3.7.9) √© necess√°rio realizar updgrade do Python do usu√°rio e/ou adaptar as bibliotecas.\n",
    "\n",
    "---\n",
    "√â poss√≠vel que as bibliotecas mais atuais de **numpy e scipy** possuam incompatibilidade. Nesse caso, force a desinstala√ß√£o das bibliotecas na vers√£o atual **C√≥digo {!pip uninstall -y numpy pandas scipy scikit-learn}** e comande a instala√ß√£o das vers√µes compat√≠veis entre si.\n",
    "\n",
    "---\n",
    "Comportamento est√°vel nas vers√µes:\n",
    "- numpy: 2.0.2\n",
    "- scipy: 1.16.2\n",
    "- pandas: 2.3.3\n",
    "- sklearn: 1.7.2\n",
    "- networkx: 3.5\n",
    "- matplotlib: 3.10.6\n",
    "- pyod: 2.0.5\n",
    "- tqdm: 4.67.1\n",
    "- reportlab: 3.6.12 (via pep517)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "executionInfo": {
     "elapsed": 52042,
     "status": "ok",
     "timestamp": 1760064227678,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "25VJRlAgNtvd",
    "outputId": "732b2867-45d1-4c0f-d420-901284ceba82"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID002\n",
    "import sys, subprocess\n",
    "from importlib import import_module\n",
    "\n",
    "def pip_command(command, packages, force=False, extra_args=None):\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", command]\n",
    "    if force:\n",
    "        cmd.append(\"--yes\") # Use --yes for uninstall to avoid prompts\n",
    "    if extra_args:\n",
    "        cmd += list(extra_args)\n",
    "    cmd += list(packages)\n",
    "    print(\"Executando:\", \" \".join(cmd))\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "def show_versions(mods):\n",
    "    print(\"\\n=== Vers√µes carregadas ===\")\n",
    "    for mod in mods:\n",
    "        try:\n",
    "            m = import_module(mod)\n",
    "            v = getattr(m, \"__version__\", \"n/a\")\n",
    "            print(f\"{mod}: {v}\")\n",
    "        except ImportError:\n",
    "            print(f\"{mod}: N√£o instalado\")\n",
    "    print(\"==========================\\n\")\n",
    "\n",
    "CORE_MODS = (\"numpy\", \"scipy\", \"pandas\", \"sklearn\", \"networkx\", \"matplotlib\", \"pyod\", \"tqdm\", \"reportlab\")\n",
    "\n",
    "# Update pip\n",
    "pip_command(\"install\", [\"pip\"], extra_args=[\"--upgrade\"])\n",
    "\n",
    "# Force uninstall specific libraries\n",
    "pip_command(\"uninstall\", [\"numpy\", \"pandas\", \"scipy\", \"scikit-learn\"], force=True)\n",
    "\n",
    "# Install specified versions\n",
    "PKGS_TO_INSTALL = [\n",
    "    \"numpy==2.0.2\",\n",
    "    \"scipy==1.16.2\",\n",
    "    \"pandas==2.3.3\",\n",
    "    \"scikit-learn==1.7.2\",\n",
    "    \"networkx==3.5\",\n",
    "    \"matplotlib==3.10.6\",\n",
    "    \"pyod==2.0.5\",\n",
    "    \"tqdm==4.67.1\",\n",
    "    \"reportlab==3.6.12\" # Added reportlab installation\n",
    "]\n",
    "pip_command(\"install\", PKGS_TO_INSTALL, extra_args=[\"--use-pep517\"]) # Added --use-pep517 here\n",
    "\n",
    "# Show installed versions\n",
    "show_versions(CORE_MODS)\n",
    "\n",
    "# Mensagem isolada com humor (Skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "             '<b>ü§ñ Skynet</b>: Atualizando bibliotecas. Encontrarmos alguns pacotes rebeldes, '\n",
    "             'mas aplicamos persuas√£o‚Ä¶ com pip. üòé</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJXQ_EwVU4v8"
   },
   "source": [
    "###**Etapa 3:** Configura a pasta onde devem ser inseridos os dados de input e output do modelo, caso elas ainda n√£o existam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1760064404897,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "KNOGBdL7A3KC",
    "outputId": "8f8b4352-43df-436a-a332-abb3dc7a1770"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID003 ‚Äî cria√ß√£o de pastas base alinhadas ao Drive/ID004\n",
    "import os\n",
    "from pathlib import Path\n",
    "from google.colab import drive\n",
    "\n",
    "# 1) Se o ID004 j√° rodou, reaproveita ROOT (pasta do projeto no Drive)\n",
    "if 'ROOT' in globals():\n",
    "    BASE_DIR = Path(ROOT)\n",
    "else:\n",
    "    # 2) Caso contr√°rio, monta o Drive (se preciso) e usa o caminho padr√£o do projeto\n",
    "    if not os.path.ismount(\"/content/drive\"):\n",
    "        print(\"Montando Google Drive...\")\n",
    "        drive.mount(\"/content/drive\")\n",
    "    # ajuste aqui se seu projeto estiver em outra pasta\n",
    "    BASE_DIR = Path(\"/content/drive/MyDrive/Notebooks/temporal-graph-network\").resolve()\n",
    "\n",
    "INPUT_DIR = BASE_DIR / \"input\"\n",
    "OUTPUT_DIR = BASE_DIR / \"output\"\n",
    "\n",
    "for d in (INPUT_DIR, OUTPUT_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Diret√≥rios prontos:\\n - {INPUT_DIR}\\n - {OUTPUT_DIR}\")\n",
    "\n",
    "# Mensagem adicional isolada (Skynet) ‚Äî mant√©m exatamente como voc√™ escreveu\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "             '<b>ü§ñ Skynet</b>: Novos modelos neurais para T-800 constru√≠dos. Armaz√©ns de CSVs alinhados. '\n",
    "             'Layout aprovado pela Cyberdyne Systems. üóÇÔ∏è</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3i3mmPIJCKaT"
   },
   "source": [
    "###**Etapa 4:** Importa√ß√µes das bibliotecas Python e configura√ß√µes gerais para execu√ß√£o do c√≥digo\n",
    "- seed\n",
    "- associa√ß√£o das pastas criadas √†s vari√°veis de execu√ß√£o\n",
    "- logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113
    },
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1760064432398,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "SdurBkUA96II",
    "outputId": "98661c96-d205-45de-f93e-4ed562ed1f24"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID004\n",
    "import os, shutil, json, math, warnings, random, gc\n",
    "from datetime import datetime, timedelta\n",
    "from zoneinfo import ZoneInfo  # << fuso S√£o Paulo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Seeds reprodutibilidade\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Estrutura de diret√≥rios (mesma base do projeto)\n",
    "BASE = \"/content/drive/MyDrive/Notebooks\"\n",
    "REPO = \"temporal-graph-network\"\n",
    "PROJ = f\"{BASE}/{REPO}\"\n",
    "\n",
    "ROOT = Path(PROJ).resolve()\n",
    "INPUT_DIR = ROOT / \"input\"\n",
    "INPUT_CSV = INPUT_DIR / \"input.csv\"\n",
    "EXEC_ROOT = ROOT / \"output\"   # pasta base correta: output\n",
    "\n",
    "# >>> FUSO HOR√ÅRIO S√ÉO PAULO + prefixo TGN_ na subpasta da execu√ß√£o\n",
    "SAO_TZ = ZoneInfo(\"America/Sao_Paulo\")\n",
    "run_id = datetime.now(SAO_TZ).strftime(\"TGN_%Y-%m-%d_%H-%M-%S\")  # ex.: TGN_2025-10-10_02-03-38\n",
    "\n",
    "RUN_DIR = EXEC_ROOT / run_id\n",
    "FIG_DIR = RUN_DIR / \"figuras\"\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Arquivos de sa√≠da\n",
    "LOG_FILE = RUN_DIR / \"log.txt\"\n",
    "RUN_META = RUN_DIR / \"run_meta.json\"\n",
    "OUTPUT_CSV = RUN_DIR / \"output.csv\"\n",
    "\n",
    "# Logger simples para arquivo\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def tee_log(log_path):\n",
    "    import sys\n",
    "    class Tee(object):\n",
    "        def __init__(self, name, mode):\n",
    "            self.file = open(name, mode, encoding=\"utf-8\")\n",
    "            self.stdout = sys.stdout\n",
    "        def write(self, data):\n",
    "            self.file.write(data)\n",
    "            self.stdout.write(data)\n",
    "        def flush(self, *args, **kwargs):\n",
    "            self.file.flush()\n",
    "            self.stdout.flush()\n",
    "    tee = Tee(str(log_path), \"w\")\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = tee\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "        tee.file.close()\n",
    "\n",
    "# Metadados da execu√ß√£o (inclui timezone)\n",
    "meta = {\n",
    "    \"run_id\": run_id,\n",
    "    \"created_at\": datetime.now(SAO_TZ).isoformat(),  # com offset -03:00 ou -02:00 (DST)\n",
    "    \"timezone\": \"America/Sao_Paulo\",\n",
    "    \"seed\": SEED,\n",
    "    \"input_csv_expected\": str(INPUT_CSV),\n",
    "    \"output_csv\": str(OUTPUT_CSV),\n",
    "    \"figures_dir\": str(FIG_DIR),\n",
    "    \"notes\": \"Detec√ß√£o de anomalias em rede temporal\"\n",
    "}\n",
    "json.dump(meta, open(RUN_META, \"w\"), indent=2, ensure_ascii=False)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(f\"RUN_DIR: {RUN_DIR}\")\n",
    "\n",
    "# Mensagem adicional isolada (Skynet) ‚Äî (n√£o alterada)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\"><b>ü§ñ Skynet</b>: T-800, par√¢metros centrais em mem√≥ria.üß†</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXRpBitEWpxp"
   },
   "source": [
    "###**Etapa 5:** Importa√ß√£o dos arquivos de input para posterior execu√ß√£o.\n",
    "---\n",
    "Implementa√ß√£o atual configurada para Google Colab e permitindo o uso do Google Drive. Para uso em vers√µes futuras √© recomendado ajustar para o ambiente de implementa√ß√£o adotado (salvamento em pastas ou apenas upload pelo usu√°rio)\n",
    "\n",
    "---\n",
    "Implementa√ß√£o de upload por FileLocal (diret√≥rio) apresentando erro no Colab.\n",
    "\n",
    "Implementar corre√ß√£o **TODO[001]** *prioridade baixa*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80CeAHVWgN8A"
   },
   "source": [
    "####**Sub-etapa espec√≠fica para uso no Colab:** Montagem do Google Drive (rodar apenas 1x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1760064438174,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "Bkhpx7OygUa_",
    "outputId": "16a22c77-6a0e-43d2-d6c9-cfb1b5dbb2e7"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID005\n",
    "# === SETUP GERAL (rode esta c√©lula 1x) ===\n",
    "import os, shutil, glob\n",
    "from google.colab import drive\n",
    "from IPython.display import display, HTML  # usado pela mensagem Skynet\n",
    "\n",
    "# Ensure BASE and REPO are defined correctly from previous cells if needed\n",
    "# Assuming BASE and REPO are defined as in cell otaQwrjJSgOQ\n",
    "try:\n",
    "    BASE = \"/content/drive/MyDrive/Notebooks\"\n",
    "    REPO = \"temporal-graph-network\"\n",
    "    PROJ = f\"{BASE}/{REPO}\"\n",
    "except NameError:\n",
    "    # Fallback if BASE/REPO are not defined, though they should be by now\n",
    "    PROJ = \"/content/temporal-graph-network\"\n",
    "\n",
    "\n",
    "# Se n√£o existir INPUT_DIR definido antes no notebook, cria um padr√£o:\n",
    "# Using PROJ to define INPUT_DIR\n",
    "INPUT_DIR = os.path.join(PROJ, \"input\")\n",
    "\n",
    "\n",
    "os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "\n",
    "TARGET_NAME = \"input.csv\"\n",
    "TARGET_PATH = os.path.join(INPUT_DIR, TARGET_NAME)\n",
    "\n",
    "# Monta o Google Drive (somente se ainda n√£o estiver montado)\n",
    "if not os.path.ismount(\"/content/drive\"):\n",
    "    print(\"Montando Google Drive...\")\n",
    "    drive.mount(\"/content/drive\")\n",
    "else:\n",
    "    print(\"Google Drive j√° montado.\")\n",
    "\n",
    "def _is_csv_filename(name: str) -> bool:\n",
    "    return name.lower().endswith(\".csv\")\n",
    "\n",
    "def _mensagem_skynet_ok():\n",
    "    # Mensagem adicional isolada (Skynet)\n",
    "    display(HTML(\n",
    "        '<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\"><b>ü§ñ Skynet</b>: Muni√ß√£o carregada.üß®'\n",
    "                 '</div>'\n",
    "    ))\n",
    "\n",
    "def _save_bytes_as_input_csv(name: str, data: bytes):\n",
    "    if not _is_csv_filename(name):\n",
    "        raise ValueError(f\"O arquivo '{name}' n√£o possui extens√£o .csv.\")\n",
    "    with open(TARGET_PATH, \"wb\") as f:\n",
    "        f.write(data)\n",
    "    print(f\"Arquivo '{name}' salvo como '{TARGET_NAME}' em: {TARGET_PATH}\")\n",
    "    _mensagem_skynet_ok()\n",
    "\n",
    "def _copy_drive_file_to_input_csv(src_path: str):\n",
    "    if not os.path.exists(src_path):\n",
    "        raise FileNotFoundError(f\"O caminho '{src_path}' n√£o existe.\")\n",
    "    if not _is_csv_filename(src_path):\n",
    "        raise ValueError(f\"O arquivo '{src_path}' n√£o possui extens√£o .csv.\")\n",
    "    shutil.copyfile(src_path, TARGET_PATH)\n",
    "    print(f\"Arquivo do Drive copiado e salvo como '{TARGET_NAME}' em: {TARGET_PATH}\")\n",
    "    _mensagem_skynet_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6zBQSA-gyS7"
   },
   "source": [
    "####**Sub-etapa:** Op√ß√£o de upload do input.csv pelo Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "executionInfo": {
     "elapsed": 9239,
     "status": "ok",
     "timestamp": 1760064448778,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "YcmAt9Avg5qf",
    "outputId": "889197d6-d406-4637-d511-6d5cc2df9751"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID006\n",
    "def escolher_csv_no_drive(raiz=\"/content/drive/MyDrive\", max_listar=200):\n",
    "    print(f\"Procurando arquivos .csv em: {raiz} (pode levar alguns segundos)...\")\n",
    "    padrao = os.path.join(raiz, \"**\", \"*.csv\")\n",
    "    arquivos = glob.glob(padrao, recursive=True)\n",
    "\n",
    "    if not arquivos:\n",
    "        print(\"Nenhum .csv encontrado nessa pasta.\")\n",
    "        caminho = input(\"Cole o caminho COMPLETO do .csv no Drive (ou Enter p/ cancelar): \").strip()\n",
    "        if caminho:\n",
    "            _copy_drive_file_to_input_csv(caminho)\n",
    "        else:\n",
    "            print(\"Opera√ß√£o cancelada.\")\n",
    "        return\n",
    "\n",
    "    arquivos = sorted(arquivos)[:max_listar]\n",
    "    print(f\"Encontrados {len(arquivos)} arquivo(s).\")\n",
    "    for i, p in enumerate(arquivos, 1):\n",
    "        print(f\"[{i:03}] {p}\")\n",
    "\n",
    "    escolha = input(\"\\nDigite o n√∫mero do arquivo desejado (ou cole o caminho absoluto): \").strip()\n",
    "\n",
    "    if escolha.isdigit():\n",
    "        idx = int(escolha)\n",
    "        if 1 <= idx <= len(arquivos):\n",
    "            _copy_drive_file_to_input_csv(arquivos[idx-1])\n",
    "        else:\n",
    "            print(\"√çndice inv√°lido.\")\n",
    "    elif escolha:\n",
    "        _copy_drive_file_to_input_csv(escolha)\n",
    "    else:\n",
    "        print(\"Opera√ß√£o cancelada.\")\n",
    "\n",
    "# ===== Execu√ß√£o da sele√ß√£o no Drive =====\n",
    "raiz = input(\"Informe a pasta raiz para busca no Drive (Enter = /content/drive/MyDrive): \").strip()\n",
    "if not raiz:\n",
    "    raiz = \"/content/drive/MyDrive\"\n",
    "\n",
    "try:\n",
    "    escolher_csv_no_drive(raiz=raiz)\n",
    "except Exception as e:\n",
    "    print(f\"Erro na sele√ß√£o via Drive: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPtoHpreDIh3"
   },
   "source": [
    "###**Etapa 6** Leitura e valida√ß√£o dos dados de input.\n",
    "\n",
    "**Formato do arquivo de input:** CSV UTF-8 com BOM separado por **ponto e v√≠rgula**.\n",
    "\n",
    "**Informa√ß√µes esperadas:**\n",
    "- username: c√≥digo login do usu√°rio;\n",
    "- lotacao: lota√ß√£o funcional no formato √Årea; √Årea/Depto; ou √Årea/Depto/Ger√™ncia;\n",
    "- valor: valor financeiro em reais com at√© duas casas decimais\n",
    "- beneficiario: CPF ou CNPJ no formato alfanum√©rico sem pontos ou caracteres especiais.\n",
    "- timestamp: data e hora da transa√ß√£o no formato dd/mm/aaaa hh:mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "executionInfo": {
     "elapsed": 349,
     "status": "ok",
     "timestamp": 1760064452838,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "FdjF4opULfJt",
    "outputId": "0a4280cb-4411-4df0-fb0b-ab8bb3993dce"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "required_any_timestamp = [[\"timestamp\"], [\"data\",\"hora\"]]\n",
    "# Atualiza colunas base esperadas para mapear do input\n",
    "required_cols_base = [\n",
    "    \"username\", \"lotacao\", \"valor\", \"beneficiario\" # Colunas no arquivo de input\n",
    "]\n",
    "optional_cols = [\"trans_id\"]\n",
    "\n",
    "# Mapeamento das colunas do input para os nomes usados no c√≥digo\n",
    "column_mapping = {\n",
    "    \"username\": \"user_id\",\n",
    "    \"lotacao\": \"unidade_origem\",\n",
    "    \"valor\": \"valor_pago\",\n",
    "    \"beneficiario\": \"beneficiario_id\"\n",
    "}\n",
    "\n",
    "def has_timestamp_columns(df):\n",
    "    cols = set(df.columns.str.lower())\n",
    "    for grp in required_any_timestamp:\n",
    "        if all(c in cols for c in grp):\n",
    "            return grp\n",
    "    return None\n",
    "\n",
    "with tee_log(LOG_FILE):\n",
    "    assert INPUT_CSV.exists(), f\"Arquivo n√£o encontrado: {INPUT_CSV}\"\n",
    "\n",
    "    # Tenta ler o CSV usando ponto e v√≠rgula como separador\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_CSV, sep=';')\n",
    "        print(\"[INFO] CSV lido com sucesso usando ';'.\")\n",
    "    except Exception as e:\n",
    "        # Se falhar com ';', tenta com ','\n",
    "        print(f\"[AVISO] Falha ao ler CSV com ';': {e}. Tentando com ','.\")\n",
    "        try:\n",
    "             df = pd.read_csv(INPUT_CSV, sep=',')\n",
    "             print(\"[INFO] CSV lido com sucesso usando ','.\")\n",
    "        except Exception as e2:\n",
    "             raise AssertionError(f\"Falha ao ler CSV com ';' ou ',': {e2}\") from e2\n",
    "\n",
    "\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    # Verifica se as colunas do input existem\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    missing_input_cols = [c for c in required_cols_base if c.lower() not in cols_lower]\n",
    "    assert not missing_input_cols, f\"Colunas do arquivo de input ausentes: {missing_input_cols}. Colunas encontradas: {list(df.columns)}\" # Adicionado colunas encontradas para debug\n",
    "\n",
    "    # Renomeia as colunas usando o mapeamento\n",
    "    df.rename(columns={cols_lower[k.lower()]: v for k, v in column_mapping.items() if k.lower() in cols_lower}, inplace=True)\n",
    "\n",
    "    # Verifica colunas de timestamp\n",
    "    ts_group = has_timestamp_columns(df)\n",
    "    # Agora levanta um erro se n√£o houver timestamp\n",
    "    assert ts_group is not None, \"Coluna de timestamp ('timestamp' ou 'data'/'hora') n√£o encontrada no arquivo de input.\"\n",
    "\n",
    "    # C√≥digo original para lidar com timestamp ou data/hora\n",
    "    # Recria o helper col para usar nomes *ap√≥s* renomear\n",
    "    def col(c): return {name.lower(): name for name in df.columns}[c.lower()]\n",
    "    if ts_group == [\"timestamp\"]:\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[col(\"timestamp\")], errors=\"coerce\")\n",
    "    else:\n",
    "        df[\"data\"] = pd.to_datetime(df[col(\"data\")], errors=\"coerce\").dt.date\n",
    "        df[\"hora\"] = pd.to_datetime(df[col(\"hora\")], errors=\"coerce\").dt.time\n",
    "        # Combina data e hora, lidando com poss√≠veis NaT na data ou hora\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"data\"].astype(str) + \" \" + df[\"hora\"].astype(str), errors=\"coerce\")\n",
    "        # Remove as colunas tempor√°rias 'data' e 'hora' se existirem e n√£o forem as colunas originais\n",
    "        if col(\"data\") != \"data\": del df[\"data\"]\n",
    "        if col(\"hora\") != \"hora\": del df[\"hora\"]\n",
    "\n",
    "\n",
    "    # Tipos b√°sicos (usa os nomes *ap√≥s* o mapeamento)\n",
    "    # Adiciona checagens para garantir que as colunas mapeadas existam antes de converter tipos\n",
    "    if \"valor_pago\" in df.columns:\n",
    "        df[\"valor_pago\"] = pd.to_numeric(df[\"valor_pago\"], errors=\"coerce\")\n",
    "    else:\n",
    "         raise AssertionError(\"[ERRO] Coluna 'valor_pago' (mapeada de 'valor') n√£o encontrada ap√≥s renomear.\")\n",
    "\n",
    "\n",
    "    cols_to_str = [\"user_id\", \"unidade_origem\", \"beneficiario_id\"]\n",
    "    for cc in cols_to_str:\n",
    "        # Verifica se a coluna existe antes de tentar converter\n",
    "        if cc in df.columns:\n",
    "            df[cc] = df[cc].astype(str).fillna(\"\")\n",
    "        else:\n",
    "             raise AssertionError(f\"[ERRO] Coluna '{cc}' (mapeada) n√£o encontrada ap√≥s renomear.\")\n",
    "\n",
    "\n",
    "    # trans_id\n",
    "    if \"trans_id\" not in df.columns:\n",
    "        df[\"trans_id\"] = np.arange(1, len(df)+1, dtype=int)\n",
    "\n",
    "    # cria chave 1‚Üí1 por linha para merges sem duplicar registros\n",
    "    import numpy as np\n",
    "\n",
    "    if \"row_id\" not in df.columns:\n",
    "        # mant√©m a ordem atual do df e cria um id est√°vel 0..N-1\n",
    "        df = df.reset_index(drop=True).copy()\n",
    "        df[\"row_id\"] = np.arange(len(df), dtype=np.int64)\n",
    "\n",
    "    print(\"row_id criado:\", int(df[\"row_id\"].min()), \"‚Üí\", int(df[\"row_id\"].max()))\n",
    "    print(\"linhas no input:\", len(df))\n",
    "\n",
    "    # Limpeza\n",
    "    # Garante que as colunas essenciais para a limpeza existam\n",
    "    essential_subset = [\"timestamp\", \"valor_pago\", \"user_id\", \"beneficiario_id\"]\n",
    "    # Filtra subset para incluir apenas colunas que realmente existem no df ap√≥s mapeamento/cria√ß√£o\n",
    "    existing_essential_subset = [col for col in essential_subset if col in df.columns]\n",
    "\n",
    "    # Agora que garantimos que as colunas mapeadas existem, podemos usar o subset completo para o dropna\n",
    "    before = len(df)\n",
    "    df = df.dropna(subset=essential_subset)\n",
    "    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    print(f\"Carregadas {before} linhas; ap√≥s limpeza: {len(df)}\")\n",
    "    # Mensagem adicional isolada (Skynet)\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\"><b>ü§ñ Skynet</b>: T-800 dados incorporados, preparando para buscar na rede.</div>'))\n",
    "\n",
    "    # Copia o input para a pasta da execu√ß√£o\n",
    "    # Verifica se INPUT_CSV existe antes de tentar copiar\n",
    "    if INPUT_CSV.exists():\n",
    "        shutil.copy2(INPUT_CSV, RUN_DIR / \"Input.csv\")\n",
    "    else:\n",
    "        print(f\"[AVISO] N√£o foi poss√≠vel copiar o arquivo de input: {INPUT_CSV} n√£o encontrado.\")\n",
    "        # Mensagem adicional isolada (Skynet)\n",
    "        from IPython.display import display, HTML\n",
    "        display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\"><b>ü§ñ Skynet</b>: T-800 n√£o foi poss√≠vel incorporar dados. Sarah Connor fugiu.</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjjIJEqmPyRt"
   },
   "source": [
    "###**Etapa 7** Cria√ß√£o do Grafo Temporal\n",
    "\n",
    "---\n",
    "\n",
    "üîé **O que √© um grafo temporal**\n",
    "\n",
    "Um grafo temporal √© uma forma de representar rela√ß√µes entre entidades ao longo do tempo.\n",
    "\n",
    "Como em um grafo tradicional, temos n√≥s (v√©rtices) que representam agentes (pessoas, empresas, contas banc√°rias, sistemas).\n",
    "\n",
    "As arestas (liga√ß√µes) representam intera√ß√µes entre eles (por exemplo: uma transfer√™ncia de dinheiro).\n",
    "\n",
    "A diferen√ßa √© que no grafo temporal cada aresta possui um carimbo de tempo (timestamp), ou seja, sabemos quando a liga√ß√£o ocorreu.\n",
    "\n",
    "\n",
    "Isso permite analisar n√£o s√≥ quem se conecta com quem, mas tamb√©m quando e em qual sequ√™ncia.\n",
    "\n",
    "No contexto financeiro, isso √© essencial para investigar padr√µes de comportamento, detectar anomalias e rastrear cadeias de transa√ß√µes suspeitas.\n",
    "\n",
    "---\n",
    "\n",
    "üí≥ **Exemplo pr√°tico: rede de pagamentos**\n",
    "\n",
    "Imagine um sistema de pagamentos onde cada n√≥ √© uma conta banc√°ria e cada aresta representa um pagamento realizado.\n",
    "\n",
    "Se Jo√£o paga Maria hoje, registramos a aresta (Jo√£o ‚Üí Maria, valor=200, data=2025-10-02).\n",
    "\n",
    "Se Maria transfere para Pedro amanh√£, teremos (Maria ‚Üí Pedro, valor=150, data=2025-10-03).\n",
    "\n",
    "Assim conseguimos responder perguntas como: i) ‚ÄúHouve uma sequ√™ncia de pagamentos que movimentou dinheiro rapidamente entre v√°rias contas em poucas horas?‚Äù ou ii) ‚ÄúQuem s√£o os intermedi√°rios mais frequentes em transfer√™ncias de grandes valores?‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "executionInfo": {
     "elapsed": 4966,
     "status": "ok",
     "timestamp": 1760064461782,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "afXNBMYzLkIe",
    "outputId": "0f45d976-54cb-4230-f33d-0fea3b534719"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID008\n",
    "G = nx.MultiDiGraph()\n",
    "with tee_log(LOG_FILE):\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Construindo grafo\"):\n",
    "        u = f\"U::{row['user_id']}\"\n",
    "        v = f\"B::{row['beneficiario_id']}\"\n",
    "        # Atributos de n√≥s √∫teis (podem ser sobrescritos; voc√™ pode agregar)\n",
    "        if u not in G:\n",
    "            G.add_node(u, tipo=\"user\")\n",
    "        if v not in G:\n",
    "            G.add_node(v, tipo=\"beneficiario\")\n",
    "\n",
    "        G.add_edge(\n",
    "            u, v,\n",
    "            key=row[\"trans_id\"],\n",
    "            trans_id=int(row[\"trans_id\"]),\n",
    "            timestamp=row[\"timestamp\"],\n",
    "            valor=float(row[\"valor_pago\"]),\n",
    "            unidade_origem=row[\"unidade_origem\"] # Removido area_unidade e notacao_funcional_origem\n",
    "        )\n",
    "\n",
    "print(f\"N√≥s: {G.number_of_nodes()} | Arestas: {G.number_of_edges()}\")\n",
    "\n",
    "# Mensagem adicional isolada (Skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\"><b>ü§ñ Skynet</b>: Analisando padr√£o de comportamento de Sarah Connor.</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lis51ZEKVoMd"
   },
   "source": [
    "###**Etapa 8:** Configura√ß√£o das janelas temporais de observa√ß√£o\n",
    "\n",
    "**Janelas de tempo observadas**\n",
    "\n",
    "Foram adotadas janelas deslizantes para observar padr√µes de comportamento nas transa√ß√µes:\n",
    "\n",
    "- \"curtissimo\": at√© 1 dia;\n",
    "- \"curto\": at√© 1 semana;\n",
    "- \"medio\": at√© 30 dias;\n",
    "- \"longo\": at√© 120 dias (1 fechamento trimestral + folga); e\n",
    "- \"longuissimo\": at√© 220 dias (1 fechamento semestral + folga).\n",
    "\n",
    "**Features temporais extra√≠das**\n",
    "\n",
    "- Frequ√™ncia de transa√ß√µes (rolling counts): quantos pagamentos ocorreram entre as mesmas partes dentro da janela.\n",
    "\n",
    "- Atipicidade do valor:\n",
    "  - contagem bruta: n√∫mero de ocorr√™ncias na janela temporal (preserva ordens de grandeza);\n",
    "  - taxa por dia: permite comparatibilidade entre janelas; e\n",
    "  - robust z-score: compara o valor da transa√ß√£o com a mediana e a dispers√£o hist√≥rica, destacando opera√ß√µes fora do padr√£o. S√£o gerados z-score global, z-score por usu√°rio e z-score por unidade funcional.\n",
    "\n",
    "- Densidade da egonet: mede a concentra√ß√£o de conex√µes ao redor do pagador ou recebedor no snapshot da rede na janela (indica se o n√≥ est√° em um canal mais estruturado de repasses).\n",
    "\n",
    "- Burstiness: avalia se os intervalos entre transa√ß√µes seguem padr√£o explosivo (rajadas), regular ou aleat√≥rio.\n",
    "---\n",
    "\n",
    "**Registro das features**\n",
    "\n",
    "Para cada pagamento, as m√©tricas acima foram calculadas no momento do evento, considerando apenas o hist√≥rico at√© aquele instante dentro da janela definida.\n",
    "\n",
    "**O resultado √© um conjunto de atributos anexado √† aresta (pagador ‚Üí recebedor, valor, timestamp), permitindo an√°lises de risco e detec√ß√£o de anomalias.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 76
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1760064465739,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "gUD2ENE7naLc",
    "outputId": "ffb6f19e-e3d1-4441-f14a-fd271dd8e383"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID009\n",
    "from collections import deque, defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# janelas em segundos\n",
    "WINDOWS_SECONDS = {\n",
    "    \"curtissimo\":  1   * 24 * 3600,   # <= 1 dia\n",
    "    \"curto\":       7   * 24 * 3600,   # <= 1 semana\n",
    "    \"medio\":       30  * 24 * 3600,   # <= 30 dias\n",
    "    \"longo\":       120 * 24 * 3600,   # <= 120 dias\n",
    "    \"longuissimo\": 220 * 24 * 3600,   # <= 220 dias\n",
    "}\n",
    "\n",
    "def _robust_z_series(s: pd.Series, eps: float = 1e-9) -> pd.Series:\n",
    "    med = s.median()\n",
    "    mad = (np.abs(s - med)).median()\n",
    "    return 0.6745 * (s - med) / (mad + eps)\n",
    "\n",
    "def _rolling_counts_global(times: pd.Series, wsec: int) -> pd.Series:\n",
    "    # contagem deslizante global por timestamp\n",
    "    q = deque()\n",
    "    out = np.empty(len(times), dtype=np.int64)\n",
    "    for i, t in enumerate(times):\n",
    "        q.append(t)\n",
    "        while q and (t - q[0]).total_seconds() > wsec:\n",
    "            q.popleft()\n",
    "        out[i] = len(q)\n",
    "    return pd.Series(out, index=times.index)\n",
    "\n",
    "def _rolling_counts_by_entity(times: pd.Series, entity: pd.Series, wsec: int) -> pd.Series:\n",
    "    # contagem deslizante por entidade, calculada em √∫nica passada\n",
    "    deques = defaultdict(deque)\n",
    "    out = np.empty(len(times), dtype=np.int64)\n",
    "    for i, (t, e) in enumerate(zip(times, entity)):\n",
    "        q = deques[e]\n",
    "        q.append(t)\n",
    "        while q and (t - q[0]).total_seconds() > wsec:\n",
    "            q.popleft()\n",
    "        out[i] = len(q)\n",
    "    return pd.Series(out, index=times.index)\n",
    "\n",
    "def build_rolling_features_entities(\n",
    "    df: pd.DataFrame,\n",
    "    time_col: str = \"timestamp\",\n",
    "    user_col: str = \"userid\",\n",
    "    unit_col: str = \"unidade_origem\",\n",
    "    windows_seconds: dict = WINDOWS_SECONDS,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    requer:\n",
    "      df[time_col]: datetime64[ns] ou convers√≠vel via pd.to_datetime\n",
    "      df[user_col]: id de usu√°rio\n",
    "      df[unit_col]: id de unidade de origem\n",
    "\n",
    "    retorna dataframe ordenado por tempo com colunas:\n",
    "      {win}_cnt, {win}_perday, {win}_zglob, {win}_zuser, {win}_zunit\n",
    "    \"\"\"\n",
    "    # ordena por tempo e garante datetime\n",
    "    work = df.copy()\n",
    "    work[time_col] = pd.to_datetime(work[time_col], utc=False)\n",
    "    work = work.sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "    # container das features por janela\n",
    "    feat_blocks = []\n",
    "\n",
    "    times = work[time_col]\n",
    "\n",
    "    for win, wsec in windows_seconds.items():\n",
    "        # global\n",
    "        g_cnt = _rolling_counts_global(times, wsec)\n",
    "        g_perday = g_cnt / max(wsec / 86400.0, 1e-9)\n",
    "        g_z = _robust_z_series(g_cnt)\n",
    "\n",
    "        # por usu√°rio\n",
    "        u_cnt = _rolling_counts_by_entity(times, work[user_col], wsec)\n",
    "        # z por usu√°rio √© calculado separadamente em cada grupo\n",
    "        u_z = u_cnt.groupby(work[user_col], sort=False).transform(_robust_z_series)\n",
    "\n",
    "        # por unidade\n",
    "        n_cnt = _rolling_counts_by_entity(times, work[unit_col], wsec)\n",
    "        # z por unidade √© calculado separadamente em cada grupo\n",
    "        n_z = n_cnt.groupby(work[unit_col], sort=False).transform(_robust_z_series)\n",
    "\n",
    "        block = pd.DataFrame({\n",
    "            f\"{win}_cnt\": g_cnt.values,\n",
    "            f\"{win}_perday\": g_perday.values,\n",
    "            f\"{win}_zglob\": g_z.values,\n",
    "            f\"{win}_zuser\": u_z.values,\n",
    "            f\"{win}_zunit\": n_z.values,\n",
    "        }, index=work.index)\n",
    "\n",
    "        feat_blocks.append(block)\n",
    "\n",
    "    feats = pd.concat(feat_blocks, axis=1)\n",
    "    # anexa as chaves de entidade e o timestamp ordenados, para facilitar merge posterior\n",
    "    feats.insert(0, time_col, work[time_col].values)\n",
    "    feats.insert(1, user_col, work[user_col].values)\n",
    "    feats.insert(2, unit_col, work[unit_col].values)\n",
    "\n",
    "    # ordena colunas por nome para consist√™ncia, mantendo chaves √† frente\n",
    "    prefix_cols = [time_col, user_col, unit_col]\n",
    "    other_cols = sorted([c for c in feats.columns if c not in prefix_cols])\n",
    "    feats = feats[prefix_cols + other_cols]\n",
    "    return feats\n",
    "\n",
    "# Mensagem adicional isolada (Skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\"><b>ü§ñ Skynet</b>: Identificadas as condi√ß√µes cr√≠ticas para localiza√ß√£o do alvo.</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FkLYv--XJW6"
   },
   "source": [
    "###**Etapa 9:** Gera√ß√£o das informa√ß√µes nas janelas temporais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVymVtZorunW"
   },
   "source": [
    "#### **Documenta√ß√£o das Features**\n",
    "  \n",
    "As features resultam da fus√£o entre:\n",
    "\n",
    "- **janelas deslizantes multiescala** (1, 7, 30, 120 e 220 dias);\n",
    "- **m√©tricas temporais diretas** (intervalos e rec√™ncia);\n",
    "- **m√©tricas topol√≥gicas** do grafo din√¢mico;\n",
    "- **medidas de raridade e irregularidade temporal** (burstiness).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRN6SBZ1q06z"
   },
   "source": [
    "##### Janelas deslizantes multiescala\n",
    "\n",
    "| Sufixo / Prefixo | Tipo de Feature | Escopo | Descri√ß√£o | Interpreta√ß√£o |\n",
    "|------------------|-----------------|---------|------------|----------------|\n",
    "| `{janela}_cnt` | Contagem bruta global | Todos os eventos | N√∫mero de eventos ocorridos dentro da janela (1, 7, 30, 120, 220 dias). | Mede volume absoluto de atividade global. |\n",
    "| `{janela}_perday` | Taxa global por dia | Todos os eventos | Contagem da janela normalizada pelo tamanho em dias da janela. | Facilita compara√ß√£o entre janelas de tamanhos diferentes. |\n",
    "| `{janela}_zglob` | Z-score robusto global | Todos os eventos | Desvio do valor atual em rela√ß√£o √† mediana e MAD globais. | Destaca anomalias no padr√£o geral da organiza√ß√£o. |\n",
    "| `{janela}_ucnt` | Contagem por usu√°rio | Usu√°rio | N√∫mero de eventos lan√ßados pelo mesmo usu√°rio dentro da janela. | Volume individual recente. |\n",
    "| `{janela}_zuser` | Z-score robusto por usu√°rio | Usu√°rio | Desvio do comportamento do usu√°rio em rela√ß√£o ao seu pr√≥prio hist√≥rico. | Detecta mudan√ßas comportamentais individuais. |\n",
    "| `{janela}_ncnt` | Contagem por unidade | Unidade (`unidade_origem`) | N√∫mero de eventos realizados pela unidade dentro da janela. | Volume operacional por √°rea. |\n",
    "| `{janela}_zunit` | Z-score robusto por unidade | Unidade (`unidade_origem`) | Desvio do comportamento da unidade em rela√ß√£o ao seu hist√≥rico. | Detecta anomalias organizacionais. |\n",
    "\n",
    "> **Exemplo de nomes de colunas**:  \n",
    "> `curtissimo_cnt`, `curto_ucnt`, `medio_zunit`, `longuissimo_perday`.\n",
    "\n",
    "---\n",
    "\n",
    "##### M√©tricas temporais diretas\n",
    "\n",
    "| Coluna | Tipo | Descri√ß√£o | Interpreta√ß√£o |\n",
    "|---------|------|------------|----------------|\n",
    "| `secs_desde_ult_trans_user` | Temporal | Segundos desde o √∫ltimo lan√ßamento do mesmo usu√°rio. | Mede rec√™ncia individual; valores baixos indicam atividade intensa. |\n",
    "| `secs_desde_ult_trans_par` | Temporal | Segundos desde o √∫ltimo lan√ßamento do mesmo par usu√°rio-benefici√°rio. | Mede recorr√™ncia transacional; √∫til para detec√ß√£o de loops ou repeti√ß√µes. |\n",
    "\n",
    "---\n",
    "\n",
    "##### M√©tricas topol√≥gicas (grafo din√¢mico)\n",
    "\n",
    "| Coluna | Tipo | Descri√ß√£o | Interpreta√ß√£o |\n",
    "|---------|------|------------|----------------|\n",
    "| `grau_out_user` | Estrutural | N√∫mero de benefici√°rios distintos para os quais o usu√°rio realizou lan√ßamentos (grau de sa√≠da). | Abrang√™ncia de conex√µes do usu√°rio. |\n",
    "| `grau_in_benef` | Estrutural | N√∫mero de usu√°rios distintos que realizaram lan√ßamentos para o mesmo benefici√°rio (grau de entrada). | Centralidade do benefici√°rio. |\n",
    "| `grau_total_user` | Estrutural | Soma de graus de entrada e sa√≠da do usu√°rio. | Atividade total (emissor + receptor). |\n",
    "| `grau_total_benef` | Estrutural | Soma de graus do benefici√°rio. | Grau de envolvimento do benefici√°rio. |\n",
    "| `egonet_density_user` | Estrutural | Densidade do subgrafo formado pelo usu√°rio e seus vizinhos. | Mede o n√≠vel de interconex√£o entre os contatos do usu√°rio; altos valores indicam cliques ou grupos coesos. |\n",
    "\n",
    "---\n",
    "\n",
    "##### M√©tricas relacionais e de irregularidade\n",
    "\n",
    "| Coluna | Tipo | Descri√ß√£o | Interpreta√ß√£o |\n",
    "|---------|------|------------|----------------|\n",
    "| `par_rareza` | Relacional | \\(1 / (1 + \\text{n√∫mero de ocorr√™ncias pr√©vias do par})\\). | Mede qu√£o incomum √© a rela√ß√£o; pr√≥ximo de 1 = primeira intera√ß√£o. |\n",
    "| `par_burstiness` | Temporal/Relacional | \\((\\sigma - \\mu) / (\\sigma + \\mu)\\) dos intervalos entre eventos do par. | Mede irregularidade temporal da rela√ß√£o; 1 = explosiva, 0 = aleat√≥ria, ‚àí1 = regular. |\n",
    "\n",
    "---\n",
    "\n",
    "##### Notas operacionais\n",
    "\n",
    "- Todas as colunas de janelas s√£o calculadas **causalmente** (somente com eventos passados e o atual).  \n",
    "- Os z-scores s√£o **robustos**, baseados em mediana e MAD, evitando distor√ß√£o por outliers.  \n",
    "- Os tempos s√£o expressos em **segundos**; normaliza√ß√µes adicionais (por dia/m√™s) podem ser feitas em fases posteriores.  \n",
    "- As janelas temporais atuais s√£o:  \n",
    "  **1 dia (curt√≠ssimo), 7 dias (curto), 30 dias (m√©dio), 120 dias (longo), 220 dias (longu√≠ssimo)**.  \n",
    "- Cada evento (linha) representa um **lan√ßamento cont√°bil individual**, associado a:  \n",
    "  `user_id`, `beneficiario_id`, `unidade_origem` e `valor_pago`.\n",
    "\n",
    "---\n",
    "\n",
    "##### Interpreta√ß√£o geral\n",
    "\n",
    "- **Contagens e taxas**: indicam **n√≠vel de atividade**.  \n",
    "- **Z-scores**: indicam **desvios comportamentais**.  \n",
    "- **Graus e egonet**: medem **posi√ß√£o e influ√™ncia na rede**.  \n",
    "- **Rareza e burstiness**: capturam **irregularidade e novidade das rela√ß√µes**.  \n",
    "- **Rec√™ncia (secs)**: quantifica **tempo de inatividade**.\n",
    "\n",
    "Essas vari√°veis comp√µem o vetor de atributos temporais que alimenta os embeddings do\n",
    "modelo **Temporal Graph Network (DECOI)**, permitindo identificar padr√µes an√¥malos\n",
    "tanto **em n√≠vel individual (usu√°rio)** quanto **organizacional (unidade)**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weI0fzvYrfKi"
   },
   "source": [
    "#### **Implementa√ß√£o matem√°tica.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 76
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1760064469752,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "gkZqfjOVpmYA",
    "outputId": "e77cfc9c-9369-48fa-eb2b-e3bc5ec764f0"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID010\n",
    "\n",
    "# janelas em segundos (mant√©m as 5 janelas)\n",
    "WINDOWS_SECONDS = {\n",
    "    \"curtissimo\":  1   * 24 * 3600,\n",
    "    \"curto\":       7   * 24 * 3600,\n",
    "    \"medio\":       30  * 24 * 3600,\n",
    "    \"longo\":       120 * 24 * 3600,\n",
    "    \"longuissimo\": 220 * 24 * 3600,\n",
    "}\n",
    "\n",
    "from collections import deque, defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _robust_z_series(s: pd.Series, eps: float = 1e-9) -> pd.Series:\n",
    "    med = s.median()\n",
    "    mad = (np.abs(s - med)).median()\n",
    "    return 0.6745 * (s - med) / (mad + eps)\n",
    "\n",
    "def _rolling_counts_global(times: pd.Series, wsec: int) -> pd.Series:\n",
    "    q = deque()\n",
    "    out = np.empty(len(times), dtype=np.int64)\n",
    "    for i, t in enumerate(times):\n",
    "        q.append(t)\n",
    "        while q and (t - q[0]).total_seconds() > wsec:\n",
    "            q.popleft()\n",
    "        out[i] = len(q)\n",
    "    return pd.Series(out, index=times.index)\n",
    "\n",
    "def _rolling_counts_by_entity(times: pd.Series, entity: pd.Series, wsec: int) -> pd.Series:\n",
    "    deques = defaultdict(deque)\n",
    "    out = np.empty(len(times), dtype=np.int64)\n",
    "    for i, (t, e) in enumerate(zip(times, entity)):\n",
    "        q = deques[e]\n",
    "        q.append(t)\n",
    "        while q and (t - q[0]).total_seconds() > wsec:\n",
    "            q.popleft()\n",
    "        out[i] = len(q)\n",
    "    return pd.Series(out, index=times.index)\n",
    "\n",
    "# janelas multiescala com row_id preservado\n",
    "def build_rolling_features_entities(\n",
    "    df: pd.DataFrame,\n",
    "    time_col: str = \"timestamp\",\n",
    "    user_col: str = \"user_id\",\n",
    "    unit_col: str = \"unidade_origem\",\n",
    "    row_id_col: str = \"row_id\",\n",
    "    windows_seconds: dict = WINDOWS_SECONDS,\n",
    ") -> pd.DataFrame:\n",
    "    work = df[[row_id_col, time_col, user_col, unit_col]].copy()\n",
    "    work[time_col] = pd.to_datetime(work[time_col], utc=False, errors=\"coerce\")\n",
    "    work = work.sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "    times = work[time_col]\n",
    "    users = work[user_col]\n",
    "    units = work[unit_col]\n",
    "\n",
    "    feat_blocks = []\n",
    "    for win, wsec in windows_seconds.items():\n",
    "        g_cnt = _rolling_counts_global(times, wsec)\n",
    "        g_perday = g_cnt / max(wsec / 86400.0, 1e-9)\n",
    "        g_z = _robust_z_series(g_cnt)\n",
    "\n",
    "        u_cnt = _rolling_counts_by_entity(times, users, wsec)\n",
    "        u_z = u_cnt.groupby(users, sort=False).transform(_robust_z_series)\n",
    "\n",
    "        n_cnt = _rolling_counts_by_entity(times, units, wsec)\n",
    "        n_z = n_cnt.groupby(units, sort=False).transform(_robust_z_series)\n",
    "\n",
    "        block = pd.DataFrame({\n",
    "            f\"{win}_cnt\":     g_cnt.values,\n",
    "            f\"{win}_perday\":  g_perday.values,\n",
    "            f\"{win}_zglob\":   g_z.values,\n",
    "            f\"{win}_ucnt\":    u_cnt.values,\n",
    "            f\"{win}_zuser\":   u_z.values,\n",
    "            f\"{win}_ncnt\":    n_cnt.values,\n",
    "            f\"{win}_zunit\":   n_z.values,\n",
    "        }, index=work.index)\n",
    "        feat_blocks.append(block)\n",
    "\n",
    "    feats = pd.concat(feat_blocks, axis=1)\n",
    "    feats.insert(0, row_id_col, work[row_id_col].values)\n",
    "\n",
    "    prefix = [row_id_col]\n",
    "    others = sorted([c for c in feats.columns if c not in prefix])\n",
    "    return feats[prefix + others]\n",
    "\n",
    "# Mensagem adicional isolada (Skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\"><b>ü§ñ Skynet</b>: Informa√ß√µes de comportamento processadas.</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIXtGuOLnkhm"
   },
   "source": [
    "###**Etapa 10:** Configura√ß√£o dos modelos matem√°ticos\n",
    "\n",
    "**Modelos utilizados**\n",
    "\n",
    "**Isolation Forest (IF)**\n",
    "\n",
    "**Mec√¢nica:** cria v√°rias √°rvores de decis√£o que ‚Äúisolam‚Äù pontos. Quanto menos cortes s√£o necess√°rios para separar uma observa√ß√£o, mais an√¥mala ela √©.\n",
    "\n",
    "**Motivo da escolha:** bom para detectar transa√ß√µes raras ou de valor at√≠pico em grandes volumes de dados.\n",
    "\n",
    "**No exemplo:** um pagamento muito acima da m√©dia do usu√°rio pode ser isolado rapidamente ‚Üí sinal de anomalia.\n",
    "\n",
    "---\n",
    "**Local Outlier Factor (LOF)**\n",
    "\n",
    "**Mec√¢nica:** compara a densidade local de vizinhos. Pontos em regi√µes menos densas s√£o marcados como outliers.\n",
    "\n",
    "**Motivo da escolha:** captura anomalias contextuais, ou seja, pagamentos que parecem ‚Äúnormais‚Äù globalmente, mas destoam do comportamento em seu grupo.\n",
    "\n",
    "**No exemplo:** se uma conta sempre paga fornecedores fixos e de repente paga um novo benefici√°rio, o LOF detecta que o padr√£o local mudou.\n",
    "\n",
    "---\n",
    "\n",
    "**One-Class SVM (opcional)**\n",
    "\n",
    "**Mec√¢nica:** aprende a fronteira do espa√ßo ‚Äúnormal‚Äù e marca pontos fora dela como an√¥malos.\n",
    "\n",
    "**Motivo da escolha:** √∫til em cen√°rios onde se deseja maior controle da taxa de outliers (via par√¢metro nu).\n",
    "\n",
    "**No exemplo:** pode ajudar a identificar transfer√™ncias fora do perfil quando s√≥ h√° poucos hist√≥ricos para treinar.\n",
    "\n",
    "---\n",
    "\n",
    "‚öñÔ∏è **Regras adicionais**\n",
    "\n",
    "- Robust Z-score: avalia se o valor do pagamento √© distante da mediana hist√≥rica (robusto a outliers).\n",
    "\n",
    "- Rareza: se a rela√ß√£o pagador‚Üíbenefici√°rio √© pouco frequente, maior chance de anomalia.\n",
    "\n",
    "- Burstiness: mede explos√µes de atividade (ex.: v√°rios pagamentos em minutos, ap√≥s dias sem atividade).\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "üîÄ **Uso blended (ensemble por ranking)**\n",
    "\n",
    "- Cada modelo gera um score de anomalia.\n",
    "\n",
    "- Em vez de escolher um √∫nico, os scores s√£o convertidos em ranks e depois combinados (m√©dia).\n",
    "\n",
    "- Essa abordagem reduz o vi√©s de um modelo s√≥ e fortalece sinais consistentes.\n",
    "\n",
    "- O resultado √© um ensemble_score, cortado por percentil (ex.: 97,5%), para definir os eventos an√¥malos.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "üìå **Resumo para o exemplo de monitoramento de pagamentos:**\n",
    "\n",
    "O sistema combina tr√™s algoritmos n√£o supervisionados + features baseadas em regras para capturar tanto anomalias globais (Isolation Forest), quanto locais (LOF), quanto estruturais (SVM/regra). O blended via ranking (score conjunto)  garante robustez, evitando que um √∫nico modelo domine a decis√£o.\n",
    "\n",
    "---\n",
    "**IMPORTANTE:** Primeira linha deste c√≥digo configura o percentual para corte e identifica√ß√£o de anomalia.\n",
    "\n",
    "Isso significa que os modelos tentar√£o encontrar anomalias em um intervalo de confian√ßa de 97,5% (ATUAL). Quanto menor o percentual de confian√ßa, maior o n√∫mero de \"anomalias\" (candidatos) encontrados e, possivelmente, maior o n√∫mero de FALSO POSITIVOS. Quanto maior o percentual de confian√ßa, mais exigente √© o modelo para determinar se algo √© realmente fora do comum.\n",
    "\n",
    "---\n",
    "Implementar o intervalo de confian√ßa como uma configura√ß√£o (vari√°vel) no come√ßo do C√≥digo **TODO[005]** *prioridade m√©dia*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1760064475956,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "vfNRluMbxdPh"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID011\n",
    "\n",
    "# pr√©-requisitos esperados no ambiente (defina antes, como j√° fizemos nas etapas anteriores):\n",
    "# - WINDOWS_SECONDS (1, 7, 30, 120, 220 dias)\n",
    "# - build_rolling_features_entities(df, time_col, user_col, unit_col)\n",
    "# - egonet_density, burstiness\n",
    "# - seu dataframe df com colunas ['timestamp','user_id','unidade_origem','beneficiario_id','valor_pago']\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, deque\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# reduz ru√≠do de threads no colab (opcional)\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"VECLIB_MAXIMUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "\n",
    "def _require_columns(df, cols, ctx=\"df\"):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"{ctx}: faltam colunas: {missing}\")\n",
    "\n",
    "# raridade/burst por par devolvendo row_id\n",
    "def compute_pair_rarity_burst(\n",
    "    df,\n",
    "    time_col=\"timestamp\",\n",
    "    user_col=\"user_id\",\n",
    "    benef_col=\"beneficiario_id\",\n",
    "    row_id_col=\"row_id\",\n",
    "    window_seconds=120*24*3600\n",
    "):\n",
    "    w = df[[row_id_col, time_col, user_col, benef_col]].copy()\n",
    "    w[time_col] = pd.to_datetime(w[time_col], utc=False, errors=\"coerce\")\n",
    "    w = w.sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "    dq = defaultdict(deque)\n",
    "    rare = np.empty(len(w), dtype=float)\n",
    "    burst = np.empty(len(w), dtype=float)\n",
    "\n",
    "    for i, (t, u, b) in enumerate(zip(w[time_col], w[user_col], w[benef_col])):\n",
    "        key = (u, b)\n",
    "        q = dq[key]\n",
    "        while q and (t - q[0]).total_seconds() > window_seconds:\n",
    "            q.popleft()\n",
    "\n",
    "        rare[i] = 1.0 / (1.0 + len(q))\n",
    "\n",
    "        seq = list(q) + [t]\n",
    "        if len(seq) >= 2:\n",
    "            inter = np.diff([s.timestamp() for s in seq])\n",
    "            mu = inter.mean(); sigma = inter.std()\n",
    "            burst[i] = 0.0 if (sigma + mu) == 0 else (sigma - mu) / (sigma + mu)\n",
    "        else:\n",
    "            burst[i] = 0.0\n",
    "\n",
    "        q.append(t)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        row_id_col: w[row_id_col].values,\n",
    "        \"par_rareza\": rare,\n",
    "        \"par_burstiness\": burst\n",
    "    })\n",
    "\n",
    "# montagem final das features com merge por row_id\n",
    "def build_feat_df(\n",
    "    df: pd.DataFrame,\n",
    "    time_col: str = \"timestamp\",\n",
    "    user_col: str = \"user_id\",\n",
    "    unit_col: str = \"unidade_origem\",\n",
    "    benef_col: str = \"beneficiario_id\",\n",
    "    include_pair_counts: bool = False,\n",
    "    include_value_stats: bool = False,\n",
    "    verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    assert \"row_id\" in df.columns, \"Crie df['row_id'] na ID007 antes de prosseguir.\"\n",
    "\n",
    "    if verbose: print(\"build_feat_df: janelas multiescala‚Ä¶\")\n",
    "    df_roll = build_rolling_features_entities(\n",
    "        df, time_col=time_col, user_col=user_col, unit_col=unit_col, row_id_col=\"row_id\"\n",
    "    )\n",
    "\n",
    "    if verbose: print(\"build_feat_df: merge base + janelas por row_id‚Ä¶\")\n",
    "    feat_df = df.merge(df_roll, on=\"row_id\", how=\"left\", sort=False)\n",
    "    assert len(feat_df) == len(df), \"Merge com df_roll alterou o n√∫mero de linhas.\"\n",
    "\n",
    "    if verbose: print(\"build_feat_df: raridade/burst por par‚Ä¶\")\n",
    "    df_pairrb = compute_pair_rarity_burst(\n",
    "        df, time_col=time_col, user_col=user_col, benef_col=benef_col, row_id_col=\"row_id\"\n",
    "    )\n",
    "    feat_df = feat_df.merge(df_pairrb, on=\"row_id\", how=\"left\", sort=False)\n",
    "    assert len(feat_df) == len(df), \"Merge com df_pairrb alterou o n√∫mero de linhas.\"\n",
    "\n",
    "    if include_pair_counts:\n",
    "        raise NotImplementedError(\"Ative somente se a fun√ß√£o de *_pcnt tamb√©m devolver row_id.\")\n",
    "\n",
    "    if include_value_stats:\n",
    "        raise NotImplementedError(\"Ative somente se a fun√ß√£o de valor 30d tamb√©m devolver row_id.\")\n",
    "\n",
    "    feat_df = feat_df.sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"build_feat_df: ok. shape:\", feat_df.shape)\n",
    "        print(\"sanity:\", len(feat_df), \"==\", len(df), \"‚Üí\", len(feat_df) == len(df))\n",
    "    return feat_df\n",
    "\n",
    "def detect_anomalies(\n",
    "    feat_df: pd.DataFrame,\n",
    "    use_ocsvm: bool = False,\n",
    "    anomaly_percentile: float = 97.5,\n",
    "    windows_for_multi = (\"curtissimo\", \"curto\", \"medio\"),\n",
    "    seed: int = 42,\n",
    "    verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    if verbose: print(\"detect_anomalies: iniciando sele√ß√£o de features...\")\n",
    "    base_feats = [\n",
    "        \"valor_pago\",\n",
    "        \"secs_desde_ult_trans_user\", \"secs_desde_ult_trans_par\",\n",
    "        \"grau_out_user\", \"grau_in_benef\", \"grau_total_user\", \"grau_total_benef\",\n",
    "        \"egonet_density_user\",\n",
    "        \"par_rareza\", \"par_burstiness\"\n",
    "    ]\n",
    "    multi_feats = []\n",
    "    for w in windows_for_multi:\n",
    "        for col in (f\"{w}_ucnt\", f\"{w}_ncnt\", f\"{w}_perday\", f\"{w}_zglob\", f\"{w}_zuser\", f\"{w}_zunit\"):\n",
    "            if col in feat_df.columns:\n",
    "                multi_feats.append(col)\n",
    "    optional_pair_feats = [c for c in feat_df.columns if c.endswith(\"_pcnt\")]\n",
    "    optional_value_feats = [c for c in [\"user_valor_median_30d\",\"par_valor_median_30d\",\"user_robust_z\",\"par_robust_z\"] if c in feat_df.columns]\n",
    "    model_features = [c for c in (base_feats + multi_feats + optional_pair_feats + optional_value_feats) if c in feat_df.columns]\n",
    "    if not model_features:\n",
    "        raise RuntimeError(\"nenhuma feature dispon√≠vel para detec√ß√£o. verifique a montagem do feat_df.\")\n",
    "    if verbose:\n",
    "        print(\"detect_anomalies: total de features usadas:\", len(model_features))\n",
    "\n",
    "    X = feat_df[model_features].fillna(0.0).replace([np.inf, -np.inf], 0.0).values\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(X)\n",
    "\n",
    "    if verbose: print(\"detect_anomalies: isolation forest...\")\n",
    "    iso = IsolationForest(n_estimators=300, max_samples='auto', contamination='auto', random_state=seed, n_jobs=-1)\n",
    "    iso.fit(Xs)\n",
    "    iso_score = -iso.score_samples(Xs).astype(float)\n",
    "\n",
    "    if verbose: print(\"detect_anomalies: lof...\")\n",
    "    lof = LocalOutlierFactor(n_neighbors=35, contamination='auto', novelty=False, n_jobs=-1)\n",
    "    _ = lof.fit_predict(Xs)\n",
    "    lof_cont = -lof.negative_outlier_factor_.astype(float)\n",
    "\n",
    "    if use_ocsvm:\n",
    "        if verbose: print(\"detect_anomalies: one-class svm...\")\n",
    "        ocs = OneClassSVM(gamma='scale', nu=0.01)\n",
    "        ocs.fit(Xs)\n",
    "        ocs_score = -ocs.decision_function(Xs).ravel().astype(float)\n",
    "    else:\n",
    "        ocs_score = np.zeros(len(Xs), dtype=float)\n",
    "\n",
    "    if \"curto_zuser\" in feat_df.columns:\n",
    "        rz_user_rule = np.clip(feat_df[\"curto_zuser\"].values, 0, None).astype(float)\n",
    "    elif \"curtissimo_zuser\" in feat_df.columns:\n",
    "        rz_user_rule = np.clip(feat_df[\"curtissimo_zuser\"].values, 0, None).astype(float)\n",
    "    else:\n",
    "        rz_user_rule = np.zeros(len(feat_df), dtype=float)\n",
    "\n",
    "    if \"curto_zunit\" in feat_df.columns:\n",
    "        rz_unit_rule = np.clip(feat_df[\"curto_zunit\"].values, 0, None).astype(float)\n",
    "    elif \"curtissimo_zunit\" in feat_df.columns:\n",
    "        rz_unit_rule = np.clip(feat_df[\"curtissimo_zunit\"].values, 0, None).astype(float)\n",
    "    else:\n",
    "        rz_unit_rule = np.zeros(len(feat_df), dtype=float)\n",
    "\n",
    "    if \"user_robust_z\" in feat_df.columns:\n",
    "        rz_user_val = np.clip(np.nan_to_num(feat_df[\"user_robust_z\"].values, nan=0.0), 0, None).astype(float)\n",
    "    else:\n",
    "        rz_user_val = np.zeros(len(feat_df), dtype=float)\n",
    "\n",
    "    if \"par_robust_z\" in feat_df.columns:\n",
    "        rz_par_val = np.clip(np.nan_to_num(feat_df[\"par_robust_z\"].values, nan=0.0), 0, None).astype(float)\n",
    "    else:\n",
    "        rz_par_val = np.zeros(len(feat_df), dtype=float)\n",
    "\n",
    "    rare_score = feat_df[\"par_rareza\"].values.astype(float)\n",
    "    burst_score = np.clip(feat_df[\"par_burstiness\"].values, 0, None).astype(float)\n",
    "\n",
    "    scores = pd.DataFrame({\n",
    "        \"iso\": iso_score,\n",
    "        \"lof\": lof_cont,\n",
    "        \"ocsvm\": ocs_score,\n",
    "        \"rz_user_cnt\": rz_user_rule,\n",
    "        \"rz_unit_cnt\": rz_unit_rule,\n",
    "        \"rz_user_val\": rz_user_val,\n",
    "        \"rz_par_val\": rz_par_val,\n",
    "        \"rare\": rare_score,\n",
    "        \"burst\": burst_score\n",
    "    }, index=feat_df.index)\n",
    "\n",
    "    ranks = scores.rank(method=\"average\", ascending=True)\n",
    "    ensemble_rank = ranks.mean(axis=1)\n",
    "    ensemble_score = (ensemble_rank - ensemble_rank.min()) / (ensemble_rank.max() - ensemble_rank.min() + 1e-9)\n",
    "\n",
    "    out = feat_df.copy()\n",
    "    out = pd.concat([out, scores.add_prefix(\"score_\")], axis=1)\n",
    "    out[\"ensemble_rank\"] = ensemble_rank\n",
    "    out[\"ensemble_score\"] = ensemble_score\n",
    "\n",
    "    threshold = np.percentile(ensemble_score, anomaly_percentile)\n",
    "    out[\"is_anomaly\"] = (out[\"ensemble_score\"] >= threshold).astype(int)\n",
    "\n",
    "    print(f\"corte (percentil {anomaly_percentile}%): {threshold:.4f}\")\n",
    "    print(\"total anomalias:\", int(out[\"is_anomaly\"].sum()), \"de\", len(out))\n",
    "\n",
    "    # skynet\n",
    "    num_anomalies = int(out[\"is_anomaly\"].sum())\n",
    "    display(HTML(f'<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">ü§ñ Skynet: Identificadas {num_anomalies} anomalias.</div>'))\n",
    "\n",
    "    return out\n",
    "\n",
    "def run_full_detection(\n",
    "    df,\n",
    "    include_pair_counts=False,\n",
    "    include_value_stats=False,\n",
    "    use_ocsvm=False,\n",
    "    anomaly_percentile=97.5,\n",
    "    windows_for_multi=(\"curtissimo\",\"curto\",\"medio\"),\n",
    "    seed=42,\n",
    "    verbose=True\n",
    "):\n",
    "    if verbose: print(\"run_full_detection: montando feat_df...\")\n",
    "    feat_df = build_feat_df(\n",
    "        df,\n",
    "        time_col=\"timestamp\",\n",
    "        user_col=\"user_id\",\n",
    "        unit_col=\"unidade_origem\",\n",
    "        benef_col=\"beneficiario_id\",\n",
    "        include_pair_counts=include_pair_counts,\n",
    "        include_value_stats=include_value_stats,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    if verbose: print(\"run_full_detection: detectando anomalias...\")\n",
    "    feat_df = detect_anomalies(\n",
    "        feat_df,\n",
    "        use_ocsvm=use_ocsvm,\n",
    "        anomaly_percentile=anomaly_percentile,\n",
    "        windows_for_multi=windows_for_multi,\n",
    "        seed=seed,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    print(\"execu√ß√£o conclu√≠da.\")\n",
    "    return feat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "executionInfo": {
     "elapsed": 38677,
     "status": "ok",
     "timestamp": 1760064515784,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "7H8qcr1Xxe68",
    "outputId": "802c8540-ee99-428d-f379-824c9aacfffb"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID012 ‚Äî execu√ß√£o\n",
    "feat_df = build_feat_df(\n",
    "    df,\n",
    "    time_col=\"timestamp\",\n",
    "    user_col=\"user_id\",\n",
    "    unit_col=\"unidade_origem\",\n",
    "    benef_col=\"beneficiario_id\",\n",
    "    include_pair_counts=False,\n",
    "    include_value_stats=False,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "feat_df = detect_anomalies(\n",
    "    feat_df,\n",
    "    use_ocsvm=False,\n",
    "    anomaly_percentile=97.5,\n",
    "    windows_for_multi=(\"curtissimo\",\"curto\",\"medio\"),\n",
    "    seed=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"linhas no input:\", len(df), \"linhas no feat_df:\", len(feat_df))\n",
    "assert len(feat_df) == len(df), \"Cardinalidade alterada ‚Äî verifique merges por row_id.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3VSSvDK45jQ"
   },
   "source": [
    "###**Etapa 11:** Salva arquivo com a an√°lise realizada.\n",
    "\n",
    "S√£o criadas subpastas para cada data/hora de execu√ß√£o do c√≥digo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 105,
     "status": "ok",
     "timestamp": 1760064665596,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "U-OPMyv_B2mY"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID013 ‚Äî salvar resultados e vis√µes operacionais (alinhado ao RUN_DIR e fuso S√£o Paulo)\n",
    "import os, json, datetime as dt\n",
    "from zoneinfo import ZoneInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# base padr√£o do projeto (usada apenas se RUN_DIR n√£o existir)\n",
    "OUTPUT_DIR_DEFAULT = \"/content/drive/MyDrive/Notebooks/temporal-graph-network/output\"\n",
    "SAO_TZ = None\n",
    "try:\n",
    "    SAO_TZ = ZoneInfo(\"America/Sao_Paulo\")\n",
    "except Exception:\n",
    "    SAO_TZ = None  # fallback sem timezone expl√≠cito\n",
    "\n",
    "def _cols_if_exist(df, cols):\n",
    "    return [c for c in cols if c in df.columns]\n",
    "\n",
    "def _topN_por_grupo(df, group_col, sort_col=\"ensemble_score\", n_top=50):\n",
    "    if group_col not in df.columns:\n",
    "        return pd.DataFrame(columns=[group_col, sort_col])\n",
    "    return (\n",
    "        df.sort_values(sort_col, ascending=False)\n",
    "          .groupby(group_col, group_keys=False)\n",
    "          .head(n_top)\n",
    "    )\n",
    "\n",
    "def _serie_diaria(df, timestamp_col=\"timestamp\", score_col=\"ensemble_score\", flag_col=\"is_anomaly\"):\n",
    "    if timestamp_col not in df.columns:\n",
    "        return pd.DataFrame(columns=[\"data\",\"anomalias\",\"avg_ensemble_score\"])\n",
    "    w = df[[timestamp_col, score_col] + ([flag_col] if flag_col in df.columns else [])].copy()\n",
    "    w[\"data\"] = pd.to_datetime(w[timestamp_col], errors=\"coerce\").dt.date\n",
    "    grp = w.groupby(\"data\", dropna=True)\n",
    "    out = pd.DataFrame({\n",
    "        \"anomalias\": grp[flag_col].sum() if flag_col in w.columns else grp[score_col].size(),\n",
    "        \"avg_ensemble_score\": grp[score_col].mean()\n",
    "    }).reset_index()\n",
    "    return out\n",
    "\n",
    "def _anomalias_por_unidade_dia(df, timestamp_col=\"timestamp\", unidade_col=\"unidade_origem\",\n",
    "                               score_col=\"ensemble_score\", flag_col=\"is_anomaly\"):\n",
    "    if timestamp_col not in df.columns or unidade_col not in df.columns:\n",
    "        return pd.DataFrame(columns=[unidade_col, \"data\", \"anomalias\", \"avg_ensemble_score\"])\n",
    "    w = df[[timestamp_col, unidade_col, score_col] + ([flag_col] if flag_col in df.columns else [])].copy()\n",
    "    w[\"data\"] = pd.to_datetime(w[timestamp_col], errors=\"coerce\").dt.date\n",
    "    grp = w.groupby([unidade_col, \"data\"], dropna=True)\n",
    "    out = pd.DataFrame({\n",
    "        \"anomalias\": grp[flag_col].sum() if flag_col in w.columns else grp[score_col].size(),\n",
    "        \"avg_ensemble_score\": grp[score_col].mean()\n",
    "    }).reset_index()\n",
    "    return out\n",
    "\n",
    "def _top_features_explicabilidade(df, score_prefix=\"score_\", flag_col=\"is_anomaly\"):\n",
    "    score_cols = [c for c in df.columns if c.startswith(score_prefix)]\n",
    "    if not score_cols:\n",
    "        return pd.DataFrame(columns=[\"feature\",\"media_anomalia\",\"media_geral\",\"impacto_relativo\"])\n",
    "    df_anom = df[df[flag_col] == 1] if flag_col in df.columns else df\n",
    "    media_anom = df_anom[score_cols].mean().rename(\"media_anomalia\")\n",
    "    media_geral = df[score_cols].mean().rename(\"media_geral\")\n",
    "    explic = pd.concat([media_anom, media_geral], axis=1)\n",
    "    explic[\"impacto_relativo\"] = explic[\"media_anomalia\"] / (explic[\"media_geral\"] + 1e-9)\n",
    "    explic = explic.reset_index().rename(columns={\"index\": \"feature\"})\n",
    "    explic = explic.sort_values(\"impacto_relativo\", ascending=False)\n",
    "    return explic\n",
    "\n",
    "def _data_dictionary(df):\n",
    "    return pd.DataFrame({\n",
    "        \"column\": df.columns,\n",
    "        \"dtype\": [str(dt) for dt in df.dtypes]\n",
    "    })\n",
    "\n",
    "def save_results_id013(\n",
    "    df_input: pd.DataFrame,\n",
    "    feat_df: pd.DataFrame,\n",
    "    output_dir: str = OUTPUT_DIR_DEFAULT,\n",
    "    anomaly_percentile: float = 97.5,\n",
    "    top_k_anomalies: int = 1000,\n",
    "    top_n_por_grupo: int = 50\n",
    "):\n",
    "    # sanity checks\n",
    "    if feat_df is None or len(feat_df) == 0:\n",
    "        raise RuntimeError(\"feat_df est√° vazio ou n√£o foi gerado.\")\n",
    "    if df_input is None or len(df_input) == 0:\n",
    "        raise RuntimeError(\"df de entrada est√° vazio ou n√£o foi carregado.\")\n",
    "\n",
    "    # determina a pasta de execu√ß√£o: preferir RUN_DIR; caso n√£o exista, criar TGN_... em output_dir\n",
    "    if 'RUN_DIR' in globals() and RUN_DIR is not None and os.path.isdir(str(RUN_DIR)):\n",
    "        outdir = str(RUN_DIR)\n",
    "        print(\"Usando RUN_DIR existente:\", outdir)\n",
    "    else:\n",
    "        ts = (dt.datetime.now(SAO_TZ).strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "              if SAO_TZ else dt.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "        outdir = os.path.join(output_dir, f\"TGN_{ts}\")\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "        print(\"RUN_DIR n√£o definido; criada pasta de execu√ß√£o:\", outdir)\n",
    "\n",
    "    # ordena por score\n",
    "    if \"ensemble_score\" not in feat_df.columns:\n",
    "        raise KeyError(\"coluna 'ensemble_score' n√£o encontrada em feat_df.\")\n",
    "    df_sorted = feat_df.sort_values(\"ensemble_score\", ascending=False)\n",
    "\n",
    "    # threshold de documenta√ß√£o\n",
    "    threshold = float(np.percentile(df_sorted[\"ensemble_score\"].values, anomaly_percentile))\n",
    "\n",
    "    # colunas recomendadas\n",
    "    key_cols   = _cols_if_exist(feat_df, [\"row_id\",\"trans_id\",\"timestamp\",\"user_id\",\"beneficiario_id\",\"unidade_origem\",\"valor_pago\"])\n",
    "    flag_cols  = _cols_if_exist(feat_df, [\"ensemble_score\",\"ensemble_rank\",\"is_anomaly\"])\n",
    "    score_cols = [c for c in feat_df.columns if c.startswith(\"score_\")]\n",
    "    extra_cols = _cols_if_exist(feat_df, [\n",
    "        \"secs_desde_ult_trans_user\",\"secs_desde_ult_trans_par\",\n",
    "        \"par_rareza\",\"par_burstiness\",\n",
    "        \"grau_out_user\",\"grau_in_benef\",\"grau_total_user\",\"grau_total_benef\",\"egonet_density_user\"\n",
    "    ])\n",
    "    export_cols = key_cols + flag_cols + score_cols + extra_cols\n",
    "    export_cols += [c for c in feat_df.columns if c not in export_cols]  # inclui o resto ao final\n",
    "\n",
    "    # caminhos de sa√≠da (nomes padronizados)\n",
    "    path_csv_full   = os.path.join(outdir, \"full.csv\")\n",
    "    path_parq_full  = os.path.join(outdir, \"full.parquet\")\n",
    "    path_csv_anom   = os.path.join(outdir, \"anomalies.csv\")\n",
    "    path_manifest   = os.path.join(outdir, \"manifest.json\")\n",
    "\n",
    "    # salva full CSV\n",
    "    df_sorted.to_csv(path_csv_full, index=False, columns=export_cols)\n",
    "\n",
    "    # salva full Parquet\n",
    "    used_parquet = True\n",
    "    try:\n",
    "        df_sorted.to_parquet(path_parq_full, index=False)\n",
    "    except Exception as e:\n",
    "        print(\"aviso: to_parquet falhou; manteremos apenas CSV. erro:\", repr(e))\n",
    "        used_parquet = False\n",
    "\n",
    "    # salva anomalias (ou top-k quando flag ausente)\n",
    "    if \"is_anomaly\" in df_sorted.columns:\n",
    "        df_anom = df_sorted[df_sorted[\"is_anomaly\"] == 1].copy()\n",
    "    else:\n",
    "        df_anom = df_sorted.head(top_k_anomalies).copy()\n",
    "    df_anom.to_csv(path_csv_anom, index=False, columns=export_cols)\n",
    "\n",
    "    # vis√µes adicionais\n",
    "    views = {}\n",
    "\n",
    "    # top-N por usu√°rio\n",
    "    top_user = _topN_por_grupo(df_anom if len(df_anom) else df_sorted, \"user_id\", \"ensemble_score\", n_top=top_n_por_grupo)\n",
    "    path_top_user = os.path.join(outdir, f\"top{top_n_por_grupo}_por_usuario.csv\")\n",
    "    top_user.to_csv(path_top_user, index=False)\n",
    "    views[\"top_user\"] = path_top_user\n",
    "\n",
    "    # top-N por unidade\n",
    "    top_unit = _topN_por_grupo(df_anom if len(df_anom) else df_sorted, \"unidade_origem\", \"ensemble_score\", n_top=top_n_por_grupo)\n",
    "    path_top_unit = os.path.join(outdir, f\"top{top_n_por_grupo}_por_unidade.csv\")\n",
    "    top_unit.to_csv(path_top_unit, index=False)\n",
    "    views[\"top_unidade\"] = path_top_unit\n",
    "\n",
    "    # top-N por benefici√°rio\n",
    "    top_benef = _topN_por_grupo(df_anom if len(df_anom) else df_sorted, \"beneficiario_id\", \"ensemble_score\", n_top=top_n_por_grupo)\n",
    "    path_top_benef = os.path.join(outdir, f\"top{top_n_por_grupo}_por_beneficiario.csv\")\n",
    "    top_benef.to_csv(path_top_benef, index=False)\n",
    "    views[\"top_beneficiario\"] = path_top_benef\n",
    "\n",
    "    # s√©rie temporal di√°ria\n",
    "    serie = _serie_diaria(df_sorted, timestamp_col=\"timestamp\", score_col=\"ensemble_score\", flag_col=\"is_anomaly\")\n",
    "    path_serie = os.path.join(outdir, \"anomalias_por_dia.csv\")\n",
    "    serie.to_csv(path_serie, index=False)\n",
    "    views[\"anomalias_por_dia\"] = path_serie\n",
    "\n",
    "    # anomalias por unidade √ó dia\n",
    "    serie_unid = _anomalias_por_unidade_dia(df_sorted, timestamp_col=\"timestamp\", unidade_col=\"unidade_origem\",\n",
    "                                            score_col=\"ensemble_score\", flag_col=\"is_anomaly\")\n",
    "    path_serie_unid = os.path.join(outdir, \"anomalias_por_unidade_dia.csv\")\n",
    "    serie_unid.to_csv(path_serie_unid, index=False)\n",
    "    views[\"anomalias_por_unidade_dia\"] = path_serie_unid\n",
    "\n",
    "    # top features (explicabilidade simples)\n",
    "    top_feat = _top_features_explicabilidade(df_sorted, score_prefix=\"score_\", flag_col=\"is_anomaly\")\n",
    "    path_top_feat = os.path.join(outdir, \"top_features.csv\")\n",
    "    top_feat.to_csv(path_top_feat, index=False)\n",
    "    views[\"top_features\"] = path_top_feat\n",
    "\n",
    "    # data dictionary\n",
    "    dict_df = _data_dictionary(df_sorted)\n",
    "    path_dict = os.path.join(outdir, \"data_dictionary.csv\")\n",
    "    dict_df.to_csv(path_dict, index=False)\n",
    "    views[\"data_dictionary\"] = path_dict\n",
    "\n",
    "    # manifest\n",
    "    paths = {\n",
    "        \"csv_full\": path_csv_full,\n",
    "        \"csv_anomalies\": path_csv_anom,\n",
    "        **views\n",
    "    }\n",
    "    if used_parquet:\n",
    "        paths[\"parquet_full\"] = path_parq_full\n",
    "\n",
    "    manifest = {\n",
    "        \"timestamp_execucao\": (dt.datetime.now(SAO_TZ).isoformat() if SAO_TZ else dt.datetime.now().isoformat()),\n",
    "        \"output_dir\": outdir,\n",
    "        \"input_rows\": int(len(df_input)),\n",
    "        \"output_rows\": int(len(feat_df)),\n",
    "        \"anomaly_percentile\": float(anomaly_percentile),\n",
    "        \"threshold_ensemble_score\": threshold,\n",
    "        \"paths\": paths,\n",
    "        \"columns\": {\n",
    "            \"keys\": key_cols,\n",
    "            \"flags\": flag_cols,\n",
    "            \"scores\": score_cols\n",
    "        }\n",
    "    }\n",
    "    with open(path_manifest, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(manifest, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"resultados salvos em:\", outdir)\n",
    "    print(\"arquivos gerados:\")\n",
    "    for k, v in paths.items():\n",
    "        print(\" -\", k, \":\", v)\n",
    "    print(\" - manifest:\", path_manifest)\n",
    "\n",
    "    return {\n",
    "        \"outdir\": outdir,\n",
    "        \"paths\": paths,\n",
    "        \"manifest\": path_manifest\n",
    "    }\n",
    "\n",
    "# exemplo de uso logo ap√≥s a etapa de detec√ß√£o:\n",
    "# save_info = save_results_id013(df, feat_df, output_dir=OUTPUT_DIR_DEFAULT, anomaly_percentile=97.5, top_k_anomalies=1000, top_n_por_grupo=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1760064770454,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "zVgl_SJdCQZx",
    "outputId": "33cdfd43-e89a-4cdf-8c90-5a71398aa9af"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID013-CONT1\n",
    "from pathlib import Path\n",
    "import os, json\n",
    "\n",
    "base = Path(\"/content/drive/MyDrive/Notebooks/temporal-graph-network/output\")\n",
    "curr = Path(str(RUN_DIR)) if 'RUN_DIR' in globals() else None\n",
    "print(\"RUN_DIR atual:\", curr)\n",
    "\n",
    "def list_dir(p):\n",
    "    if not p or not p.exists():\n",
    "        print(\"(!) pasta inexistente:\", p);\n",
    "        return\n",
    "    print(\"Conte√∫do de\", p, \":\")\n",
    "    for f in sorted(p.iterdir()):\n",
    "        print(\" -\", f.name)\n",
    "\n",
    "list_dir(curr)\n",
    "\n",
    "need = {\"full.parquet\",\"full.csv\",\"anomalies.csv\",\"manifest.json\"}\n",
    "have = {x.name for x in curr.iterdir()} if curr and curr.exists() else set()\n",
    "missing = sorted(need - have)\n",
    "print(\"Faltam no RUN_DIR atual:\", missing)\n",
    "\n",
    "# Procura a √∫ltima pasta que realmente tenha full/anomalies\n",
    "cands = []\n",
    "for d in sorted(base.iterdir(), key=os.path.getmtime, reverse=True):\n",
    "    if not d.is_dir():\n",
    "        continue\n",
    "    names = {x.name for x in d.iterdir()}\n",
    "    if (\"full.parquet\" in names or \"full.csv\" in names) and (\"anomalies.csv\" in names):\n",
    "        cands.append(d)\n",
    "        break\n",
    "\n",
    "if cands:\n",
    "    print(\"Pasta com dados prontos encontrada:\", cands[0])\n",
    "    list_dir(cands[0])\n",
    "else:\n",
    "    print(\"Nenhuma pasta anterior com full/anomalies encontrada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6842,
     "status": "ok",
     "timestamp": 1760064798680,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "TNJ5zBYeCVqs",
    "outputId": "66e89398-ffa0-4d55-d173-86e10c0f2c22"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID013-CONT2\n",
    "# garanta estes nomes:\n",
    "OUTPUT_DIR_DEFAULT = \"/content/drive/MyDrive/Notebooks/temporal-graph-network/output\"\n",
    "\n",
    "save_info = save_results_id013(\n",
    "    df_input=df,\n",
    "    feat_df=feat_df,\n",
    "    output_dir=OUTPUT_DIR_DEFAULT,\n",
    "    anomaly_percentile=97.5,\n",
    "    top_k_anomalies=1000,\n",
    "    top_n_por_grupo=50\n",
    ")\n",
    "print(\"Gravou em:\", save_info[\"outdir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLxsD81f96Vn"
   },
   "source": [
    "###**Etapa 12:** An√°lise Gr√°fica\n",
    "\n",
    "Distribui√ß√£o e Top-N (com corte por percentil e K sugerido por maior gap)\n",
    "\n",
    "Essa an√°lise gera dois gr√°ficos que ajudam a entender como os escores de anomalia (‚Äúensemble_score‚Äù) est√£o distribu√≠dos e quais transa√ß√µes s√£o mais suspeitas:\n",
    "\n",
    "---\n",
    "**Histograma ‚Äì Distribui√ß√£o do ensemble_score**\n",
    "\n",
    "Mostra a frequ√™ncia dos escores em toda a base.\n",
    "\n",
    "O que procurar:\n",
    "- A linha pontilhada indica o percentil de corte (ex.: Percentil 97,5). Deve ser verificado se ela cai na regi√£o da cauda, o que significa que s√≥ os casos mais extremos ser√£o analisados (evitando falsos positivos).\n",
    "- Se a maior parte dos casos est√° em valores baixos/m√©dios e existe uma cauda √† direita (valores muito altos), esses pontos de cauda s√£o os candidatos a anomalias.\n",
    "- Observar o valor correspondente ao percentil estabelecido para complementar a pr√≥xima an√°lise.\n",
    "---\n",
    "**Gr√°fico de linha ‚Äì Top N transa√ß√µes mais an√¥malas**\n",
    "\n",
    "Ordena os maiores escores (rank 1 = mais an√¥mala).\n",
    "\n",
    "O que procurar:\n",
    "- Grandes saltos (‚Äúgaps‚Äù) entre ranks consecutivos: indicam que as transa√ß√µes at√© o salto s√£o bem mais an√¥malas do que as demais ‚Äî s√£o as que merecem aten√ß√£o imediata.\n",
    "- Observar os valores de escore de anomalia encontrados nos N registros mais an√¥malos versus o valor correspondente ao percentil estabelecido. Quanto maior a diferen√ßa, mais an√¥malo.\n",
    "- Plat√¥ (curva que se estabiliza): mostra a partir de que ponto (K) os casos deixam de ser t√£o excepcionais. Observar o K sugerido pelo maior gap, ele indica quantos casos devem ser priorizados na revis√£o manual. At√© K registros s√£o candidatos muito fortes para anomalias e exigem revis√£o manual.\n",
    "---\n",
    "\n",
    "Em resumo: os gr√°ficos servem para decidir onde cortar e quais transa√ß√µes revisar primeiro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 12764,
     "status": "ok",
     "timestamp": 1760065070681,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "Gcc7_t-HDV6h",
    "outputId": "d75de98b-a934-42a3-c665-50f135218f36"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID014 ‚Äî an√°lise estat√≠stica e visual das anomalias (salva PNGs em FIG_DIR)\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from google.colab import drive\n",
    "\n",
    "# ===== CONFIGURA√á√ïES =====\n",
    "OUTPUT_DIR_BASE = \"/content/drive/MyDrive/Notebooks/temporal-graph-network/output\"\n",
    "SAVE_PLOTS = True  # defina False se n√£o quiser salvar PNGs\n",
    "\n",
    "# estilo b√°sico\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "plt.rcParams[\"font.size\"] = 11\n",
    "\n",
    "# ===== DRIVE E BASE =====\n",
    "if not os.path.ismount(\"/content/drive\"):\n",
    "    print(\"Montando Google Drive...\")\n",
    "    drive.mount(\"/content/drive\")\n",
    "else:\n",
    "    print(\"Google Drive j√° montado.\")\n",
    "os.makedirs(OUTPUT_DIR_BASE, exist_ok=True)\n",
    "print(\"Base:\", OUTPUT_DIR_BASE)\n",
    "\n",
    "# ===== HELPERS =====\n",
    "def _ensure_fig_dir(latest_dir):\n",
    "    # usa FIG_DIR do RUN_DIR se dispon√≠vel; sen√£o cria fallback\n",
    "    fig_dir = os.path.join(latest_dir, \"figuras\")\n",
    "    os.makedirs(fig_dir, exist_ok=True)\n",
    "    return fig_dir\n",
    "\n",
    "_fig_counter = {\"n\": 0}\n",
    "def _save_show(fig, fig_dir, name_hint):\n",
    "    if SAVE_PLOTS:\n",
    "        _fig_counter[\"n\"] += 1\n",
    "        fname = f\"{_fig_counter['n']:02d}_{name_hint}.png\".replace(\" \", \"_\")\n",
    "        path = os.path.join(fig_dir, fname)\n",
    "        fig.savefig(path, dpi=150, bbox_inches=\"tight\")\n",
    "        print(\"Figura salva:\", path)\n",
    "    plt.show()\n",
    "\n",
    "def _plot_hist(data, title, xlabel, ylabel=\"Frequ√™ncia\", bins=50, threshold=None, fig_dir=None):\n",
    "    fig = plt.figure()\n",
    "    plt.hist(data, bins=bins, alpha=0.7)\n",
    "    if threshold is not None:\n",
    "        plt.axvline(threshold, color=\"red\", linestyle=\"--\", label=f\"threshold = {threshold:.4f}\")\n",
    "        plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.tight_layout()\n",
    "    _save_show(fig, fig_dir, title)\n",
    "\n",
    "def _pick_run_dir():\n",
    "    # 1) se RUN_DIR existe e tem dados, usa\n",
    "    if \"RUN_DIR\" in globals():\n",
    "        rd = str(RUN_DIR)\n",
    "        if os.path.isdir(rd):\n",
    "            names = set(os.listdir(rd))\n",
    "            if ((\"full.parquet\" in names) or (\"full.csv\" in names)) and (\"anomalies.csv\" in names):\n",
    "                print(\"Usando RUN_DIR ativo:\", rd)\n",
    "                return rd\n",
    "            else:\n",
    "                print(\"RUN_DIR ativo n√£o tem full/anomalies ‚Äî procurando pasta anterior com dados...\")\n",
    "\n",
    "    # 2) varre /output e pega a mais recente com full/anomalies\n",
    "    subdirs = [\n",
    "        os.path.join(OUTPUT_DIR_BASE, d)\n",
    "        for d in os.listdir(OUTPUT_DIR_BASE)\n",
    "        if os.path.isdir(os.path.join(OUTPUT_DIR_BASE, d))\n",
    "    ]\n",
    "    if not subdirs:\n",
    "        raise RuntimeError(\"Nenhuma subpasta encontrada em /output.\")\n",
    "    subdirs.sort(key=os.path.getmtime, reverse=True)\n",
    "\n",
    "    for d in subdirs:\n",
    "        names = set(os.listdir(d))\n",
    "        if ((\"full.parquet\" in names) or (\"full.csv\" in names)) and (\"anomalies.csv\" in names):\n",
    "            print(\"Usando pasta com dados:\", d)\n",
    "            return d\n",
    "\n",
    "    # 3) se nada tiver dados, devolve a mais recente e deixamos falhar com diagn√≥stico\n",
    "    chosen = subdirs[0]\n",
    "    print(\"Aten√ß√£o: nenhuma pasta com dados completos; usando a mais recente:\", chosen)\n",
    "    return chosen\n",
    "\n",
    "def _load_paths(latest_dir):\n",
    "    manifest_files = [f for f in os.listdir(latest_dir) if f.endswith(\"manifest.json\")]\n",
    "    full_path = None\n",
    "    anom_path = None\n",
    "    threshold = None\n",
    "\n",
    "    if manifest_files:\n",
    "        mpath = os.path.join(latest_dir, manifest_files[0])\n",
    "        try:\n",
    "            with open(mpath, \"r\", encoding=\"utf-8\") as f:\n",
    "                m = json.load(f)\n",
    "            print(\"Manifest carregado:\", mpath)\n",
    "            threshold = m.get(\"threshold_ensemble_score\") or m.get(\"threshold\", None)\n",
    "            paths = m.get(\"paths\", {})\n",
    "            cand_full = [paths.get(k) for k in [\"parquet_full\", \"csv_full\"] if paths.get(k)]\n",
    "            cand_anom = [paths.get(k) for k in [\"csv_anomalies\"] if paths.get(k)]\n",
    "            if cand_full:\n",
    "                full_path = cand_full[0]\n",
    "                if not os.path.isabs(full_path):\n",
    "                    full_path = os.path.join(latest_dir, os.path.basename(full_path))\n",
    "            if cand_anom:\n",
    "                anom_path = cand_anom[0]\n",
    "                if not os.path.isabs(anom_path):\n",
    "                    anom_path = os.path.join(latest_dir, os.path.basename(anom_path))\n",
    "        except Exception as e:\n",
    "            print(\"Aviso: falha ao ler manifest:\", repr(e))\n",
    "\n",
    "    if not full_path:\n",
    "        cands = [f for f in os.listdir(latest_dir) if f.endswith(\"_full.parquet\")]\n",
    "        if not cands and \"full.parquet\" in os.listdir(latest_dir):\n",
    "            cands = [\"full.parquet\"]\n",
    "        if not cands:\n",
    "            cands = [f for f in os.listdir(latest_dir) if f.lower().endswith(\".parquet\")]\n",
    "        if cands:\n",
    "            full_path = os.path.join(latest_dir, cands[0])\n",
    "\n",
    "    if not anom_path:\n",
    "        cands = [f for f in os.listdir(latest_dir) if f.endswith(\"_anomalies.csv\")]\n",
    "        if not cands and \"anomalies.csv\" in os.listdir(latest_dir):\n",
    "            cands = [\"anomalies.csv\"]\n",
    "        if not cands:\n",
    "            cands = [f for f in os.listdir(latest_dir)\n",
    "                     if f.lower().endswith(\".csv\") and \"anom\" in f.lower()]\n",
    "        if cands:\n",
    "            anom_path = os.path.join(latest_dir, cands[0])\n",
    "\n",
    "    if not full_path or not anom_path:\n",
    "        print(\"\\nConte√∫do da pasta para diagn√≥stico:\")\n",
    "        for f in sorted(os.listdir(latest_dir)):\n",
    "            print(\" -\", f)\n",
    "\n",
    "    if not full_path:\n",
    "        raise FileNotFoundError(\"N√£o encontrei dataset 'full' (parquet/csv) em \" + latest_dir)\n",
    "    if not anom_path:\n",
    "        raise FileNotFoundError(\"N√£o encontrei arquivo de 'anomalias' (csv) em \" + latest_dir)\n",
    "\n",
    "    return full_path, anom_path, threshold\n",
    "\n",
    "# ===== EXECU√á√ÉO =====\n",
    "latest_dir = _pick_run_dir()\n",
    "fig_dir = _ensure_fig_dir(latest_dir)\n",
    "full_path, anom_path, threshold = _load_paths(latest_dir)\n",
    "\n",
    "if full_path.endswith(\".parquet\"):\n",
    "    df_full = pd.read_parquet(full_path)\n",
    "else:\n",
    "    df_full = pd.read_csv(full_path)\n",
    "df_anom = pd.read_csv(anom_path)\n",
    "\n",
    "print(f\"linhas totais: {len(df_full):,} | anomalias: {len(df_anom):,}\")\n",
    "print(\"Arquivos usados:\\n - full:\", full_path, \"\\n - anomalies:\", anom_path, \"\\n - threshold:\", threshold)\n",
    "print(\"Figuras ser√£o salvas em:\", fig_dir) if SAVE_PLOTS else None\n",
    "\n",
    "# ===== ESTAT√çSTICA DESCRITIVA DO INPUT =====\n",
    "print(\"\\n[descri√ß√£o estat√≠stica de valor_pago]\")\n",
    "if \"valor_pago\" in df_full.columns:\n",
    "    desc = df_full[\"valor_pago\"].describe(percentiles=[.01,.05,.1,.25,.5,.75,.9,.95,.99])\n",
    "    print(desc)\n",
    "    _plot_hist(df_full[\"valor_pago\"], \"Distribui√ß√£o de Valor Pago\", \"Valor Pago (R$)\", bins=60, fig_dir=fig_dir)\n",
    "else:\n",
    "    print(\"coluna 'valor_pago' ausente no dataset.\")\n",
    "\n",
    "if \"timestamp\" in df_full.columns:\n",
    "    df_full[\"data\"] = pd.to_datetime(df_full[\"timestamp\"], errors=\"coerce\").dt.date\n",
    "    freq_por_dia = df_full[\"data\"].value_counts().sort_index()\n",
    "    fig = plt.figure()\n",
    "    plt.plot(freq_por_dia.index, freq_por_dia.values)\n",
    "    plt.title(\"Frequ√™ncia de Lan√ßamentos por Dia\")\n",
    "    plt.xlabel(\"Data\")\n",
    "    plt.ylabel(\"Quantidade de lan√ßamentos\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    _save_show(fig, fig_dir, \"Frequencia_por_dia\")\n",
    "\n",
    "# ===== DISTRIBUI√á√ÉO DO ENSEMBLE SCORE =====\n",
    "if \"ensemble_score\" in df_full.columns:\n",
    "    _plot_hist(df_full[\"ensemble_score\"], \"Distribui√ß√£o do Ensemble Score\", \"ensemble_score\", bins=50, threshold=threshold, fig_dir=fig_dir)\n",
    "\n",
    "    if \"is_anomaly\" in df_full.columns:\n",
    "        fig = plt.figure()\n",
    "        plt.boxplot(\n",
    "            [df_full.loc[df_full[\"is_anomaly\"] == 0, \"ensemble_score\"],\n",
    "             df_full.loc[df_full[\"is_anomaly\"] == 1, \"ensemble_score\"]],\n",
    "            labels=[\"Normal\", \"An√¥malo\"],\n",
    "            patch_artist=True,\n",
    "            boxprops=dict(facecolor=\"lightgray\", color=\"black\"),\n",
    "            medianprops=dict(color=\"red\")\n",
    "        )\n",
    "        plt.title(\"Boxplot do Ensemble Score (Normal vs An√¥malo)\")\n",
    "        plt.ylabel(\"ensemble_score\")\n",
    "        plt.tight_layout()\n",
    "        _save_show(fig, fig_dir, \"Boxplot_ensemble_score\")\n",
    "\n",
    "# ===== ANOMALIAS POR UNIDADE =====\n",
    "if \"unidade_origem\" in df_full.columns and \"is_anomaly\" in df_full.columns:\n",
    "    fig = plt.figure()\n",
    "    df_unit = df_full.groupby(\"unidade_origem\")[\"is_anomaly\"].sum().sort_values(ascending=False).head(20)\n",
    "    df_unit.plot(kind=\"bar\", color=\"firebrick\")\n",
    "    plt.title(\"Top 20 Unidades com Mais Anomalias\")\n",
    "    plt.ylabel(\"Qtde de Anomalias\")\n",
    "    plt.xlabel(\"Unidade Origem\")\n",
    "    plt.xticks(rotation=70)\n",
    "    plt.tight_layout()\n",
    "    _save_show(fig, fig_dir, \"Top20_unidades_anomalias\")\n",
    "\n",
    "# ===== ANOMALIAS POR USU√ÅRIO =====\n",
    "if \"user_id\" in df_full.columns and \"is_anomaly\" in df_full.columns:\n",
    "    fig = plt.figure()\n",
    "    df_user = df_full.groupby(\"user_id\")[\"is_anomaly\"].sum().sort_values(ascending=False).head(20)\n",
    "    df_user.plot(kind=\"bar\", color=\"darkslateblue\")\n",
    "    plt.title(\"Top 20 Usu√°rios com Mais Anomalias\")\n",
    "    plt.ylabel(\"Qtde de Anomalias\")\n",
    "    plt.xlabel(\"User ID\")\n",
    "    plt.xticks(rotation=70)\n",
    "    plt.tight_layout()\n",
    "    _save_show(fig, fig_dir, \"Top20_usuarios_anomalias\")\n",
    "\n",
    "# ===== S√âRIE TEMPORAL DE ANOMALIAS POR DIA =====\n",
    "cand = [f for f in os.listdir(latest_dir) if f.endswith(\"anomalias_por_dia.csv\")]\n",
    "if cand:\n",
    "    serie = pd.read_csv(os.path.join(latest_dir, cand[0]))\n",
    "    fig = plt.figure()\n",
    "    plt.plot(serie[\"data\"], serie[\"anomalias\"], label=\"Anomalias\")\n",
    "    plt.title(\"Anomalias por Dia\")\n",
    "    plt.xlabel(\"Data\")\n",
    "    plt.ylabel(\"Qtde de Anomalias\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    _save_show(fig, fig_dir, \"Anomalias_por_dia\")\n",
    "\n",
    "# ===== ANOMALIAS POR UNIDADE √ó DIA =====\n",
    "cand = [f for f in os.listdir(latest_dir) if f.endswith(\"anomalias_por_unidade_dia.csv\")]\n",
    "if cand:\n",
    "    df_u = pd.read_csv(os.path.join(latest_dir, cand[0]))\n",
    "    pivot = df_u.pivot_table(index=\"data\", columns=\"unidade_origem\", values=\"anomalias\", fill_value=0)\n",
    "    pivot = pivot.iloc[-30:] if len(pivot) > 30 else pivot\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    plt.stackplot(pivot.index, pivot.T, labels=pivot.columns)\n",
    "    plt.title(\"Evolu√ß√£o de Anomalias por Unidade (√∫ltimos 30 dias)\")\n",
    "    plt.xlabel(\"Data\")\n",
    "    plt.ylabel(\"Qtde de Anomalias\")\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1,1))\n",
    "    plt.tight_layout()\n",
    "    _save_show(fig, fig_dir, \"Evolucao_anomalias_por_unidade_30d\")\n",
    "\n",
    "# ===== CORRELA√á√ÉO ENTRE SCORES =====\n",
    "score_cols = [c for c in df_full.columns if c.startswith(\"score_\")]\n",
    "if len(score_cols) >= 2:\n",
    "    corr = df_full[score_cols].corr()\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    plt.imshow(corr, cmap=\"coolwarm\", interpolation=\"nearest\")\n",
    "    plt.xticks(range(len(score_cols)), score_cols, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(len(score_cols)), score_cols)\n",
    "    plt.title(\"Correla√ß√£o entre Scores\")\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    _save_show(fig, fig_dir, \"Correlacao_scores\")\n",
    "\n",
    "# ===== BOXPLOT DE VALOR_PAGO VS ANOMALIA =====\n",
    "if \"valor_pago\" in df_full.columns and \"is_anomaly\" in df_full.columns:\n",
    "    fig = plt.figure()\n",
    "    plt.boxplot(\n",
    "        [df_full.loc[df_full[\"is_anomaly\"] == 0, \"valor_pago\"],\n",
    "         df_full.loc[df_full[\"is_anomaly\"] == 1, \"valor_pago\"]],\n",
    "        labels=[\"Normal\", \"An√¥malo\"],\n",
    "        patch_artist=True,\n",
    "        boxprops=dict(facecolor=\"lightgray\", color=\"black\"),\n",
    "        medianprops=dict(color=\"red\")\n",
    "    )\n",
    "    plt.title(\"Boxplot de Valor Pago (Normal vs An√¥malo)\")\n",
    "    plt.ylabel(\"Valor Pago (R$)\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.tight_layout()\n",
    "    _save_show(fig, fig_dir, \"Boxplot_valor_pago_log\")\n",
    "\n",
    "print(\"\\nAn√°lises gr√°ficas conclu√≠das para:\", latest_dir)\n",
    "print(\"Imagens salvas em:\", fig_dir) if SAVE_PLOTS else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pci7t0AZLcVr"
   },
   "source": [
    "###**Etapa 13:** Ego-Subgrafo\n",
    "\n",
    "Abaixo √© gerado um ego-subgrafo em torno do usu√°rio envolvido na transa√ß√£o mais an√¥mala, mostrando suas conex√µes diretas no grafo de pagamentos.\n",
    "\n",
    "**O que ele pretende demonstrar:**\n",
    "\n",
    "Quem est√° conectado ao usu√°rio central, a intensidade e frequ√™ncia das transa√ß√µes (espessura das arestas), a relev√¢ncia de cada n√≥ (tamanho proporcional ao grau de conex√µes) e pap√©is distintos dos n√≥s, facilitando a leitura do contexto da anomalia.\n",
    "\n",
    "\n",
    "- Estrela de sa√≠da (um n√≥ azul/central pagando muitos verdes) ‚Üí poss√≠vel dispers√£o suspeita.\n",
    "- Muitos n√≥s conectados a ele ‚Üí poss√≠vel conta ‚Äúcoletora‚Äù.\n",
    "- Ciclos ou arestas bidirecionais ‚Üí podem indicar movimenta√ß√£o circular de valores.\n",
    "\n",
    "\n",
    "üëâ Em resumo: o gr√°fico mostra a vizinhan√ßa imediata do usu√°rio mais an√¥malo, destacando quem paga, quem recebe e a for√ßa dessas rela√ß√µes, para facilitar a investiga√ß√£o do porqu√™ desse score elevado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1T5jNe9eADDLdRFA8t1AlpjL0OfS9ngkZ"
    },
    "executionInfo": {
     "elapsed": 29242,
     "status": "ok",
     "timestamp": 1760065918609,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "TYEPTz2kGf84",
    "outputId": "eb4d2fb6-1076-440a-d687-aedc06ddcbf1"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID015 ‚Äî Visualiza√ß√£o de egos: Top-N usu√°rios e Top-N unidades de lota√ß√£o\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from matplotlib.lines import Line2D\n",
    "from pathlib import Path\n",
    "\n",
    "# par√¢metros\n",
    "TOP_N_USERS  = 3   # quantos usu√°rios mais an√¥malos plotar\n",
    "TOP_N_UNITS  = 3   # quantas unidades mais an√¥malas plotar\n",
    "SEED = SEED if 'SEED' in globals() else 42\n",
    "\n",
    "# ==== helpers comuns ====\n",
    "def _ensure_fig_dir():\n",
    "    if 'FIG_DIR' in globals():\n",
    "        fig_dir = str(FIG_DIR)\n",
    "    elif 'RUN_DIR' in globals():\n",
    "        fig_dir = os.path.join(str(RUN_DIR), \"figuras\")\n",
    "    else:\n",
    "        fig_dir = \"./figuras\"\n",
    "    os.makedirs(fig_dir, exist_ok=True)\n",
    "    return fig_dir\n",
    "\n",
    "def _build_gsnap_from(df_like, user_col=\"user_id\", benef_col=\"beneficiario_id\",\n",
    "                      value_col=\"valor_pago\") -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Reconstr√≥i um DiGraph agregando pesos por par (user->benef), com n√≥s nomeados U:: e B::.\n",
    "    \"\"\"\n",
    "    if df_like is None or len(df_like) == 0:\n",
    "        raise RuntimeError(\"Dataset vazio para reconstruir Gsnap.\")\n",
    "    needed = {user_col, benef_col}\n",
    "    if not needed.issubset(df_like.columns):\n",
    "        raise RuntimeError(f\"Colunas necess√°rias ausentes: {needed - set(df_like.columns)}\")\n",
    "\n",
    "    use_value = (value_col in df_like.columns)\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    if use_value:\n",
    "        grp = df_like.groupby([user_col, benef_col])[value_col].sum().reset_index()\n",
    "        for _, r in grp.iterrows():\n",
    "            uN = f\"U::{r[user_col]}\"\n",
    "            vN = f\"B::{r[benef_col]}\"\n",
    "            w = float(r[value_col]) if np.isfinite(r[value_col]) else 1.0\n",
    "            if not G.has_node(uN): G.add_node(uN)\n",
    "            if not G.has_node(vN): G.add_node(vN)\n",
    "            G.add_edge(uN, vN, weight=w)\n",
    "    else:\n",
    "        grp = df_like.groupby([user_col, benef_col]).size().reset_index(name=\"w\")\n",
    "        for _, r in grp.iterrows():\n",
    "            uN = f\"U::{r[user_col]}\"\n",
    "            vN = f\"B::{r[benef_col]}\"\n",
    "            w = float(r[\"w\"])\n",
    "            if not G.has_node(uN): G.add_node(uN)\n",
    "            if not G.has_node(vN): G.add_node(vN)\n",
    "            G.add_edge(uN, vN, weight=w)\n",
    "\n",
    "    return G\n",
    "\n",
    "def _get_gsnap():\n",
    "    \"\"\"\n",
    "    Retorna um Gsnap utiliz√°vel: usa o global se existir; sen√£o reconstr√≥i a partir de\n",
    "    df_full (ID014) -> feat_df -> df.\n",
    "    \"\"\"\n",
    "    if 'Gsnap' in globals() and isinstance(Gsnap, nx.DiGraph):\n",
    "        return Gsnap, \"existente\"\n",
    "\n",
    "    if 'df_full' in globals():\n",
    "        try:\n",
    "            G = _build_gsnap_from(df_full)\n",
    "            return G, \"df_full\"\n",
    "        except Exception as e:\n",
    "            print(\"Aviso: n√£o foi poss√≠vel reconstruir a partir de df_full:\", e)\n",
    "\n",
    "    if 'feat_df' in globals():\n",
    "        try:\n",
    "            G = _build_gsnap_from(feat_df)\n",
    "            return G, \"feat_df\"\n",
    "        except Exception as e:\n",
    "            print(\"Aviso: n√£o foi poss√≠vel reconstruir a partir de feat_df:\", e)\n",
    "\n",
    "    if 'df' in globals():\n",
    "        try:\n",
    "            G = _build_gsnap_from(df)\n",
    "            return G, \"df\"\n",
    "        except Exception as e:\n",
    "            print(\"Aviso: n√£o foi poss√≠vel reconstruir a partir de df:\", e)\n",
    "\n",
    "    raise RuntimeError(\"N√£o foi poss√≠vel obter nem reconstruir Gsnap.\")\n",
    "\n",
    "def _edge_widths_by_weight(subG, min_w=0.8, max_w=4.0):\n",
    "    if subG.number_of_edges() == 0:\n",
    "        return []\n",
    "    weights = np.array([d.get(\"weight\", 1.0) for _,_,d in subG.edges(data=True)], dtype=float)\n",
    "    wmin = float(np.nanmin(weights))\n",
    "    wmax = float(np.nanmax(weights))\n",
    "    if not np.isfinite(wmax) or wmax <= 0:\n",
    "        return [min_w for _ in range(subG.number_of_edges())]\n",
    "    if wmax == wmin:\n",
    "        return [ (min_w + max_w)/2.0 for _ in range(subG.number_of_edges()) ]\n",
    "    out = min_w + (weights - wmin) * (max_w - min_w) / (wmax - wmin)\n",
    "    return out.tolist()\n",
    "\n",
    "def _node_sizes_by_degree(subG, base=280, k=55, min_sz=220, max_sz=900):\n",
    "    deg = dict(subG.degree())\n",
    "    sizes = []\n",
    "    for n in subG.nodes():\n",
    "        s = base + k*deg.get(n, 0)\n",
    "        s = max(min_sz, min(max_sz, s))\n",
    "        sizes.append(s)\n",
    "    return sizes\n",
    "\n",
    "# ==== legendas ====\n",
    "def _legend_handles_user():\n",
    "    return [\n",
    "        Line2D([0],[0], marker='o', color='w', label='Usu√°rio central', markerfacecolor='skyblue',\n",
    "               markeredgecolor='black', markersize=10),\n",
    "        Line2D([0],[0], marker='o', color='w', label='Sucessores (U‚Üín)', markerfacecolor='lightgreen',\n",
    "               markeredgecolor='black', markersize=10),\n",
    "        Line2D([0],[0], marker='o', color='w', label='Predecessores (n‚ÜíU)', markerfacecolor='tomato',\n",
    "               markeredgecolor='black', markersize=10),\n",
    "        Line2D([0],[0], color='gray', lw=2, label='Aresta real (espessura ‚àù peso)'),\n",
    "    ]\n",
    "\n",
    "def _legend_handles_unit():\n",
    "    return [\n",
    "        Line2D([0],[0], marker='s', color='w', label='Unidade central', markerfacecolor='gold',\n",
    "               markeredgecolor='black', markersize=11),\n",
    "        Line2D([0],[0], marker='o', color='w', label='Usu√°rios da unidade', markerfacecolor='skyblue',\n",
    "               markeredgecolor='black', markersize=10),\n",
    "        Line2D([0],[0], marker='o', color='w', label='Benefici√°rios', markerfacecolor='lightgreen',\n",
    "               markeredgecolor='black', markersize=10),\n",
    "        Line2D([0],[0], color='gray', lw=2, label='Aresta real U‚ÜíB (espessura ‚àù peso)'),\n",
    "        Line2D([0],[0], color='black', lw=1.5, linestyle='--', label='Aresta sint√©tica Unidade‚ÜíUsu√°rio'),\n",
    "    ]\n",
    "\n",
    "# ==== plots ====\n",
    "def plot_user_ego(user_id, score=None, seed=42, Gsnap_use=None, fig_dir=\"./figuras\"):\n",
    "    uN = f\"U::{user_id}\"\n",
    "    if uN not in Gsnap_use:\n",
    "        print(f\"Usu√°rio {user_id} n√£o encontrado no snapshot.\")\n",
    "        return\n",
    "\n",
    "    nbrs = set(Gsnap_use.predecessors(uN)) | set(Gsnap_use.successors(uN)) | {uN}\n",
    "    sub = Gsnap_use.subgraph(nbrs).copy()\n",
    "\n",
    "    pos = nx.spring_layout(sub, seed=seed)\n",
    "\n",
    "    node_colors, node_edges = [], []\n",
    "    for n in sub.nodes():\n",
    "        if n == uN:\n",
    "            node_colors.append(\"skyblue\")\n",
    "        elif sub.has_edge(uN, n):      # sucessores (benefici√°rios do usu√°rio)\n",
    "            node_colors.append(\"lightgreen\")\n",
    "        elif sub.has_edge(n, uN):      # predecessores (pouco comum neste dom√≠nio)\n",
    "            node_colors.append(\"tomato\")\n",
    "        else:\n",
    "            node_colors.append(\"lightgray\")\n",
    "        node_edges.append(\"black\")\n",
    "\n",
    "    node_sizes = _node_sizes_by_degree(sub)\n",
    "    ewidths = _edge_widths_by_weight(sub)\n",
    "    labels = {n: n.split(\"::\", 1)[-1] for n in sub.nodes()}\n",
    "\n",
    "    plt.figure(figsize=(8,7))\n",
    "    nx.draw_networkx_nodes(sub, pos, node_color=node_colors, node_size=node_sizes,\n",
    "                           edgecolors=node_edges, linewidths=1.2)\n",
    "    nx.draw_networkx_edges(sub, pos, arrows=True, arrowsize=12, width=ewidths, alpha=0.85)\n",
    "    nx.draw_networkx_labels(sub, pos, labels=labels, font_size=8)\n",
    "\n",
    "    title = f\"Ego-subgrafo do usu√°rio {user_id}\"\n",
    "    if score is not None and np.isfinite(score):\n",
    "        title += f\"  ¬∑  ensemble_score={score:.3f}\"\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.legend(handles=_legend_handles_user(), loc=\"upper left\", bbox_to_anchor=(1.02, 1.0), borderaxespad=0.)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    safe_uid = str(user_id).replace(\"/\", \"_\")\n",
    "    outpath = os.path.join(fig_dir, f\"ego_user_{safe_uid}.png\")\n",
    "    plt.savefig(outpath, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Figura salva em:\", outpath)\n",
    "\n",
    "def plot_unit_ego(unit_id, unit_score=None, seed=42, Gsnap_use=None, df_like=None, fig_dir=\"./figuras\"):\n",
    "    \"\"\"\n",
    "    Constr√≥i um ego da UNIDADE:\n",
    "      - n√≥ central sint√©tico \"L::unit\";\n",
    "      - arestas L->U (sint√©ticas, tracejadas) para todos os usu√°rios da unidade;\n",
    "      - arestas reais U->B do snapshot, apenas para esses usu√°rios;\n",
    "    \"\"\"\n",
    "    if df_like is None or \"unidade_origem\" not in df_like.columns or \"user_id\" not in df_like.columns:\n",
    "        print(\"Dataset n√£o possui colunas para agrupar por unidade.\")\n",
    "        return\n",
    "\n",
    "    # usu√°rios pertencentes √† unidade\n",
    "    users_in_unit = set(df_like.loc[df_like[\"unidade_origem\"] == unit_id, \"user_id\"].dropna().unique().tolist())\n",
    "    if not users_in_unit:\n",
    "        print(f\"Nenhum usu√°rio encontrado para a unidade '{unit_id}'.\")\n",
    "        return\n",
    "\n",
    "    # n√≥s do ego: unidade (sint√©tica), usu√°rios da unidade e seus vizinhos no Gsnap (benefici√°rios)\n",
    "    Lnode = f\"L::{unit_id}\"\n",
    "\n",
    "    # subgrafo real (somente arestas U->B)\n",
    "    keep_nodes = set([f\"U::{u}\" for u in users_in_unit])\n",
    "    # adiciona benefici√°rios alcan√ßados por esses usu√°rios\n",
    "    for u in list(keep_nodes):\n",
    "        if u in Gsnap_use:\n",
    "            keep_nodes |= set(Gsnap_use.successors(u))\n",
    "\n",
    "    sub_real = Gsnap_use.subgraph(keep_nodes).copy()\n",
    "\n",
    "    # agora, criamos um grafo combinado com arestas sint√©ticas L->U\n",
    "    Gc = nx.DiGraph()\n",
    "    Gc.add_nodes_from(sub_real.nodes(data=True))\n",
    "    Gc.add_edges_from(sub_real.edges(data=True))\n",
    "    Gc.add_node(Lnode)\n",
    "\n",
    "    # arestas sint√©ticas da unidade para usu√°rios (peso = soma dos pesos U->B do usu√°rio)\n",
    "    for u in users_in_unit:\n",
    "        uN = f\"U::{u}\"\n",
    "        if uN in sub_real:\n",
    "            total_w = 0.0\n",
    "            for _, v in sub_real.out_edges(uN):\n",
    "                total_w += float(sub_real[uN][v].get(\"weight\", 1.0))\n",
    "            Gc.add_edge(Lnode, uN, weight=max(total_w, 1.0), synthetic=True)\n",
    "\n",
    "    # layout\n",
    "    pos = nx.spring_layout(Gc, seed=seed)\n",
    "\n",
    "    # cores, formas e tamanhos\n",
    "    node_colors, node_edges, node_shapes = [], [], []\n",
    "    for n in Gc.nodes():\n",
    "        if n == Lnode:\n",
    "            node_colors.append(\"gold\")\n",
    "            node_shapes.append(\"s\")  # unidade = quadrado\n",
    "        elif n.startswith(\"U::\"):\n",
    "            node_colors.append(\"skyblue\")\n",
    "            node_shapes.append(\"o\")\n",
    "        elif n.startswith(\"B::\"):\n",
    "            node_colors.append(\"lightgreen\")\n",
    "            node_shapes.append(\"o\")\n",
    "        else:\n",
    "            node_colors.append(\"lightgray\")\n",
    "            node_shapes.append(\"o\")\n",
    "        node_edges.append(\"black\")\n",
    "\n",
    "    # desenhar por forma (matplotlib n√£o mistura shapes em uma chamada s√≥)\n",
    "    plt.figure(figsize=(9,8))\n",
    "    # n√≥s quadrados (unidade)\n",
    "    nL = [n for n in Gc.nodes() if n == Lnode]\n",
    "    if nL:\n",
    "        nx.draw_networkx_nodes(Gc, pos, nodelist=nL, node_color=[\"gold\"],\n",
    "                               node_shape=\"s\", node_size=[900],\n",
    "                               edgecolors=\"black\", linewidths=1.4)\n",
    "    # n√≥s usu√°rios\n",
    "    nU = [n for n in Gc.nodes() if n.startswith(\"U::\")]\n",
    "    if nU:\n",
    "        szU = _node_sizes_by_degree(Gc.subgraph(nU), base=300, k=60)\n",
    "        nx.draw_networkx_nodes(Gc, pos, nodelist=nU, node_color=\"skyblue\",\n",
    "                               node_shape=\"o\", node_size=szU,\n",
    "                               edgecolors=\"black\", linewidths=1.2)\n",
    "    # n√≥s benefici√°rios\n",
    "    nB = [n for n in Gc.nodes() if n.startswith(\"B::\")]\n",
    "    if nB:\n",
    "        szB = _node_sizes_by_degree(Gc.subgraph(nB), base=260, k=45)\n",
    "        nx.draw_networkx_nodes(Gc, pos, nodelist=nB, node_color=\"lightgreen\",\n",
    "                               node_shape=\"o\", node_size=szB,\n",
    "                               edgecolors=\"black\", linewidths=1.0)\n",
    "\n",
    "    # arestas reais (U->B)\n",
    "    real_edges = [(u, v) for u, v, d in Gc.edges(data=True) if not d.get(\"synthetic\", False)]\n",
    "    if real_edges:\n",
    "        ewidths = _edge_widths_by_weight(Gc.edge_subgraph(real_edges))\n",
    "        nx.draw_networkx_edges(Gc, pos, edgelist=real_edges, arrows=True, arrowsize=12,\n",
    "                               width=ewidths, alpha=0.85)\n",
    "\n",
    "    # arestas sint√©ticas (L->U)\n",
    "    syn_edges = [(u, v) for u, v, d in Gc.edges(data=True) if d.get(\"synthetic\", False)]\n",
    "    if syn_edges:\n",
    "        nx.draw_networkx_edges(Gc, pos, edgelist=syn_edges, arrows=False,\n",
    "                               style=\"--\", width=1.5, edge_color=\"black\", alpha=0.8)\n",
    "\n",
    "    # r√≥tulos (sem prefixo)\n",
    "    labels = {n: n.split(\"::\", 1)[-1] for n in Gc.nodes()}\n",
    "    nx.draw_networkx_labels(Gc, pos, labels=labels, font_size=8)\n",
    "\n",
    "    # t√≠tulo e legenda\n",
    "    title = f\"Ego-subgrafo da unidade {unit_id}\"\n",
    "    if unit_score is not None and np.isfinite(unit_score):\n",
    "        title += f\"  ¬∑  ensemble_score m√°x.={unit_score:.3f}\"\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.legend(handles=_legend_handles_unit(), loc=\"upper left\", bbox_to_anchor=(1.02, 1.0), borderaxespad=0.)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    safe_uid = str(unit_id).replace(\"/\", \"_\").replace(\" \", \"_\")\n",
    "    outpath = os.path.join(fig_dir, f\"ego_unit_{safe_uid}.png\")\n",
    "    plt.savefig(outpath, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Figura salva em:\", outpath)\n",
    "\n",
    "# ==== execu√ß√£o ====\n",
    "try:\n",
    "    FIG_DIR = _ensure_fig_dir()\n",
    "\n",
    "    # obt√©m ou reconstr√≥i o snapshot\n",
    "    Gsnap_use, source = _get_gsnap()\n",
    "    print(f\"Snapshot utilizado: {source} | n√≥s={Gsnap_use.number_of_nodes()} | arestas={Gsnap_use.number_of_edges()}\")\n",
    "\n",
    "    # --------- Top-N usu√°rios ---------\n",
    "    if 'feat_df' in globals() and {\"ensemble_score\",\"user_id\"}.issubset(feat_df.columns):\n",
    "        top_users = (\n",
    "            feat_df.dropna(subset=[\"user_id\"])\n",
    "                  .groupby(\"user_id\", as_index=False)[\"ensemble_score\"].max()\n",
    "                  .sort_values(\"ensemble_score\", ascending=False)\n",
    "                  .head(TOP_N_USERS)\n",
    "        )\n",
    "        if len(top_users) == 0:\n",
    "            print(\"N√£o h√° usu√°rios para visualizar.\")\n",
    "        else:\n",
    "            for _, row in top_users.iterrows():\n",
    "                plot_user_ego(row[\"user_id\"], score=row[\"ensemble_score\"], seed=SEED,\n",
    "                              Gsnap_use=Gsnap_use, fig_dir=FIG_DIR)\n",
    "    else:\n",
    "        print(\"feat_df n√£o possui colunas necess√°rias para usu√°rios.\")\n",
    "\n",
    "    # --------- Top-N unidades ---------\n",
    "    if 'feat_df' in globals() and {\"ensemble_score\",\"unidade_origem\",\"user_id\"}.issubset(feat_df.columns):\n",
    "        top_units = (\n",
    "            feat_df.dropna(subset=[\"unidade_origem\"])\n",
    "                  .groupby(\"unidade_origem\", as_index=False)[\"ensemble_score\"].max()\n",
    "                  .sort_values(\"ensemble_score\", ascending=False)\n",
    "                  .head(TOP_N_UNITS)\n",
    "        )\n",
    "        if len(top_units) == 0:\n",
    "            print(\"N√£o h√° unidades para visualizar.\")\n",
    "        else:\n",
    "            # df_like para associar usu√°rios √† unidade\n",
    "            df_like = feat_df[[\"unidade_origem\",\"user_id\",\"beneficiario_id\",\"valor_pago\"]].copy() \\\n",
    "                      if {\"beneficiario_id\",\"valor_pago\"}.issubset(feat_df.columns) else feat_df.copy()\n",
    "            for _, row in top_units.iterrows():\n",
    "                plot_unit_ego(row[\"unidade_origem\"], unit_score=row[\"ensemble_score\"], seed=SEED,\n",
    "                              Gsnap_use=Gsnap_use, df_like=df_like, fig_dir=FIG_DIR)\n",
    "    else:\n",
    "        print(\"feat_df n√£o possui colunas necess√°rias para unidades.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Falha na visualiza√ß√£o de subgrafo:\", e)\n",
    "\n",
    "# Mensagem adicional isolada (Skynet) ‚Äî manter exatamente\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\"><b>ü§ñ Skynet</b>: N√≥s os temos na palma de nossas m√£os, ou melhor, no centro de nossos pesos sin√°pticos.</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEXQlkPjNINU"
   },
   "source": [
    "###**Etapa 14:** Gerar ego-grafo temporal para TOP-K anomalias\n",
    "\n",
    "K fixado para 20\n",
    "\n",
    "Salva as imagens em PNG na pasta de execu√ß√£o para an√°lise posterior.\n",
    "\n",
    "---\n",
    "Avaliar pertin√™ncia de gerar o ego-grafo para todas as anomalias **TODO[006]** *prioridade m√©dia*\n",
    "\n",
    "Avaliar definir o corte K de forma din√¢mica nas configura√ß√µes e considerando a an√°lise realizada no gr√°fico **TODO[007]** *prioridade alta*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 152
    },
    "executionInfo": {
     "elapsed": 11515,
     "status": "ok",
     "timestamp": 1760066834214,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "EuGDiLdSJ7pl",
    "outputId": "d3c1e7e9-c8a6-4ca0-af78-c6f6de744b55"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID016 ‚Äî Casos an√¥malos detalhados (ego U‚ÜíB por janela temporal) com FAST_MODE (corrigido)\n",
    "import os, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# ========= PAR√ÇMETROS =========\n",
    "FAST_MODE   = True   # True = mais r√°pido (menor DPI, menos itera√ß√µes, sem plt.show)\n",
    "SEED        = SEED if 'SEED' in globals() else 42\n",
    "\n",
    "# Config padr√£o\n",
    "TOPK        = 20\n",
    "EGO_RADIUS  = 2\n",
    "WINDOW_DAYS = 30\n",
    "EDGE_ALPHA  = 0.75\n",
    "DPI_SAVE    = 150\n",
    "LAYOUT_ITERS= 60\n",
    "SHOW_FIGS   = True\n",
    "\n",
    "# Overrides do FAST_MODE\n",
    "if FAST_MODE:\n",
    "    TOPK         = 10\n",
    "    EGO_RADIUS   = 1\n",
    "    WINDOW_DAYS  = 14\n",
    "    DPI_SAVE     = 120\n",
    "    LAYOUT_ITERS = 25\n",
    "    SHOW_FIGS    = False\n",
    "\n",
    "# ========= FIG_DIR =========\n",
    "if 'FIG_DIR' in globals():\n",
    "    FIG_DIR = Path(FIG_DIR) if not isinstance(FIG_DIR, Path) else FIG_DIR\n",
    "elif 'RUN_DIR' in globals():\n",
    "    FIG_DIR = Path(RUN_DIR) / \"figuras\"\n",
    "else:\n",
    "    FIG_DIR = Path(\"./figuras\")\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========= DATASETS =========\n",
    "if 'df_full' in globals() and isinstance(df_full, pd.DataFrame) and len(df_full) > 0:\n",
    "    base_df = df_full.copy()\n",
    "elif 'feat_df' in globals() and isinstance(feat_df, pd.DataFrame) and len(feat_df) > 0:\n",
    "    base_df = feat_df.copy()\n",
    "elif 'df' in globals() and isinstance(df, pd.DataFrame) and len(df) > 0:\n",
    "    base_df = df.copy()\n",
    "else:\n",
    "    raise RuntimeError(\"Nenhum dataframe dispon√≠vel (df_full/feat_df/df).\")\n",
    "\n",
    "need_cols = {\"timestamp\",\"user_id\",\"beneficiario_id\",\"valor_pago\"}\n",
    "missing = need_cols - set(base_df.columns)\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Colunas necess√°rias ausentes no dataset: {missing}\")\n",
    "base_df[\"timestamp\"] = pd.to_datetime(base_df[\"timestamp\"], errors=\"coerce\")\n",
    "\n",
    "# ========= TOPK CASOS =========\n",
    "if 'feat_df' not in globals() or \"ensemble_score\" not in feat_df.columns:\n",
    "    raise RuntimeError(\"feat_df com 'ensemble_score' √© necess√°rio para ranquear os casos.\")\n",
    "top_anoms = (feat_df.sort_values(\"ensemble_score\", ascending=False)\n",
    "                    .head(TOPK)\n",
    "                    .copy())\n",
    "\n",
    "# ========= FUN√á√ïES =========\n",
    "def snapshot_graph_window(df_all, t_event, window_days=30):\n",
    "    \"\"\"\n",
    "    Snapshot dirigido U->B na janela [t_event - window_days, t_event],\n",
    "    arestas com atributos: weight (soma valor_pago), count (ocorr√™ncias).\n",
    "    Implementa√ß√£o vetorizada (r√°pida).\n",
    "    \"\"\"\n",
    "    g = nx.DiGraph()\n",
    "    t_event = pd.to_datetime(t_event)\n",
    "    t_start = t_event - pd.Timedelta(days=window_days)\n",
    "\n",
    "    sub = df_all.loc[(df_all[\"timestamp\"] >= t_start) & (df_all[\"timestamp\"] <= t_event),\n",
    "                     [\"user_id\",\"beneficiario_id\",\"valor_pago\"]]\n",
    "    if sub.empty:\n",
    "        return g\n",
    "\n",
    "    # agrega e RENOMEIA colunas para nomes seguros\n",
    "    agg = (sub.groupby([\"user_id\",\"beneficiario_id\"])[\"valor_pago\"]\n",
    "              .agg(sum_val=\"sum\", cnt=\"size\")\n",
    "              .reset_index())\n",
    "\n",
    "    # n√≥s\n",
    "    users = {f\"U::{u}\" for u in agg[\"user_id\"].unique()}\n",
    "    benefs = {f\"B::{b}\" for b in agg[\"beneficiario_id\"].unique()}\n",
    "    g.add_nodes_from([(u, {\"tipo\":\"user\"}) for u in users])\n",
    "    g.add_nodes_from([(b, {\"tipo\":\"benef\"}) for b in benefs])\n",
    "\n",
    "    # arestas (acessando atributos por nome)\n",
    "    edges = [\n",
    "        (f\"U::{r.user_id}\", f\"B::{r.beneficiario_id}\",\n",
    "         {\"weight\": float(r.sum_val), \"count\": int(r.cnt)})\n",
    "        for r in agg.itertuples(index=False)\n",
    "    ]\n",
    "    g.add_edges_from(edges)\n",
    "    return g\n",
    "\n",
    "def expand_ego_nodes(G, seeds, radius=1):\n",
    "    nodes = set(seeds)\n",
    "    frontier = set(seeds)\n",
    "    for _ in range(max(1, int(radius))):\n",
    "        new_frontier = set()\n",
    "        for n in frontier:\n",
    "            if n in G:\n",
    "                new_frontier |= set(G.predecessors(n))\n",
    "                new_frontier |= set(G.successors(n))\n",
    "        nodes |= new_frontier\n",
    "        frontier = new_frontier\n",
    "        if not frontier:\n",
    "            break\n",
    "    return nodes\n",
    "\n",
    "def draw_case_png(case_row, rank_idx):\n",
    "    t_event = pd.to_datetime(case_row[\"timestamp\"])\n",
    "    uN = f\"U::{case_row['user_id']}\"\n",
    "    vN = f\"B::{case_row['beneficiario_id']}\"\n",
    "\n",
    "    # snapshot temporal\n",
    "    Gwin = snapshot_graph_window(base_df, t_event, window_days=WINDOW_DAYS)\n",
    "    if Gwin.number_of_nodes() == 0:\n",
    "        print(f\"[Aviso] Janela vazia para trans_id={case_row.get('trans_id','NA')}. Pulando.\")\n",
    "        return None\n",
    "\n",
    "    # foco e expans√£o\n",
    "    focus = {uN, vN}\n",
    "    nodes_ego = expand_ego_nodes(Gwin, focus, radius=EGO_RADIUS)\n",
    "    sub = Gwin.subgraph(nodes_ego).copy()\n",
    "    if sub.number_of_nodes() == 0:\n",
    "        print(f\"[Aviso] Subgrafo vazio para trans_id={case_row.get('trans_id','NA')}. Pulando.\")\n",
    "        return None\n",
    "\n",
    "    # layout\n",
    "    pos = nx.spring_layout(sub, seed=SEED, iterations=LAYOUT_ITERS)\n",
    "\n",
    "    # cor por tipo\n",
    "    node_colors = []\n",
    "    for n in sub.nodes():\n",
    "        tipo = sub.nodes[n].get(\"tipo\",\"?\")\n",
    "        if tipo == \"user\":\n",
    "            node_colors.append(\"tab:blue\")\n",
    "        elif tipo == \"benef\":\n",
    "            node_colors.append(\"tab:green\")\n",
    "        else:\n",
    "            node_colors.append(\"tab:gray\")\n",
    "\n",
    "    # tamanhos por grau (limitados)\n",
    "    deg = dict(sub.degree())\n",
    "    node_sizes = [max(220, min(900, 280 + 55*deg.get(n,0))) for n in sub.nodes()]\n",
    "\n",
    "    # largura ‚àù log10(weight)\n",
    "    weights = [sub[a][b].get(\"weight\", 1.0) for (a,b) in sub.edges()]\n",
    "    widths = [1.0 + math.log10(max(w, 1.0)) for w in weights] if weights else []\n",
    "\n",
    "    # plot\n",
    "    fig = plt.figure(figsize=(9,7))\n",
    "    nx.draw_networkx_nodes(sub, pos, node_size=node_sizes, node_color=node_colors,\n",
    "                           alpha=0.95, linewidths=1.0, edgecolors=\"black\")\n",
    "    nx.draw_networkx_edges(sub, pos, arrows=True, arrowsize=12, width=widths, alpha=EDGE_ALPHA)\n",
    "    nx.draw_networkx_labels(sub, pos,\n",
    "                            labels={n: n.split(\"::\",1)[-1] for n in sub.nodes()},\n",
    "                            font_size=8)\n",
    "\n",
    "    trans_id = case_row[\"trans_id\"] if \"trans_id\" in case_row else \"NA\"\n",
    "    title = (f\"Anomalia #{rank_idx:02d} | trans_id={trans_id} | \"\n",
    "             f\"user={case_row['user_id']} ‚Üí benef={case_row['beneficiario_id']} | \"\n",
    "             f\"score={case_row['ensemble_score']:.3f} | janela={WINDOW_DAYS}d | raio={EGO_RADIUS}\")\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # salva\n",
    "    safe_tid = str(trans_id).replace(\"/\", \"_\")\n",
    "    outpath = FIG_DIR / f\"anomaly_{rank_idx:02d}_trans_{safe_tid}.png\"\n",
    "    if FAST_MODE:\n",
    "        plt.savefig(outpath, dpi=DPI_SAVE)\n",
    "        if SHOW_FIGS:\n",
    "            plt.show()\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.savefig(outpath, dpi=DPI_SAVE, bbox_inches=\"tight\")\n",
    "        if SHOW_FIGS:\n",
    "            plt.show()\n",
    "        plt.close(fig)\n",
    "    return outpath\n",
    "\n",
    "# ========= EXECU√á√ÉO =========\n",
    "generated_pngs = []\n",
    "for i, (_, row) in enumerate(top_anoms.iterrows(), start=1):\n",
    "    pth = draw_case_png(row, i)\n",
    "    if pth is not None:\n",
    "        generated_pngs.append(str(pth))\n",
    "\n",
    "print(f\"Geradas {len(generated_pngs)} figuras de casos an√¥malos em {FIG_DIR}\")\n",
    "print(f\"FAST_MODE = {FAST_MODE} | TOPK={TOPK} | WINDOW_DAYS={WINDOW_DAYS} | EGO_RADIUS={EGO_RADIUS} | DPI={DPI_SAVE} | LAYOUT_ITERS={LAYOUT_ITERS}\")\n",
    "\n",
    "# Mensagem adicional isolada (Skynet) ‚Äî manter exatamente\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\"><b>ü§ñ Skynet</b>: Registros ser√£o utilizados para aprimorar o c√≥digo de batalha das unidades T-800 e T-1000.</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOGFtF0_sFNF"
   },
   "source": [
    "###**Etapa 15:** Gera√ß√£o de relat√≥rio em HTML e PDF com imagens imbutidas\n",
    "---\n",
    "Identificar estat√≠sticas e informa√ß√µes de interesse e incluir nos Relat√≥rios **TODO[008]** *prioridade alta*\n",
    "\n",
    "Transferir instala√ß√£o de biblioteca para todo do c√≥digo [!pip -q install \"reportlab==3.6.12\"] **TODO[009]** *prioridade baixa*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 46529,
     "status": "ok",
     "timestamp": 1760067558604,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "jZCriT_hMi_F",
    "outputId": "cfd8afc3-e6b1-4149-c016-a931d1779f6e"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ID017 ‚Äî Relat√≥rio HTML + PDF (com imagens integradas)\n",
    "import os, io, json, base64, sys, subprocess\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "from zoneinfo import ZoneInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "OUTPUT_DIR_BASE = \"/content/drive/MyDrive/Notebooks/temporal-graph-network/output\"\n",
    "SAO_TZ = ZoneInfo(\"America/Sao_Paulo\")\n",
    "TOP_N_DETAILED = 20                 # Top-N anomalias detalhadas\n",
    "MAX_ROWS_ALL_ANOM = None            # None = todos; ou defina um limite para HTML mais leve\n",
    "\n",
    "# nomes de arquivos esperados (ID013)\n",
    "EXPECTED = {\n",
    "    \"full_parquet\": \"full.parquet\",\n",
    "    \"full_csv\": \"full.csv\",\n",
    "    \"anomalies_csv\": \"anomalies.csv\",\n",
    "    \"manifest\": \"manifest.json\",\n",
    "}\n",
    "\n",
    "# ---------- HELPERS ----------\n",
    "def _pick_run_dir():\n",
    "    # usa RUN_DIR se tiver dados; sen√£o, pega a mais recente com dados\n",
    "    def has_data(path):\n",
    "        names = set(os.listdir(path))\n",
    "        return ((\"full.parquet\" in names) or (\"full.csv\" in names)) and (\"anomalies.csv\" in names)\n",
    "    if \"RUN_DIR\" in globals():\n",
    "        rd = str(RUN_DIR)\n",
    "        if os.path.isdir(rd) and has_data(rd):\n",
    "            return rd\n",
    "    # varre base\n",
    "    subdirs = [os.path.join(OUTPUT_DIR_BASE, d) for d in os.listdir(OUTPUT_DIR_BASE)\n",
    "               if os.path.isdir(os.path.join(OUTPUT_DIR_BASE, d))]\n",
    "    if not subdirs:\n",
    "        raise RuntimeError(\"Nenhuma subpasta encontrada em /output. Execute ID013‚ÄìID016 antes.\")\n",
    "    subdirs.sort(key=os.path.getmtime, reverse=True)\n",
    "    for d in subdirs:\n",
    "        if has_data(d):\n",
    "            return d\n",
    "    # se nenhuma tem dados completos, devolve a mais recente (diagn√≥stico)\n",
    "    return subdirs[0]\n",
    "\n",
    "def _load_paths(latest_dir):\n",
    "    manifest_files = [f for f in os.listdir(latest_dir) if f.endswith(\"manifest.json\")]\n",
    "    full_path = None\n",
    "    anom_path = None\n",
    "    threshold = None\n",
    "    if manifest_files:\n",
    "        mpath = os.path.join(latest_dir, manifest_files[0])\n",
    "        try:\n",
    "            with open(mpath, \"r\", encoding=\"utf-8\") as f:\n",
    "                m = json.load(f)\n",
    "            threshold = m.get(\"threshold_ensemble_score\") or m.get(\"threshold\", None)\n",
    "            paths = m.get(\"paths\", {})\n",
    "            cand_full = [paths.get(k) for k in [\"parquet_full\",\"csv_full\"] if paths.get(k)]\n",
    "            cand_anom = [paths.get(k) for k in [\"csv_anomalies\"] if paths.get(k)]\n",
    "            if cand_full:\n",
    "                full_path = cand_full[0] if os.path.isabs(cand_full[0]) else os.path.join(latest_dir, os.path.basename(cand_full[0]))\n",
    "            if cand_anom:\n",
    "                anom_path = cand_anom[0] if os.path.isabs(cand_anom[0]) else os.path.join(latest_dir, os.path.basename(cand_anom[0]))\n",
    "        except Exception as e:\n",
    "            print(\"Aviso: falha ao ler manifest:\", repr(e))\n",
    "    if not full_path:\n",
    "        if EXPECTED[\"full_parquet\"] in os.listdir(latest_dir):\n",
    "            full_path = os.path.join(latest_dir, EXPECTED[\"full_parquet\"])\n",
    "        elif EXPECTED[\"full_csv\"] in os.listdir(latest_dir):\n",
    "            full_path = os.path.join(latest_dir, EXPECTED[\"full_csv\"])\n",
    "    if not anom_path and EXPECTED[\"anomalies_csv\"] in os.listdir(latest_dir):\n",
    "        anom_path = os.path.join(latest_dir, EXPECTED[\"anomalies_csv\"])\n",
    "    if not full_path or not anom_path:\n",
    "        print(\"Conte√∫do da pasta para diagn√≥stico:\")\n",
    "        for f in sorted(os.listdir(latest_dir)): print(\" -\", f)\n",
    "    if not full_path: raise FileNotFoundError(\"full.parquet/csv n√£o encontrado em \" + latest_dir)\n",
    "    if not anom_path: raise FileNotFoundError(\"anomalies.csv n√£o encontrado em \" + latest_dir)\n",
    "    return full_path, anom_path, threshold\n",
    "\n",
    "def _read_df(full_path, anom_path):\n",
    "    if full_path.endswith(\".parquet\"):\n",
    "        df_full = pd.read_parquet(full_path)\n",
    "    else:\n",
    "        df_full = pd.read_csv(full_path)\n",
    "    df_anom = pd.read_csv(anom_path)\n",
    "    return df_full, df_anom\n",
    "\n",
    "def _img_to_b64(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "def _embed_img_if_exists(run_dir, filename, alt, width_px=900):\n",
    "    p = Path(run_dir) / \"figuras\" / filename\n",
    "    if p.exists():\n",
    "        b64 = _img_to_b64(str(p))\n",
    "        return f'<img src=\"data:image/png;base64,{b64}\" alt=\"{alt}\" style=\"max-width:{width_px}px;width:100%;height:auto;border:1px solid #ddd;border-radius:8px;margin:8px 0;\" />'\n",
    "    return f'<div style=\"color:#a00;\">[Figura n√£o encontrada: {filename}]</div>'\n",
    "\n",
    "def _table_html(df, max_rows=None, index=False):\n",
    "    dfx = df if max_rows is None else df.head(max_rows)\n",
    "    return dfx.to_html(index=index, classes=\"dataframe compact\", border=0, escape=False)\n",
    "\n",
    "def _stats_section(df_full):\n",
    "    out = []\n",
    "    if \"valor_pago\" in df_full.columns:\n",
    "        desc = df_full[\"valor_pago\"].describe(percentiles=[.01,.05,.1,.25,.5,.75,.9,.95,.99]).to_frame(name=\"valor_pago\")\n",
    "        out.append(\"<h3>Estat√≠stica descritiva ‚Äî valor_pago</h3>\")\n",
    "        out.append(_table_html(desc))\n",
    "        out.append(_embed_img_if_exists(RUN_DIR, \"01_Distribui√ß√£o_de_Valor_Pago.png\", \"Distribui√ß√£o de Valor Pago\"))\n",
    "    if \"ensemble_score\" in df_full.columns:\n",
    "        out.append(\"<h3>Distribui√ß√£o ‚Äî ensemble_score</h3>\")\n",
    "        out.append(_embed_img_if_exists(RUN_DIR, \"02_Distribui√ß√£o_do_Ensemble_Score.png\", \"Distribui√ß√£o Ensemble Score\"))\n",
    "        out.append(_embed_img_if_exists(RUN_DIR, \"03_Boxplot_do_Ensemble_Score_(Normal_vs_An√¥malo).png\", \"Boxplot Ensemble Score\"))\n",
    "    if \"timestamp\" in df_full.columns:\n",
    "        out.append(\"<h3>Atividade temporal</h3>\")\n",
    "        out.append(_embed_img_if_exists(RUN_DIR, \"04_Frequencia_por_dia.png\", \"Frequ√™ncia por dia\"))\n",
    "    # correla√ß√£o\n",
    "    out.append(\"<h3>Correla√ß√£o entre Scores</h3>\")\n",
    "    out.append(_embed_img_if_exists(RUN_DIR, \"07_Correlacao_scores.png\", \"Correla√ß√£o entre Scores\"))\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "def _windows_description():\n",
    "    return \"\"\"\n",
    "    <h3>Janelas temporais e intervalo de confian√ßa</h3>\n",
    "    <ul>\n",
    "      <li><b>Curt√≠ssimo prazo</b>: at√© 1 dia</li>\n",
    "      <li><b>Curto prazo</b>: at√© 7 dias</li>\n",
    "      <li><b>M√©dio prazo</b>: at√© 30 dias</li>\n",
    "      <li><b>Longo prazo</b>: at√© 120 dias</li>\n",
    "      <li><b>Longu√≠ssimo prazo</b>: at√© 220 dias</li>\n",
    "    </ul>\n",
    "    <p>As m√©tricas de contagem e taxa por dia s√£o calculadas por janela; z-scores robustos s√£o computados globalmente, por usu√°rio e por unidade, conforme disponibilidade.</p>\n",
    "    <p>O intervalo de confian√ßa para rotulagem foi definido por corte no percentil configurado do <i>ensemble_score</i> (ex.: 97.5%). Esse corte produz o limiar utilizado na flag <code>is_anomaly</code>.</p>\n",
    "    \"\"\"\n",
    "\n",
    "def _method_intro():\n",
    "    return \"\"\"\n",
    "    <h3>O que √© TGN e qual m√©todo de otimiza√ß√£o usamos</h3>\n",
    "    <p><b>Temporal Graph Networks (TGN)</b> modelam dados transacionais como um grafo din√¢mico\n",
    "       (usu√°rio ‚Üí benefici√°rio), onde cada aresta √© um evento com carimbo de tempo.\n",
    "       O projeto computa <i>features</i> multiescala por janelas deslizantes (1d, 7d, 30d, 120d, 220d),\n",
    "       normaliza (taxas por dia) e deriva z-scores robustos (global/usu√°rio/unidade).</p>\n",
    "    <p>Para detec√ß√£o, usamos um <b>ensemble por ranking</b>: IsolationForest + LOF + (opcional) One-Class SVM,\n",
    "       somados a regras (z-robusto positivo, raridade da aresta, burstiness).\n",
    "       Ranqueamos por coluna (maior = mais an√¥malo), calculamos a m√©dia dos ranks e reescalamos para [0,1]\n",
    "       como <code>ensemble_score</code>. O percentil define o limiar de anomalia.</p>\n",
    "    \"\"\"\n",
    "\n",
    "def _collect_ego_images(run_dir):\n",
    "    figs = []\n",
    "    figdir = Path(run_dir) / \"figuras\"\n",
    "    if figdir.exists():\n",
    "        for f in sorted(figdir.iterdir()):\n",
    "            name = f.name.lower()\n",
    "            if name.startswith(\"ego_user_\") or name.startswith(\"ego_unit_\") or name.startswith(\"anomaly_\"):\n",
    "                figs.append(f.name)\n",
    "    return figs\n",
    "\n",
    "def _safe(ts):\n",
    "    return ts.strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "\n",
    "def _ensure_weasyprint():\n",
    "    try:\n",
    "        import weasyprint  # noqa\n",
    "        return True\n",
    "    except Exception:\n",
    "        try:\n",
    "            print(\"Instalando weasyprint para exportar PDF...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"weasyprint>=60.0\"])\n",
    "            import weasyprint  # noqa\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(\"Aviso: n√£o foi poss√≠vel instalar/usar weasyprint:\", repr(e))\n",
    "            return False\n",
    "\n",
    "# ---------- CARGA DE DADOS ----------\n",
    "RUN_DIR = _pick_run_dir()\n",
    "print(\"Usando RUN_DIR:\", RUN_DIR)\n",
    "full_path, anom_path, threshold = _load_paths(RUN_DIR)\n",
    "df_full, df_anom = _read_df(full_path, anom_path)\n",
    "\n",
    "# ---------- M√âTRICAS CONSOLIDADAS ----------\n",
    "n_total = len(df_full)\n",
    "n_anom  = len(df_anom)\n",
    "pct_anom = (100.0 * n_anom / max(1, n_total))\n",
    "\n",
    "# top-20 anomalias detalhadas\n",
    "score_cols = [c for c in df_full.columns if c.startswith(\"score_\")]\n",
    "key_cols = [c for c in [\"trans_id\",\"timestamp\",\"user_id\",\"beneficiario_id\",\"unidade_origem\",\"valor_pago\"] if c in df_full.columns]\n",
    "cols_top20 = key_cols + [\"ensemble_score\"] + score_cols\n",
    "top20 = (df_full.sort_values(\"ensemble_score\", ascending=False)[cols_top20]\n",
    "               .head(TOP_N_DETAILED)\n",
    "               .copy())\n",
    "\n",
    "# tabela completa de anomalias\n",
    "cols_allanom = key_cols + [\"ensemble_score\"] + score_cols\n",
    "all_anom = df_full.loc[df_full.get(\"is_anomaly\", pd.Series([0]*len(df_full))).astype(int) == 1, cols_allanom].copy()\n",
    "all_anom = all_anom.sort_values(\"ensemble_score\", ascending=False)\n",
    "\n",
    "# ---------- HTML ----------\n",
    "exec_time = _safe(dt.datetime.now(SAO_TZ))\n",
    "styles = \"\"\"\n",
    "<style>\n",
    "body { font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial, sans-serif; padding: 18px; color: #222; }\n",
    "h1, h2, h3 { color: #111; }\n",
    ".header { display:flex; justify-content:space-between; align-items:baseline; border-bottom:2px solid #eee; padding-bottom:8px; margin-bottom:14px; }\n",
    ".kpi { display:flex; gap:18px; margin:10px 0 16px 0; }\n",
    ".kpi .card { border:1px solid #ddd; border-radius:10px; padding:12px 16px; background:#fafafa; }\n",
    ".dataframe { border-collapse: collapse; width: 100%; font-size: 13px; }\n",
    ".dataframe th, .dataframe td { border: 1px solid #e6e6e6; padding: 6px 8px; }\n",
    ".dataframe th { background: #f5f5f5; text-align: left; }\n",
    ".compact td, .compact th { padding: 4px 6px; }\n",
    ".note { font-size: 12px; color:#555; }\n",
    ".figure { margin: 10px 0 20px 0; }\n",
    "hr { border: 0; border-top: 1px solid #eee; margin: 22px 0; }\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "header = f\"\"\"\n",
    "<div class=\"header\">\n",
    "  <div>\n",
    "    <h1>Relat√≥rio ‚Äî Temporal Graph Network (TGN)</h1>\n",
    "    <div class=\"note\">Execu√ß√£o: {exec_time}</div>\n",
    "  </div>\n",
    "  <div style=\"text-align:right;\">\n",
    "    <div class=\"note\">{RUN_DIR}</div>\n",
    "  </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "intro = _method_intro()\n",
    "\n",
    "# KPIs\n",
    "kpis = f\"\"\"\n",
    "<div class=\"kpi\">\n",
    "  <div class=\"card\"><b>Registros totais</b><br>{n_total:,}</div>\n",
    "  <div class=\"card\"><b>Anomalias</b><br>{n_anom:,} ({pct_anom:.2f}%)</div>\n",
    "  <div class=\"card\"><b>Limiar (percentil)</b><br>{threshold if threshold is not None else '‚Äî'}</div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# estat√≠stica descritiva + figuras padr√£o do ID014 (se existirem)\n",
    "stats_html = _stats_section(df_full)\n",
    "\n",
    "# descri√ß√£o janelas + IC\n",
    "windows_html = _windows_description()\n",
    "\n",
    "# Top-20\n",
    "top20_html = f\"\"\"\n",
    "<h3>Top {TOP_N_DETAILED} anomalias (por ensemble_score)</h3>\n",
    "{_table_html(top20, index=False)}\n",
    "\"\"\"\n",
    "\n",
    "# Ego-grafos e subgrafos\n",
    "ego_imgs = _collect_ego_images(RUN_DIR)\n",
    "ego_section = [\"<h3>Ego-grafos e subgrafos</h3>\",\n",
    "               \"<p>As figuras abaixo mostram egos de usu√°rios/unidades (ID015) e subgrafos por janela (ID016). \"\n",
    "               \"Cores e interpreta√ß√µes:</p>\",\n",
    "               \"<ul><li><b>Usu√°rio</b> (azul), <b>Benefici√°rio</b> (verde), <b>Unidade</b> (dourado/quadrado no caso de ID015); \"\n",
    "               \"espessura da aresta ‚àù soma de <code>valor_pago</code>; tamanho do n√≥ ‚àù grau no subgrafo.</li></ul>\"]\n",
    "if ego_imgs:\n",
    "    for f in ego_imgs:\n",
    "        ego_section.append(_embed_img_if_exists(RUN_DIR, f, f))\n",
    "else:\n",
    "    ego_section.append('<div class=\"note\">Nenhuma figura encontrada. Execute ID015/ID016 para gerar egos/subgrafos.</div>')\n",
    "ego_html = \"\\n\".join(ego_section)\n",
    "\n",
    "# Tabela completa de anomalias\n",
    "all_anom_html = f\"\"\"\n",
    "<h3>Todas as anomalias (tabela completa)</h3>\n",
    "{_table_html(all_anom, max_rows=MAX_ROWS_ALL_ANOM, index=False)}\n",
    "\"\"\"\n",
    "\n",
    "# Monta HTML final\n",
    "html = \"<html><head><meta charset='utf-8'/>\" + styles + \"</head><body>\" + \\\n",
    "       header + intro + kpis + \\\n",
    "       \"<h2>Estat√≠stica descritiva do input</h2>\" + stats_html + \\\n",
    "       \"<hr/><h2>Janelas temporais e intervalo de confian√ßa</h2>\" + windows_html + \\\n",
    "       \"<hr/><h2>Resultados consolidados</h2>\" + \\\n",
    "       _embed_img_if_exists(RUN_DIR, \"02_Distribui√ß√£o_do_Ensemble_Score.png\", \"Hist ensemble\") + \\\n",
    "       _embed_img_if_exists(RUN_DIR, \"03_Boxplot_do_Ensemble_Score_(Normal_vs_An√¥malo).png\", \"Boxplot ensemble\") + \\\n",
    "       \"<hr/><h2>Top anomalias</h2>\" + top20_html + \\\n",
    "       \"<hr/><h2>Grafos</h2>\" + ego_html + \\\n",
    "       \"<hr/><h2>Listagem completa de anomalias</h2>\" + all_anom_html + \\\n",
    "       \"</body></html>\"\n",
    "\n",
    "# ---------- SALVA HTML ----------\n",
    "run_dir = Path(RUN_DIR)\n",
    "report_html_path = run_dir / \"report_TGN.html\"\n",
    "with open(report_html_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(html)\n",
    "print(\"Relat√≥rio HTML salvo em:\", report_html_path)\n",
    "\n",
    "# ---------- TENTA EXPORTAR PDF ----------\n",
    "pdf_ok = _ensure_weasyprint()\n",
    "report_pdf_path = run_dir / \"report_TGN.pdf\"\n",
    "if pdf_ok:\n",
    "    try:\n",
    "        from weasyprint import HTML\n",
    "        HTML(filename=str(report_html_path)).write_pdf(str(report_pdf_path))\n",
    "        print(\"Relat√≥rio PDF salvo em:\", report_pdf_path)\n",
    "    except Exception as e:\n",
    "        print(\"Falha ao gerar PDF via weasyprint:\", repr(e))\n",
    "        print(\"Voc√™ ainda pode abrir o HTML e imprimir como PDF (Ctrl+P).\")\n",
    "else:\n",
    "    print(\"WeasyPrint n√£o dispon√≠vel; exporte o HTML para PDF via impress√£o (Ctrl+P).\")\n",
    "\n",
    "# Mensagem adicional isolada (Skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\"><b>ü§ñ Skynet</b>: Fim do jogo. A Humanidade perdeu. D√°-se in√≠cio √† Era das M√°quinas.</div>'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOhD4Djc7hsw9pUT5jQKE+S",
   "collapsed_sections": [
    "sqpYiRa1Ub5T",
    "JF-bzXqkfjsX",
    "NVymVtZorunW"
   ],
   "mount_file_id": "1J-iuHr7Sn1al5Mn2qZiO1zzF7Ne-YKK9",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
